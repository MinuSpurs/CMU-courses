{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Introduction\n",
                "\n",
                "In this assignment, you will build a language identification classifier that distinguishes between six languages:\n",
                "\n",
                "- [Hausa](https://en.wikipedia.org/wiki/Hausa_language)\n",
                "- [Indonesian](https://en.wikipedia.org/wiki/Indonesian_language)\n",
                "- [Manobo](https://en.wikipedia.org/wiki/Manobo_languages)\n",
                "- [Nahuatl](https://en.wikipedia.org/wiki/Nahuatl)\n",
                "- [Swahili](https://en.wikipedia.org/wiki/Swahili_language)\n",
                "- [Tagalog](https://en.wikipedia.org/wiki/Tagalog_language)\n",
                "\n",
                "Some languages can be distinguished easily, because they use different scripts. These six languages, however, use the same ([Latin](https://en.wikipedia.org/wiki/Latin_script)) script with minimal [diacritics](https://en.wikipedia.org/wiki/Diacritic) so it is difficult to hand-craft classifiers based on the presence or absence of particular characters. Indeed, unless you have linguistic training or familiarity with the languages, it is difficult to tell them apart.\n",
                "\n",
                "How can they be distinguished? A naïve approach is to use word counts as unigram features. However, the number of possible words in a large corpus of five languages is vast. It is essential to look at something smaller — characters.\n",
                "\n",
                "Even though the six languages use roughly the same characters, the relative frequencies of these characters vary greatly. Thus, using characters as features (unigram character models) is appealing (and fairly effective). It is also true that languages very in their *phonotactics*, the way in which consonants and vowels combine in sequence. Thus, looking at character ngrams (for small values of $n$) is also appealing (and effective). Note, however, that as the value of $n$ increases, this approach runs into the same problem as the word unigram model (sparcity). In this scenario, the model is likely to overfit.\n",
                "\n",
                "Various kinds of classifiers can be used for this application. NB classifiers, for example, are quite effective. However, inference is slow and performance, given the same training set, is likely to be worse than other options. Simple logistic regression cannot be used because this is an n-way (multinomial) classification problem. Multinomial Logistic Regression (Softmax Regression) is a good fit. \n",
                "\n",
                "## Summary\n",
                "\n",
                "You will perform the following tasks:\n",
                "\n",
                "1. Implement a training loop for Multinomial Logistic Regression.\n",
                "2. Implement inference for Multinomal Logistic Regression\n",
                "3. Determine the optimal order of $n$ for ngrams for MNLR trained on the training set.\n",
                "4. Calculate and display a confusion matrix for a trigram model evaluated on the test set.\n",
                "5. Inspect the feature weights, and display the most predictive features for each language. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Imports\n",
                "Do not change."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "metadata": {},
            "outputs": [],
            "source": [
                "import csv\n",
                "import random\n",
                "\n",
                "import numpy as np\n",
                "import numpy.typing as npt\n",
                "import numpy.testing as testing\n",
                "\n",
                "from collections import Counter"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Helper Functions"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Metrics for Binary Classification"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [],
            "source": [
                "def precision(tp: int, fp: int) -> float:\n",
                "    \"\"\"Computes the precision, given true positives and false positives.\"\"\"\n",
                "    return tp / (tp + fp)\n",
                "\n",
                "\n",
                "def recall(tp: int, fn: int) -> float:\n",
                "    \"\"\"Computes the recall, given the true positives and false negatives.\"\"\"\n",
                "    return tp / (tp + fn)\n",
                "\n",
                "\n",
                "def f_measure(beta: float, tp: int, fp: int, fn: int) -> float:\n",
                "    \"\"\"Computes the F-measure for a given beta, true positives, false positives, and false negatives.\"\"\"\n",
                "    return (\n",
                "        (1 + beta**2)\n",
                "        * (precision(tp, fp) * recall(tp, fn))\n",
                "        / (beta**2 * precision(tp, fp) * recall(tp, fn))\n",
                "    )\n",
                "\n",
                "\n",
                "def f1(tp: int, fp: int, fn: int) -> float:\n",
                "    \"\"\"Computes the F1 measure for a given TP, FP, and FN.\"\"\"\n",
                "    return f_measure(1, tp, fp, fn)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Micro-Averaged Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [],
            "source": [
                "def micro_precision(tp: \"dict[str, int]\", fp: \"dict[str, int]\") -> float:\n",
                "    \"\"\"Computes micro-averaged precision.\"\"\"\n",
                "    tp_sum = sum(tp.values())\n",
                "    fp_sum = sum(fp.values())\n",
                "    return tp_sum / (tp_sum + fp_sum)\n",
                "\n",
                "\n",
                "def micro_recall(tp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
                "    \"\"\"Computes micro-averaged recall.\"\"\"\n",
                "    tp_sum = sum(tp.values())\n",
                "    fn_sum = sum(fn.values())\n",
                "    return tp_sum / (tp_sum + fn_sum)\n",
                "\n",
                "\n",
                "def micro_f1(tp: \"dict[str, int]\", fp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
                "    \"\"\"Computes micro-averaged F1.\"\"\"\n",
                "    mp = micro_precision(tp, fp)\n",
                "    mr = micro_recall(tp, fn)\n",
                "    return 2 * (mp * mr) / (mp + mr)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Macro-Averaged Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "metadata": {},
            "outputs": [],
            "source": [
                "def macro_precision(tp: \"dict[str, int]\", fp: \"dict[str, int]\") -> float:\n",
                "    \"\"\"Computes macro-averaged precision.\"\"\"\n",
                "    n = len(tp)\n",
                "    return (1 / n) * sum([precision(tp[c], fp[c]) for c in tp.keys()])\n",
                "\n",
                "\n",
                "def macro_recall(tp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
                "    \"\"\"Computes macro-averaged recall.\"\"\"\n",
                "    n = len(tp)\n",
                "    return (1 / n) * sum([recall(tp[c], fn[c]) for c in tp.keys()])\n",
                "\n",
                "\n",
                "def macro_f1(tp: \"dict[str, int]\", fp: \"dict[str, int]\", fn: \"dict[str, int]\") -> float:\n",
                "    \"\"\"Computes macro-averaged F1.\"\"\"\n",
                "    n = len(tp)\n",
                "    return (\n",
                "        2\n",
                "        * (macro_precision(tp, fp) * macro_recall(tp, fn))\n",
                "        / (macro_precision(tp, fp) + macro_recall(tp, fn))\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Preprocess Data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Loading data from files\n",
                "\n",
                "We first need to load data from the provided TSV files. Each file is two columns, the language of the document and the document text, separated by a tab (`\\t`) character. We load this data into a list of tuples, to maintain the coupling between each document and its label."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_data(data_filename: str) -> list[tuple[str, str]]:\n",
                "    with open(data_filename) as fin:\n",
                "        reader = csv.reader(fin, delimiter=\"\\t\")\n",
                "        language_document_tuples = [(lang, doc) for (lang, doc) in reader]\n",
                "    return language_document_tuples"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feature Extraction\n",
                "\n",
                "We will use ngrams as features, so we need to be able to extract them."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "metadata": {},
            "outputs": [],
            "source": [
                "def extract_ngrams(x: str, n=3) -> \"list[str]\":\n",
                "    \"\"\"Given a string, return all character ngrams of order `n`.\"\"\"\n",
                "    return [\"\".join(s) for s in (zip(*[x[i:] for i in range(n)]))]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Language Codes to One-Hot Vectors\n",
                "And we need a function to convert a language code into a **one-hot vector** (called $\\mathbf{y}$)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "metadata": {},
            "outputs": [],
            "source": [
                "def to_onehot_vector(lang: str, langs: list[str]) -> np.ndarray:\n",
                "    y = np.zeros(len(langs))\n",
                "    y[langs.index(lang)] = 1\n",
                "    return y"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We need to be able to convert `dict`s of ngram counts to vectors of ngram counts (using a map from ngrams to dimensions of the vectors)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [],
            "source": [
                "def vectorize_ngrams(counter: dict[str, int], feature_map: dict[str, int]) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Given a dict of ngram counts and a map from features to indices, returns a vector of ngram counts.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    counter : dict\n",
                "        Counter/dict of ngram counts\n",
                "    feature_map : dict\n",
                "        Map from ngrams to indices\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    np.ndarray\n",
                "        A vector of ngram counts\n",
                "    \"\"\"\n",
                "    feature_vector = np.zeros(len(feature_map))\n",
                "    for ngram, count in counter.items():\n",
                "        if ngram in feature_map:\n",
                "            feature_vector[feature_map[ngram]] = count\n",
                "    return feature_vector"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "And putting together the conversion from text into ngrams, and vectorizing the ngrams:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "metadata": {},
            "outputs": [],
            "source": [
                "def vectorize_document(document: str, feature_map: dict[str, int], ngram_length: int) -> np.ndarray:\n",
                "    document_ngrams = extract_ngrams(document, ngram_length)\n",
                "    vector = vectorize_ngrams(Counter(document_ngrams), feature_map)\n",
                "    return vector"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We need separate functions for preprocessing the training observations and the dev/test observations. The former function must return a map from language names to labels as one-hot vectors as well as a map from features (ngrams) to indices of vector dimensions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess_training_observations(\n",
                "    training_observations: list[tuple[str, str]], n: int = 1\n",
                ") -> tuple[list[tuple[np.ndarray, np.ndarray]], dict[str, np.ndarray], dict[str, int]]:\n",
                "    langs = set()\n",
                "    features = set()\n",
                "    obs = []\n",
                "\n",
                "    for lang, doc in training_observations:\n",
                "        langs.add(lang)\n",
                "        ngrams = extract_ngrams(doc, n)\n",
                "        features = features | set(ngrams)\n",
                "        obs.append((lang, ngrams))\n",
                "    feature_map = {feature: idx for idx, feature in enumerate(sorted(features))}\n",
                "    lang_list = list(sorted(langs))\n",
                "    lang_map = {lang: to_onehot_vector(lang, lang_list) for lang in lang_list}\n",
                "\n",
                "    obs = [\n",
                "        (lang_map[lang], vectorize_ngrams(Counter(ngrams), feature_map)) for (lang, ngrams) in obs\n",
                "    ]\n",
                "\n",
                "    print(f\"{len(obs)} training observations.\")\n",
                "    return obs, lang_map, feature_map"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The function for preprocessing test observations (and dev observations) takes the feature map and the language map as arguments and returns only the list of labeled observations."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess_test_observations(\n",
                "    test_observations, feature_map: dict[str, int], lang_map: dict[str, np.ndarray], n: int = 1\n",
                ") -> tuple[list[np.ndarray], list[np.ndarray]]:\n",
                "    obs = []\n",
                "\n",
                "    for i, (lang, doc) in enumerate(test_observations):\n",
                "        vectorized_doc = vectorize_document(doc, feature_map, ngram_length=n)\n",
                "        try:\n",
                "            obs.append((lang_map[lang], vectorized_doc))\n",
                "        except KeyError:\n",
                "            print(f\"Unkown language {lang} at index {i}. Known languages are: {lang_map.keys()}\")\n",
                "    print(f\"{len(obs)} test observations.\")\n",
                "    return obs"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Classification Function"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We also need to define the softmax function.\n",
                "\n",
                "Softmax is technically\n",
                "\n",
                "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum^K_{j=1} e^{z_j}}$$\n",
                "\n",
                "However, if implemented naïvely, this is not numerically stable. Instead, we use:\n",
                "\n",
                "$$\\text{softmax}(z_i) = \\frac{e^{z_i - \\text{max}(z)}}{\\sum^K_{j=1} e^{z_j - \\text{max}(z)}}$$\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [],
            "source": [
                "def softmax(z: npt.ArrayLike) -> npt.ArrayLike:\n",
                "    \"\"\"Compute the softmax of a vector `z`\"\"\"\n",
                "    # exp(z) can get very large. For numerical stability, we subtract a vector of very large values (np.max(z)) from z.\n",
                "    return np.exp(z - np.max(z)) / np.exp(z - np.max(z)).sum()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Training the Classifier"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Compute the gradient\n",
                "\n",
                "The formula for computing one element in our gradient is as follows (the partial derivitive of the negative log likelihood loss):\n",
                "\n",
                "$$\\frac{\\partial L_{CE}}{\\partial \\mathbf{w}_{k,i}}=-(\\mathbf{y}_k-\\hat{\\mathrm{y}}_k)\\mathbf{x}_i$$\n",
                "\n",
                "where $k$ is the **class** (rows of the matrix $\\mathbf{w}$) and $i$ corresponds the the feature (columns of the matrix $\\mathbf{w}$).\n",
                "\n",
                "We will define a function `grad` for computing the whole gradient, a $K \\times N$ matrix.\n",
                "\n",
                "**This is the first piece of code that you'll write.**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [],
            "source": [
                "def grad(W: np.ndarray, y: np.ndarray, x: np.ndarray) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Calculates the gradient of the negative log likelihood loss, a [K * N] matrix, with respect to W.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    W : np.ndarray\n",
                "       A [K * N] matrix of weights, where K is the number of classes and N is the number of features.\n",
                "    y : np.ndarray\n",
                "       The true label of the observation, expressed as a [K * 1] one-hot vector.\n",
                "    x : np.ndarray\n",
                "       A [N * 1] vector of features.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    np.ndarray\n",
                "        The gradient of the loss with respect to W.\n",
                "    \"\"\"\n",
                "    \n",
                "    # Step 1: Compute the prediction (Softmax)\n",
                "    z = W @ x  # Linear transformation (K * 1)\n",
                "    exp_z = np.exp(z - np.max(z))  # Subtracting max for numerical stability\n",
                "    softmax = exp_z / np.sum(exp_z)\n",
                "    \n",
                "    # Step 2: Compute the gradient (y - softmax) * x^T\n",
                "    gradient = np.outer(softmax - y, x)  # (K * 1) - (K * 1), and x is (N * 1)\n",
                "    \n",
                "    return gradient"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Loop\n",
                "\n",
                "Iterate over the observations in the training set $n$ times (in random order). For each item, compute the one-hot vector $\\mathbf{y}$ and the probability distribution $\\hat{\\mathbf{y}}$. Use these values to compute the gradient. Update the parameters based on the gradient. At the end of each epoch (pass through the training data), compute the true positives, false positives, and false negatives for each target class based on the current weights, and report micro-averaged precision and recall.\n",
                "\n",
                "How you report the metrics is up to you — we will not look at what you output to STDOUT — but it is important that you do this. **Otherwise, you will not be able to determine whether your model is training.**\n",
                "\n",
                "You can also output the loss at each step (very noisy!), each epoch, or run the classifier on the dev set and report the metrics at the end of each epoch.\n",
                "\n",
                "Remember that the 0th column in $W$ contains the biases. You will have to insert a $1$ at the beginning of the feature vector $x$ in order to accomodate this."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {
                "tags": [
                    "Answer Expected"
                ]
            },
            "outputs": [],
            "source": [
                "def train(observations: list, eta: float, epochs: int = 1) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Given a set of observations, returns a trained multinomial logistic regression (softmax regression) model.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    observations : list of tuples\n",
                "        List of tuples where each tuple contains (label, features).\n",
                "    eta : float\n",
                "        The learning rate for gradient descent.\n",
                "    epochs : int, optional (default=1)\n",
                "        The number of epochs to train the model.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    np.ndarray\n",
                "        The trained model as a [K * N] weight matrix.\n",
                "    \"\"\"\n",
                "\n",
                "    # Extract labels (y) and features (X) from the observations\n",
                "    y = np.array([obs[0] for obs in observations])  # Extract labels\n",
                "    X = np.array([obs[1] for obs in observations])  # Extract features\n",
                "\n",
                "    # Get the number of samples and number of features\n",
                "    num_samples, num_features = X.shape\n",
                "    num_classes = y.shape[1]  # Assuming one-hot encoding for labels\n",
                "\n",
                "    # Initialize weights randomly (shape: [K * N])\n",
                "    W = np.random.randn(num_classes, num_features)\n",
                "\n",
                "    # Training loop\n",
                "    for epoch in range(epochs):\n",
                "        total_loss = 0\n",
                "        \n",
                "        for i in range(num_samples):\n",
                "            x_i = X[i]           # Feature vector for sample i (shape: [N])\n",
                "            y_i = y[i]           # True label for sample i (one-hot vector, shape: [K])\n",
                "            \n",
                "            # Compute prediction (softmax)\n",
                "            z = W @ x_i          # Linear transformation (shape: [K])\n",
                "            exp_z = np.exp(z - np.max(z))  # Stability trick\n",
                "            softmax = exp_z / np.sum(exp_z)  # Softmax output (shape: [K])\n",
                "            \n",
                "            # Compute the loss (negative log likelihood)\n",
                "            loss = -np.sum(y_i * np.log(softmax + 1e-8))  # Adding a small value to avoid log(0)\n",
                "            total_loss += loss\n",
                "            \n",
                "            # Compute gradient of the loss with respect to W\n",
                "            grad_W = np.outer(softmax - y_i, x_i)  # Gradient for this sample (shape: [K * N])\n",
                "            \n",
                "            # Update weights using gradient descent\n",
                "            W -= eta * grad_W\n",
                "        \n",
                "        # Print the loss for the current epoch (optional)\n",
                "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/num_samples:.4f}\")\n",
                "    \n",
                "    return W"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Classification\n",
                "\n",
                "The classification function is very simple."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {
                "tags": [
                    "Answer Expected"
                ]
            },
            "outputs": [],
            "source": [
                "def classify(W: np.ndarray, x: np.ndarray) -> np.intp:\n",
                "    \"\"\"\n",
                "    Return the index of the hypothesized language (or class).\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    W : np.ndarray\n",
                "        Weight matrix (one row for each category/language, one column for each feature).\n",
                "    x : np.ndarray\n",
                "        Vector of real-valued features.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    np.intp\n",
                "        The index of the hypothesized language (or class).\n",
                "    \"\"\"\n",
                "    \n",
                "    # Step 1: Compute the linear transformation\n",
                "    z = W @ x  # z is the scores for each class (shape: [K])\n",
                "    \n",
                "    # Step 2: Find the index of the class with the highest score\n",
                "    predicted_class = np.argmax(z)  # Index of the max value in z\n",
                "    \n",
                "    return predicted_class"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Evaluate the Classifier"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Then, a function to train and evaluate the classifier."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate(train_set, test_set, eta, epochs=3):\n",
                "    \"\"\"Trains and evaluates a model.\"\"\"\n",
                "    print(\"Training model\")\n",
                "    W = train(train_set, eta, epochs=epochs)\n",
                "    print(\"\\nCLASSIFY\")\n",
                "    tp, fp, fn = Counter(), Counter(), Counter()\n",
                "    for ref_lang_vec, x in test_set:\n",
                "        ref_lang = np.argmax(ref_lang_vec)\n",
                "\n",
                "        hyp_lang = classify(W, x)\n",
                "        if hyp_lang == ref_lang:\n",
                "            tp[ref_lang] += 1\n",
                "        else:\n",
                "            fp[hyp_lang] += 1\n",
                "            fn[ref_lang] += 1\n",
                "    # Print metrics\n",
                "\n",
                "    test_macro_f1 = macro_f1(tp, fp, fn)\n",
                "    test_micro_f1 = micro_f1(tp, fp, fn)\n",
                "    print(f\"macro-averaged F1:\\t\\t{test_macro_f1:.3f}\")\n",
                "    print(f\"micro-averaged F1:\\t\\t{test_micro_f1:.3f}\")\n",
                "    return test_macro_f1, test_micro_f1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Putting it all together"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Load the data:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_observations = load_data(\"train.tsv\")\n",
                "test_observations = load_data(\"test.tsv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Inspecting the data\n",
                "\n",
                "Before we actually train the classifier, it's important to look at your data, and check that any assumptions you're making about it are justified. It's always useful at this point to check basic things, like:\n",
                "- How many instances of train and test data do you have?\n",
                "- What labels are in your data, and do those match between the train and test splits?\n",
                "- What is the class balance (i.e. how many instances of each class) in your dataset? Is it balanced or unbalanced?\n",
                "\n",
                "Output a dictionary for the train and test data that maps each label to the count of instances that have that label, e.g.: \n",
                "```\n",
                "{\"hausa\": 4000, \"indonesian\":...}\n",
                "```\n",
                "\n",
                "Your dictionary should be sorted in descending order of occurrence of languages."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "metadata": {
                "output_for": "1.1",
                "tags": [
                    "Answer Expected"
                ]
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train Set Label Counts: {'swahili': 377, 'tagalog': 365, 'manobo': 111, 'hausa': 91, 'nahuatl': 90, 'indonesian': 86}\n"
                    ]
                }
            ],
            "source": [
                "# this cell's output will be used for test 1.1\n",
                "# your code here - train set\n",
                "\n",
                "import csv\n",
                "from collections import Counter\n",
                "\n",
                "def load_labels_from_tsv(file_path: str) -> list:\n",
                "    \"\"\"\n",
                "    Loads the labels (languages) from the first column of a TSV file.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    file_path : str\n",
                "        The path to the TSV file.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    list\n",
                "        A list of language labels from the first column.\n",
                "    \"\"\"\n",
                "    labels = []\n",
                "    with open(file_path, 'r', encoding='utf-8') as file:\n",
                "        reader = csv.reader(file, delimiter='\\t')\n",
                "        for row in reader:\n",
                "            labels.append(row[0])  # The first column is the language label\n",
                "    return labels\n",
                "\n",
                "# Load the labels from the train.tsv file\n",
                "train_labels = load_labels_from_tsv(\"train.tsv\")\n",
                "\n",
                "# Count the occurrences of each label\n",
                "train_label_counts = Counter(train_labels)\n",
                "\n",
                "# Sort the label counts in descending order\n",
                "sorted_train_label_counts = dict(sorted(train_label_counts.items(), key=lambda item: item[1], reverse=True))\n",
                "\n",
                "# Output the train set label counts\n",
                "print(\"Train Set Label Counts:\", sorted_train_label_counts)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 30,
            "metadata": {
                "output_for": "1.2",
                "tags": [
                    "Answer Expected"
                ]
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Test Set Label Counts: {'swahili': 142, 'tagalog': 121, 'nahuatl': 36, 'manobo': 34, 'indonesian': 32, 'hausa': 28}\n"
                    ]
                }
            ],
            "source": [
                "# this cell's output will be used for test 1.2\n",
                "# your code here - test set\n",
                "\n",
                "import csv\n",
                "from collections import Counter\n",
                "\n",
                "def load_labels_from_tsv(file_path: str) -> list:\n",
                "    \"\"\"\n",
                "    Loads the labels (languages) from the first column of a TSV file.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    file_path : str\n",
                "        The path to the TSV file.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    list\n",
                "        A list of language labels from the first column.\n",
                "    \"\"\"\n",
                "    labels = []\n",
                "    with open(file_path, 'r', encoding='utf-8') as file:\n",
                "        reader = csv.reader(file, delimiter='\\t')\n",
                "        for row in reader:\n",
                "            labels.append(row[0])  # The first column is the language label\n",
                "    return labels\n",
                "\n",
                "# Load the labels from the test.tsv file\n",
                "test_labels = load_labels_from_tsv(\"test.tsv\")\n",
                "\n",
                "# Count the occurrences of each label\n",
                "test_label_counts = Counter(test_labels)\n",
                "\n",
                "# Sort the label counts in descending order\n",
                "sorted_test_label_counts = dict(sorted(test_label_counts.items(), key=lambda item: item[1], reverse=True))\n",
                "\n",
                "# Output the test set label counts\n",
                "print(\"Test Set Label Counts:\", sorted_test_label_counts)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Set hyperparameters and parameters\n",
                "\n",
                "Before training the model, we have to set the learning rate $\\eta$ and the order of the ngrams used in feature extraction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [],
            "source": [
                "eta = 0.0005  # Do not change this.\n",
                "epochs = 4  # Do not change this.\n",
                "order_of_ngrams = 1  # Do change this."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Running our training and evaluation loops"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now train your classifier, and evaluate it on the test set. Vary the number of ngrams, and observe how it changes train and test F1."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1120 training observations.\n",
                        "393 test observations.\n",
                        "Training model\n",
                        "Epoch 1/4, Loss: 6.3406\n",
                        "Epoch 2/4, Loss: 2.7149\n",
                        "Epoch 3/4, Loss: 2.0378\n",
                        "Epoch 4/4, Loss: 1.6915\n",
                        "\n",
                        "CLASSIFY\n",
                        "macro-averaged F1:\t\t0.697\n",
                        "micro-averaged F1:\t\t0.733\n"
                    ]
                }
            ],
            "source": [
                "train_set, lang_map, feature_map = preprocess_training_observations(\n",
                "    train_observations, n=order_of_ngrams\n",
                ")\n",
                "\n",
                "\n",
                "test_set = preprocess_test_observations(test_observations, feature_map, lang_map, n=order_of_ngrams)\n",
                "\n",
                "\n",
                "random.seed(27)\n",
                "test_macro_f1, test_micro_f1 = evaluate(train_set, test_set, eta, epochs=epochs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Print the tuple of the best values of `(macro_f1, micro_f1)` for the model evaluated on your test set while varying the order of the ngrams. What value of n produced the best result? Why might that be?"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 33,
            "metadata": {
                "output_for": "2.1",
                "tags": [
                    "Answer Expected"
                ]
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(0.6971469989725778, 0.7328244274809159)\n"
                    ]
                }
            ],
            "source": [
                "# the output of this cell will be used for test 2.1\n",
                "print((test_macro_f1, test_micro_f1))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Inspecting Classification Results\n",
                "\n",
                "We've trained our classifier, and your final F1 should be pretty close to 1.0. Great job! But what does that mean for the languages you're actually classifying? Let's rewrite our evaluation code to allow us to look at our results instance-by-instance. For this, we're going to examine the results of a re-trained trigram classifier, trained for one epoch."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 34,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "1120 training observations.\n",
                        "393 test observations.\n",
                        "Epoch 1/1, Loss: 9.6054\n",
                        "macro-averaged F1:\t\t0.218\n",
                        "micro-averaged F1:\t\t0.293\n"
                    ]
                }
            ],
            "source": [
                "# Re-training a trigram classifier. Do not change this.\n",
                "\n",
                "INSPECTION_NGRAMS = 3\n",
                "\n",
                "train_set, lang_map, feature_map = preprocess_training_observations(\n",
                "    train_observations, n=INSPECTION_NGRAMS\n",
                ")\n",
                "test_set = preprocess_test_observations(\n",
                "    test_observations, feature_map, lang_map, n=INSPECTION_NGRAMS\n",
                ")\n",
                "\n",
                "random.seed(27)\n",
                "W_inspect = train(train_set, eta, epochs=1)\n",
                "\n",
                "## evaluate it as before. Check that this looks the same!\n",
                "tp, fp, fn = Counter(), Counter(), Counter()\n",
                "for ref_lang_vec, x in test_set:\n",
                "    ref_lang = np.argmax(ref_lang_vec)\n",
                "\n",
                "    hyp_lang = classify(W_inspect, x)\n",
                "    if hyp_lang == ref_lang:\n",
                "        tp[ref_lang] += 1\n",
                "    else:\n",
                "        fp[hyp_lang] += 1\n",
                "        fn[ref_lang] += 1\n",
                "# Print metrics\n",
                "\n",
                "print(f\"macro-averaged F1:\\t\\t{macro_f1(tp, fp, fn):.3f}\")\n",
                "print(f\"micro-averaged F1:\\t\\t{micro_f1(tp, fp, fn):.3f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Writing a prediction function\n",
                "\n",
                "Write a function that takes a list of observations, and produces a list of either class indices, or class names based on a parameter. This will allow you both to look at individual results from evaluating on an existing set of data (like the test set), but also for you to evaluate your classifier on new data (i.e. any string). This will involve vectorizing the list of documents, and classifying those vectors. Once that's complete, construct an inverse language mapping, from predicted indices to the language they represent."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 35,
            "metadata": {
                "tags": [
                    "Answer Expected"
                ]
            },
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import csv\n",
                "\n",
                "def vectorize_document(document: str, feature_map: dict, ngram_length: int) -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Vectorizes a document by creating an n-gram based feature vector.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    document : str\n",
                "        The input document (a string).\n",
                "    feature_map : dict\n",
                "        A mapping from n-grams to feature indices.\n",
                "    ngram_length : int\n",
                "        The length of n-grams to extract.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    np.ndarray\n",
                "        The feature vector for the document.\n",
                "    \"\"\"\n",
                "    vector = np.zeros(len(feature_map))\n",
                "\n",
                "    # Extract n-grams from the document\n",
                "    for i in range(len(document) - ngram_length + 1):\n",
                "        ngram = document[i:i + ngram_length]\n",
                "        if ngram in feature_map:\n",
                "            vector[feature_map[ngram]] += 1\n",
                "    \n",
                "    return vector\n",
                "\n",
                "def predict(\n",
                "    documents: list[str],\n",
                "    W: np.ndarray,\n",
                "    feature_map: dict[str, int],\n",
                "    lang_map: dict[str, np.ndarray],\n",
                "    ngram_length: int,\n",
                "    return_class_names=False,\n",
                "):\n",
                "    \"\"\"\n",
                "    Predicts the class indices or names for a list of documents.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    documents : list of str\n",
                "        List of input documents (strings).\n",
                "    W : np.ndarray\n",
                "        The weight matrix of the trained softmax classifier.\n",
                "    feature_map : dict\n",
                "        Mapping from n-grams to feature indices.\n",
                "    lang_map : dict\n",
                "        Mapping from language names to one-hot vectors.\n",
                "    ngram_length : int\n",
                "        The length of n-grams to extract from documents.\n",
                "    return_class_names : bool, optional (default=False)\n",
                "        If True, returns the predicted class names. Otherwise, returns class indices.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    list\n",
                "        List of predicted class indices or class names based on return_class_names.\n",
                "    \"\"\"\n",
                "    \n",
                "    # Inverse language mapping (from index to language name)\n",
                "    inverse_lang_map = {np.argmax(v): k for k, v in lang_map.items()}\n",
                "    \n",
                "    predictions = []\n",
                "\n",
                "    for doc in documents:\n",
                "        # Step 1: Vectorize the document\n",
                "        x = vectorize_document(doc, feature_map, ngram_length)\n",
                "        \n",
                "        # Step 2: Compute the prediction (softmax)\n",
                "        z = W @ x\n",
                "        exp_z = np.exp(z - np.max(z))  # Stability trick\n",
                "        softmax = exp_z / np.sum(exp_z)\n",
                "        \n",
                "        # Step 3: Find the index of the predicted class\n",
                "        predicted_index = np.argmax(softmax)\n",
                "        \n",
                "        # Step 4: Return class name or index\n",
                "        if return_class_names:\n",
                "            predictions.append(inverse_lang_map[predicted_index])\n",
                "        else:\n",
                "            predictions.append(predicted_index)\n",
                "    \n",
                "    return predictions\n",
                "\n",
                "def load_data(file_path: str) -> list:\n",
                "    \"\"\"\n",
                "    Loads data from a tsv file and returns a list of (label, document) tuples.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    file_path : str\n",
                "        The path to the tsv file.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    list\n",
                "        A list of (label, document) tuples.\n",
                "    \"\"\"\n",
                "    observations = []\n",
                "    with open(file_path, 'r', encoding='utf-8') as file:\n",
                "        reader = csv.reader(file, delimiter='\\t')\n",
                "        for row in reader:\n",
                "            label, document = row\n",
                "            observations.append((label, document))\n",
                "    return observations"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, for each instance in the test set, print a tuple of the actual label, then the predicted label."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "metadata": {
                "output_for": "3.1",
                "tags": [
                    "Answer Expected"
                ]
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "[('swahili', 'indonesian'), ('indonesian', 'hausa'), ('tagalog', 'tagalog'), ('indonesian', 'manobo'), ('hausa', 'manobo'), ('nahuatl', 'swahili'), ('hausa', 'swahili'), ('manobo', 'manobo'), ('swahili', 'swahili'), ('swahili', 'manobo'), ('tagalog', 'manobo'), ('indonesian', 'hausa'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'hausa'), ('hausa', 'hausa'), ('indonesian', 'manobo'), ('manobo', 'indonesian'), ('tagalog', 'indonesian'), ('swahili', 'manobo'), ('swahili', 'swahili'), ('manobo', 'tagalog'), ('swahili', 'tagalog'), ('manobo', 'indonesian'), ('swahili', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'hausa'), ('swahili', 'manobo'), ('nahuatl', 'nahuatl'), ('nahuatl', 'manobo'), ('swahili', 'manobo'), ('swahili', 'swahili'), ('swahili', 'hausa'), ('tagalog', 'tagalog'), ('indonesian', 'manobo'), ('nahuatl', 'indonesian'), ('tagalog', 'manobo'), ('tagalog', 'tagalog'), ('tagalog', 'nahuatl'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('indonesian', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'indonesian'), ('swahili', 'swahili'), ('nahuatl', 'nahuatl'), ('swahili', 'tagalog'), ('swahili', 'indonesian'), ('swahili', 'hausa'), ('swahili', 'hausa'), ('swahili', 'swahili'), ('manobo', 'nahuatl'), ('nahuatl', 'indonesian'), ('swahili', 'swahili'), ('indonesian', 'indonesian'), ('nahuatl', 'indonesian'), ('nahuatl', 'tagalog'), ('tagalog', 'tagalog'), ('hausa', 'swahili'), ('swahili', 'hausa'), ('tagalog', 'swahili'), ('tagalog', 'indonesian'), ('indonesian', 'hausa'), ('swahili', 'hausa'), ('tagalog', 'tagalog'), ('tagalog', 'nahuatl'), ('tagalog', 'tagalog'), ('swahili', 'nahuatl'), ('tagalog', 'tagalog'), ('manobo', 'hausa'), ('tagalog', 'tagalog'), ('tagalog', 'swahili'), ('manobo', 'nahuatl'), ('manobo', 'swahili'), ('manobo', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'hausa'), ('swahili', 'hausa'), ('swahili', 'hausa'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('manobo', 'tagalog'), ('swahili', 'hausa'), ('tagalog', 'tagalog'), ('indonesian', 'manobo'), ('swahili', 'manobo'), ('swahili', 'hausa'), ('tagalog', 'tagalog'), ('manobo', 'tagalog'), ('swahili', 'tagalog'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('indonesian', 'hausa'), ('swahili', 'hausa'), ('swahili', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'nahuatl'), ('hausa', 'swahili'), ('swahili', 'manobo'), ('tagalog', 'tagalog'), ('nahuatl', 'swahili'), ('tagalog', 'hausa'), ('swahili', 'nahuatl'), ('tagalog', 'tagalog'), ('tagalog', 'swahili'), ('tagalog', 'nahuatl'), ('swahili', 'manobo'), ('tagalog', 'indonesian'), ('swahili', 'hausa'), ('tagalog', 'indonesian'), ('swahili', 'indonesian'), ('swahili', 'hausa'), ('tagalog', 'indonesian'), ('swahili', 'swahili'), ('swahili', 'hausa'), ('tagalog', 'indonesian'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('manobo', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'manobo'), ('tagalog', 'tagalog'), ('manobo', 'tagalog'), ('manobo', 'tagalog'), ('nahuatl', 'swahili'), ('indonesian', 'swahili'), ('swahili', 'hausa'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('indonesian', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'hausa'), ('tagalog', 'indonesian'), ('nahuatl', 'manobo'), ('tagalog', 'hausa'), ('tagalog', 'indonesian'), ('swahili', 'hausa'), ('swahili', 'hausa'), ('swahili', 'swahili'), ('swahili', 'tagalog'), ('swahili', 'indonesian'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('tagalog', 'nahuatl'), ('swahili', 'nahuatl'), ('swahili', 'nahuatl'), ('hausa', 'manobo'), ('hausa', 'swahili'), ('hausa', 'manobo'), ('swahili', 'hausa'), ('swahili', 'manobo'), ('tagalog', 'tagalog'), ('manobo', 'nahuatl'), ('indonesian', 'nahuatl'), ('hausa', 'indonesian'), ('swahili', 'indonesian'), ('swahili', 'swahili'), ('tagalog', 'nahuatl'), ('manobo', 'tagalog'), ('nahuatl', 'manobo'), ('swahili', 'swahili'), ('tagalog', 'manobo'), ('manobo', 'tagalog'), ('tagalog', 'tagalog'), ('indonesian', 'manobo'), ('swahili', 'tagalog'), ('tagalog', 'indonesian'), ('tagalog', 'tagalog'), ('tagalog', 'indonesian'), ('swahili', 'indonesian'), ('tagalog', 'nahuatl'), ('swahili', 'hausa'), ('swahili', 'nahuatl'), ('hausa', 'hausa'), ('tagalog', 'nahuatl'), ('manobo', 'hausa'), ('tagalog', 'indonesian'), ('nahuatl', 'swahili'), ('swahili', 'indonesian'), ('swahili', 'indonesian'), ('tagalog', 'nahuatl'), ('swahili', 'nahuatl'), ('tagalog', 'indonesian'), ('nahuatl', 'hausa'), ('tagalog', 'tagalog'), ('tagalog', 'swahili'), ('swahili', 'manobo'), ('tagalog', 'tagalog'), ('swahili', 'tagalog'), ('swahili', 'hausa'), ('hausa', 'swahili'), ('tagalog', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'swahili'), ('swahili', 'manobo'), ('swahili', 'swahili'), ('nahuatl', 'swahili'), ('manobo', 'swahili'), ('swahili', 'hausa'), ('swahili', 'hausa'), ('manobo', 'tagalog'), ('tagalog', 'indonesian'), ('swahili', 'swahili'), ('swahili', 'indonesian'), ('nahuatl', 'swahili'), ('nahuatl', 'hausa'), ('tagalog', 'nahuatl'), ('tagalog', 'indonesian'), ('tagalog', 'indonesian'), ('manobo', 'manobo'), ('tagalog', 'tagalog'), ('hausa', 'manobo'), ('swahili', 'swahili'), ('swahili', 'hausa'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('hausa', 'tagalog'), ('swahili', 'manobo'), ('nahuatl', 'manobo'), ('swahili', 'tagalog'), ('swahili', 'manobo'), ('tagalog', 'tagalog'), ('indonesian', 'swahili'), ('hausa', 'tagalog'), ('nahuatl', 'swahili'), ('swahili', 'indonesian'), ('hausa', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'tagalog'), ('swahili', 'indonesian'), ('tagalog', 'indonesian'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('swahili', 'hausa'), ('nahuatl', 'indonesian'), ('tagalog', 'indonesian'), ('manobo', 'manobo'), ('manobo', 'indonesian'), ('swahili', 'hausa'), ('nahuatl', 'nahuatl'), ('nahuatl', 'hausa'), ('swahili', 'swahili'), ('hausa', 'tagalog'), ('swahili', 'tagalog'), ('hausa', 'indonesian'), ('tagalog', 'tagalog'), ('swahili', 'tagalog'), ('swahili', 'swahili'), ('indonesian', 'nahuatl'), ('swahili', 'hausa'), ('manobo', 'tagalog'), ('tagalog', 'swahili'), ('nahuatl', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'tagalog'), ('tagalog', 'indonesian'), ('tagalog', 'swahili'), ('tagalog', 'indonesian'), ('swahili', 'hausa'), ('swahili', 'swahili'), ('manobo', 'tagalog'), ('swahili', 'hausa'), ('swahili', 'swahili'), ('indonesian', 'hausa'), ('tagalog', 'tagalog'), ('hausa', 'swahili'), ('tagalog', 'tagalog'), ('swahili', 'manobo'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('indonesian', 'hausa'), ('tagalog', 'swahili'), ('nahuatl', 'swahili'), ('indonesian', 'nahuatl'), ('manobo', 'tagalog'), ('manobo', 'indonesian'), ('indonesian', 'swahili'), ('indonesian', 'manobo'), ('tagalog', 'tagalog'), ('hausa', 'indonesian'), ('manobo', 'manobo'), ('nahuatl', 'nahuatl'), ('swahili', 'hausa'), ('swahili', 'swahili'), ('swahili', 'tagalog'), ('hausa', 'manobo'), ('swahili', 'swahili'), ('nahuatl', 'swahili'), ('swahili', 'hausa'), ('nahuatl', 'indonesian'), ('swahili', 'swahili'), ('swahili', 'hausa'), ('tagalog', 'nahuatl'), ('swahili', 'swahili'), ('manobo', 'manobo'), ('swahili', 'hausa'), ('tagalog', 'tagalog'), ('indonesian', 'nahuatl'), ('swahili', 'manobo'), ('swahili', 'manobo'), ('swahili', 'hausa'), ('tagalog', 'nahuatl'), ('swahili', 'tagalog'), ('nahuatl', 'manobo'), ('swahili', 'hausa'), ('tagalog', 'tagalog'), ('tagalog', 'swahili'), ('hausa', 'indonesian'), ('tagalog', 'tagalog'), ('indonesian', 'manobo'), ('hausa', 'tagalog'), ('nahuatl', 'manobo'), ('swahili', 'hausa'), ('swahili', 'indonesian'), ('nahuatl', 'swahili'), ('swahili', 'swahili'), ('manobo', 'nahuatl'), ('swahili', 'hausa'), ('swahili', 'hausa'), ('nahuatl', 'swahili'), ('swahili', 'hausa'), ('tagalog', 'tagalog'), ('swahili', 'tagalog'), ('tagalog', 'indonesian'), ('swahili', 'swahili'), ('swahili', 'tagalog'), ('tagalog', 'tagalog'), ('tagalog', 'nahuatl'), ('swahili', 'manobo'), ('swahili', 'indonesian'), ('tagalog', 'tagalog'), ('tagalog', 'manobo'), ('swahili', 'hausa'), ('tagalog', 'nahuatl'), ('indonesian', 'indonesian'), ('tagalog', 'tagalog'), ('indonesian', 'indonesian'), ('indonesian', 'tagalog'), ('hausa', 'swahili'), ('tagalog', 'nahuatl'), ('tagalog', 'indonesian'), ('indonesian', 'tagalog'), ('tagalog', 'indonesian'), ('tagalog', 'tagalog'), ('nahuatl', 'swahili'), ('tagalog', 'tagalog'), ('hausa', 'swahili'), ('tagalog', 'nahuatl'), ('swahili', 'indonesian'), ('swahili', 'manobo'), ('swahili', 'swahili'), ('swahili', 'hausa'), ('swahili', 'manobo'), ('nahuatl', 'indonesian'), ('swahili', 'hausa'), ('tagalog', 'tagalog'), ('swahili', 'swahili'), ('swahili', 'hausa'), ('hausa', 'manobo'), ('hausa', 'swahili'), ('tagalog', 'indonesian'), ('indonesian', 'manobo'), ('nahuatl', 'swahili'), ('swahili', 'hausa'), ('manobo', 'tagalog'), ('hausa', 'swahili'), ('hausa', 'indonesian'), ('tagalog', 'tagalog'), ('swahili', 'tagalog'), ('nahuatl', 'indonesian'), ('indonesian', 'manobo'), ('manobo', 'tagalog'), ('nahuatl', 'swahili'), ('swahili', 'hausa'), ('indonesian', 'hausa'), ('tagalog', 'hausa'), ('tagalog', 'tagalog'), ('tagalog', 'tagalog'), ('manobo', 'swahili'), ('swahili', 'swahili'), ('manobo', 'hausa'), ('tagalog', 'hausa'), ('swahili', 'hausa'), ('swahili', 'swahili'), ('tagalog', 'tagalog'), ('indonesian', 'swahili'), ('tagalog', 'tagalog'), ('tagalog', 'nahuatl'), ('tagalog', 'tagalog'), ('swahili', 'swahili')]\n"
                    ]
                }
            ],
            "source": [
                "# Assuming we load the test data from a file\n",
                "test_observations = load_data(\"test.tsv\")  # Example function to load data\n",
                "\n",
                "# the output of this cell will be used for test 3.1\n",
                "test_labels, test_documents = zip(*test_observations)\n",
                "test_predictions = predict(\n",
                "    test_documents,\n",
                "    W_inspect,\n",
                "    feature_map,\n",
                "    lang_map,\n",
                "    ngram_length=INSPECTION_NGRAMS,\n",
                "    return_class_names=True,\n",
                ")\n",
                "\n",
                "print(list(zip(test_labels, test_predictions)))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Plotting a Confusion Matrix\n",
                "\n",
                " A _confusion matrix_ is a $k \\times k$ matrix, where k is your number of classes, where the cell in position $(i,j)$ counts the number of instances that belong to class $i$ that were predicted to be in class $j$. The diagonal entries represent correct classifications; anything off of the diagonal represents an incorrect classification. The example below, from the [scikit learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html), shows a confusion matrix for a binary classification problem. \n",
                "\n",
                "![A confusion matrix for a binary classification problem](confusion_matrix.png)\n",
                "\n",
                "\n",
                "Confusion matrices can be useful to see what types of errors your classifier is making. If errors are concentrated into particular cells, it could indicate the kind of data that your classifier struggles with. Write a function to take a list of test observations, and output a numpy array that represents your classifier's confusion matrix."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 44,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "def get_confusion_matrix(\n",
                "    labels: list[str], predictions: list[str], lang_map: dict[str, np.ndarray]\n",
                ") -> np.ndarray:\n",
                "    \"\"\"\n",
                "    Computes the confusion matrix for the given labels and predictions.\n",
                "\n",
                "    Parameters\n",
                "    ----------\n",
                "    labels : list of str\n",
                "        List of actual labels (true language classes).\n",
                "    predictions : list of str\n",
                "        List of predicted labels (predicted language classes).\n",
                "    lang_map : dict\n",
                "        Dictionary mapping language names to one-hot vectors.\n",
                "\n",
                "    Returns\n",
                "    -------\n",
                "    np.ndarray\n",
                "        A confusion matrix of shape (k, k), where k is the number of classes.\n",
                "    \"\"\"\n",
                "    # Number of classes (languages)\n",
                "    num_classes = len(lang_map)\n",
                "\n",
                "    # Create an empty confusion matrix\n",
                "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
                "\n",
                "    # Inverse mapping from one-hot vector to language index\n",
                "    inverse_lang_map = {np.argmax(v): k for k, v in lang_map.items()}\n",
                "\n",
                "    # Create a mapping from language names to indices\n",
                "    lang_indices = {lang: idx for idx, lang in enumerate(lang_map.keys())}\n",
                "\n",
                "    # Populate the confusion matrix\n",
                "    for true_label, predicted_label in zip(labels, predictions):\n",
                "        true_idx = lang_indices[true_label]         # Get the index of the true label\n",
                "        predicted_idx = lang_indices[predicted_label]  # Get the index of the predicted label\n",
                "        confusion_matrix[true_idx, predicted_idx] += 1  # Increment the corresponding cell\n",
                "\n",
                "    return confusion_matrix"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, print your confusion matrix. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 45,
            "metadata": {
                "output_for": "3.2",
                "tags": [
                    "Answer Expected"
                ]
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "array([[ 2,  5,  6,  0, 11,  4],\n",
                            "       [ 7,  4,  9,  4,  4,  4],\n",
                            "       [ 3,  4,  5,  4,  3, 15],\n",
                            "       [ 3,  7,  6,  4, 15,  1],\n",
                            "       [48, 14, 19,  6, 37, 18],\n",
                            "       [ 4, 24,  4, 17,  9, 63]])"
                        ]
                    },
                    "execution_count": 45,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "# the output of this cell will be used for test 3.2\n",
                "get_confusion_matrix(test_labels, test_predictions, lang_map)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "From your confusion matrix, how many Hausa examples are misclassified as Swahili? Print each of the misclassified documents on a new line. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 46,
            "metadata": {
                "output_for": "3.3",
                "tags": [
                    "Answer Expected"
                ]
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Number of Hausa examples misclassified as Swahili: 11\n",
                        "Misclassified Hausa documents:\n",
                        "Lokacin da mutanen da su ke kiwon aladun suka ga abin da ya faru, sai suka gudu! Suka ba da rahoton abin da ya faru ga mutanen cikin gari da na wannan kewayen.\n",
                        "Wannan shi ne dalilin shari'ar, domin haske ya zo duniya, amma mutane suka kaunaci duhu fiye da hasken, sabili da ayyukan su na mugunta ne.\n",
                        "Sai aka kawo kansa bisa tire, aka mika wa yarinyar, ta kuwa kai wa mahaifiyarta.\n",
                        "Ta wurin haka Yesu ya bada tabbatacce da kuma ingataccen alkwari.\n",
                        "Domin gaskiya ina gaya maku, har sama da kasa su shude, amma digo ko wasali daya ba za ya shude ba a cikin shari'ar, sai an cika dukan al'amura.\n",
                        "Sa'annan iklisiya dake dukan kasar Yahudiya, Galili da kuma Samariya suka sami salama da kuma ginuwa; suka kuwa ci gaba da tafiya cikin tsoron Ubangiji, da kuma ta'aziyar Ruhu Mai Tsarki, iklisiyar kuwa ta karu da yawan mutane.\n",
                        "Ya ce masu, \"A rubuce yake, cewa Almasihu za ya sha wuya, zai tashi kuma daga matattu a rana ta uku.\n",
                        "Babu shakka, abin da aka maishe shi mai daukaka a da, ba ya da sauran daukaka a wannan fanni, saboda irin daukakar da ta zarce shi.\n",
                        "Da shike muna kewaye da wannan irin kasaitaccen taro na shaidu, sai mu watsar da dukkan abubuwan dake nawaita mana da kuma zunubin da zai saurin makale ma na. Bari a cikin hakuri mu yi tseren da ke gabanmu.\n",
                        "Irin wannan mutumin mai zuciya biyu ne, mara tsai da hankali a dukan hanyoyin sa.\n",
                        "Amma da hankalinsa ya dawo, ya ce, 'Barorin mahaifina su nawa ne da suke da abinci isasshe, amma ina nan a nan, ina mutuwa sabili da yunwa!\n"
                    ]
                }
            ],
            "source": [
                "# Assuming lang_map is already defined as in previous examples\n",
                "lang_indices = {lang: idx for idx, lang in enumerate(lang_map.keys())}\n",
                "\n",
                "# Get the index for Hausa (actual class) and Swahili (predicted class)\n",
                "hausa_idx = lang_indices['hausa']\n",
                "swahili_idx = lang_indices['swahili']\n",
                "\n",
                "# Step 1: Compute the confusion matrix\n",
                "confusion_matrix = get_confusion_matrix(test_labels, test_predictions, lang_map)\n",
                "\n",
                "# Step 2: Print the number of misclassified Hausa examples as Swahili\n",
                "num_misclassified_hausa_as_swahili = confusion_matrix[hausa_idx, swahili_idx]\n",
                "print(f\"Number of Hausa examples misclassified as Swahili: {num_misclassified_hausa_as_swahili}\")\n",
                "\n",
                "# Step 3: Print the misclassified documents\n",
                "print(\"Misclassified Hausa documents:\")\n",
                "for true_label, predicted_label, document in zip(test_labels, test_predictions, test_documents):\n",
                "    if true_label == 'hausa' and predicted_label == 'swahili':\n",
                "        print(document)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Can you formulate a hypothesis for why these Hausa examples are being misclassified as Swahili? Peruse the Wikipedia pages of the two languages, and inspect the data and features of the two languages. Consider reasons based in what you know about the languages, and about machine learning. Try to come up with 2-3 experiments you might run to validate your hypothesis."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Examining Feature Weights\n",
                "\n",
                "The classifier that we've built for softmax classification behaves in many ways like using several binary softmax classifiers stacked together. The $K \\times N$ feature matrix can interpreted as a $1 \\times N$ feature vector for each class. In this section, we'll examine the weights for a few of our classified languages. To get feature names out of these vectors, we'll construct an inverted feature map, that maps indices in the feature vectors back to n-grams. Then, extract the $1 \\times N$ vector that corresponds to Hausa. Print the shape of the Hausa feature vector."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 47,
            "metadata": {
                "output_for": "4.1",
                "tags": [
                    "Answer Expected"
                ]
            },
            "outputs": [],
            "source": [
                "# the output of this cell will be used for test 4.1\n",
                "# Invert the feature map\n",
                "inverted_feature_map = {v: k for k, v in feature_map.items()}\n",
                "\n",
                "# your code here"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, let's find the features that are most strongly predictive of an instance being Hausa. From the Hausa feature vector, find the names of the features with the top-10 positive values. Print your results with a tuple of (feature name, feature_weight) on each line:\n",
                "```\n",
                "(\"aaa\", 0.04597442104739769)\n",
                "(\"bbb\", 0.03454984682736487)\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {
                "output_for": "4.2",
                "tags": [
                    "Answer Expected"
                ]
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Hausa Feature Vector Shape: (7941,)\n"
                    ]
                }
            ],
            "source": [
                "# Step 1: Invert the feature map\n",
                "inverted_feature_map = {v: k for k, v in feature_map.items()}\n",
                "\n",
                "# Step 2: Get the index for Hausa in the lang_map\n",
                "hausa_index = np.argmax(lang_map['hausa'])\n",
                "\n",
                "# Step 3: Extract the Hausa feature vector from the weight matrix W_inspect\n",
                "hausa_feature_vector = W_inspect[hausa_index]\n",
                "\n",
                "# Step 4: Print the shape of the Hausa feature vector\n",
                "print(\"Hausa Feature Vector Shape:\", hausa_feature_vector.shape)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, for each language, print the top-10 features. Print the name of each language, and then a list of it's top-10 features on the following line, e.g.\n",
                "\n",
                "```\n",
                "hausa\n",
                "[(\"aaa\", 0.04597442104739769)...]\n",
                "indonesian\n",
                "[(\"aaa\", 0.04597442104739769)...]\n",
                "```"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 49,
            "metadata": {
                "output_for": "4.3",
                "tags": [
                    "Answer Expected"
                ]
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "hausa\n",
                        "[('tod', 3.646713273175252), ('t.\\n', 3.5816502349736314), ('gva', 3.464785690623641), ('zo ', 3.459363955105436), ('ife', 3.3587871235500426), ('r-m', 3.2774736330916565), ('sip', 3.19592774908268), ('gup', 3.1950040129726553), ('g F', 3.113817370345581), ('g-l', 3.09750009884081)]\n",
                        "indonesian\n",
                        "[('rek', 3.5191118566068207), ('abe', 3.505960683174251), ('ioi', 3.2142693863097693), ('Huf', 3.13780221163755), ('r n', 3.137749347063636), ('ahO', 3.109421152855386), ('iwe', 3.0987932683152586), ('\\to ', 3.095725230014659), ('b d', 3.054443705516728), ('den', 3.0192616540537025)]\n",
                        "manobo\n",
                        "[('iok', 3.739655242987972), ('“Wa', 3.496477252822731), ('e j', 3.3231564585068383), ('Bum', 3.3216921501354784), ('Sek', 3.2872546068788004), ('wda', 3.2708411771800976), ('y s', 3.2414787221666943), ('ehi', 3.2004776865752507), ('rpi', 3.1848251834993015), ('Kud', 3.1435733557055667)]\n",
                        "nahuatl\n",
                        "[('u?”', 3.5861123394136967), ('zao', 3.546088757018726), ('t H', 3.2907012834909937), ('p. ', 3.2347044501484867), ('osé', 3.186818782621645), ('oen', 3.1321994652859506), ('-u ', 3.073072364842786), ('in?', 3.07256079182195), (' md', 3.00379175760004), ('pek', 3.0005867770736914)]\n",
                        "swahili\n",
                        "[('\\tYe', 3.698440442550861), ('kov', 3.4072299927047256), ('inh', 3.242893891465854), ('ogk', 3.219679679945717), ('Ogu', 3.1315294330612367), ('iji', 3.086321066426198), ('ufu', 3.037834938791057), ('\"Al', 3.009558137894095), ('i L', 2.9939324917556975), (':19', 2.9925486409329536)]\n",
                        "tagalog\n",
                        "[('elp', 4.2964819521943785), ('\\tMf', 3.920995088938265), ('Wao', 3.3505326255785834), ('saa', 3.312730997984366), (\"''B\", 3.28845457743939), ('mno', 3.1678901804991373), (' \"n', 3.1502307563574776), ('tsi', 3.104578141911958), ('mul', 3.101376278453302), ('t N', 3.04927259188417)]\n"
                    ]
                }
            ],
            "source": [
                "# Step 1: Invert the feature map (already done in previous cells)\n",
                "inverted_feature_map = {v: k for k, v in feature_map.items()}\n",
                "\n",
                "# Step 2: Iterate through each language in the lang_map\n",
                "for lang, one_hot_vec in lang_map.items():\n",
                "    \n",
                "    # Step 3: Get the index of the language in the weight matrix\n",
                "    lang_index = np.argmax(one_hot_vec)\n",
                "    \n",
                "    # Step 4: Extract the feature vector for the language from the weight matrix W_inspect\n",
                "    feature_vector = W_inspect[lang_index]\n",
                "    \n",
                "    # Step 5: Get the indices of the top 10 features (sorted by feature importance)\n",
                "    top_10_feature_indices = np.argsort(feature_vector)[-10:][::-1]  # Sort in descending order\n",
                "    \n",
                "    # Step 6: Get the top 10 n-grams and their corresponding weights\n",
                "    top_10_features = [(inverted_feature_map[idx], feature_vector[idx]) for idx in top_10_feature_indices]\n",
                "    \n",
                "    # Step 7: Print the language name and its top-10 features\n",
                "    print(lang)\n",
                "    print(top_10_features)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Choose one of the languages, and peruse its Wikipedia page or other reliable resources to learn a bit more about the language. Do the top 10 features in that language make sense given what you've learned about the structure of the language? Why or why not? Again, consider reasons stemming both from what you've learned about the language, as well as machine learning and multinomial logistic regression specifically. "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Extra Credit\n",
                "\n",
                "Implement at least one of the experiments you devised to test your hypothesis regarding misclassification of Hausa as Swahili. In order to get credit you must submit not just the code and results, but also clearly describe your hypothesis, the experiment, and why the experiment is suitable to test your hypothesis. Full credit will be given to hypotheses and experiments that are well thought out, explained clearly and convincingly, and backed up with suitable evidence (computed or cited)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 50,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Top 10 Hausa n-grams: [('da ', 176), (' da', 163), (' ya', 116), ('a k', 111), ('in ', 107), ('ya ', 105), ('an ', 104), (' ka', 104), ('ka ', 94), ('a s', 82)]\n",
                        "Top 10 Swahili n-grams: [('wa ', 832), (' wa', 765), ('a k', 715), ('na ', 667), (' na', 555), ('a m', 525), (' ya', 469), (' ku', 448), ('ya ', 384), (' kw', 351)]\n",
                        "Overlapping n-grams between Hausa and Swahili: {'ya ', 'a k', ' ya'}\n",
                        "Number of overlapping n-grams: 3\n"
                    ]
                }
            ],
            "source": [
                "from collections import Counter\n",
                "\n",
                "def extract_top_ngrams(documents: list[str], n: int, top_k: int = 10) -> list[tuple[str, int]]:\n",
                "    \"\"\"\n",
                "    Extracts the most frequent n-grams from a list of documents.\n",
                "    \n",
                "    Parameters\n",
                "    ----------\n",
                "    documents : list of str\n",
                "        List of documents from which to extract n-grams.\n",
                "    n : int\n",
                "        Length of n-grams to extract.\n",
                "    top_k : int\n",
                "        Number of top n-grams to return.\n",
                "    \n",
                "    Returns\n",
                "    -------\n",
                "    list of tuples\n",
                "        A list of the top_k most frequent n-grams and their counts.\n",
                "    \"\"\"\n",
                "    ngram_counter = Counter()\n",
                "    \n",
                "    # Iterate through each document and extract n-grams\n",
                "    for doc in documents:\n",
                "        ngrams = extract_ngrams(doc, n)  # Assuming extract_ngrams is defined elsewhere\n",
                "        ngram_counter.update(ngrams)\n",
                "    \n",
                "    # Return the top_k most common n-grams\n",
                "    return ngram_counter.most_common(top_k)\n",
                "\n",
                "# Swahili와 Hausa 문서에서 상위 n-gram 추출\n",
                "n = 3  # Trigram 사용\n",
                "\n",
                "# Swahili와 Hausa 문서 필터링\n",
                "hausa_documents = [doc for label, doc in train_observations if label == 'hausa']\n",
                "swahili_documents = [doc for label, doc in train_observations if label == 'swahili']\n",
                "\n",
                "# 상위 n-gram 추출\n",
                "top_hausa_ngrams = extract_top_ngrams(hausa_documents, n=n, top_k=10)\n",
                "top_swahili_ngrams = extract_top_ngrams(swahili_documents, n=n, top_k=10)\n",
                "\n",
                "# 결과 출력\n",
                "print(\"Top 10 Hausa n-grams:\", top_hausa_ngrams)\n",
                "print(\"Top 10 Swahili n-grams:\", top_swahili_ngrams)\n",
                "\n",
                "# Swahili와 Hausa의 공통 n-gram 확인\n",
                "hausa_ngram_set = set([ngram for ngram, _ in top_hausa_ngrams])\n",
                "swahili_ngram_set = set([ngram for ngram, _ in top_swahili_ngrams])\n",
                "overlap = hausa_ngram_set & swahili_ngram_set\n",
                "\n",
                "print(\"Overlapping n-grams between Hausa and Swahili:\", overlap)\n",
                "print(\"Number of overlapping n-grams:\", len(overlap))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Hypothesis:\n",
                "The reason Hausa is frequently misclassified as Swahili may be due to shared n-gram patterns between the two languages. Historically, both languages have undergone significant linguistic borrowing, particularly from languages like Arabic and English. This overlap could lead to the classifier being confused, as the model might learn similar feature representations for both languages.\n",
                "\n",
                "## Experiment Process:\n",
                "To test this hypothesis, I will run an experiment focused on analyzing the n-gram overlap between Swahili and Hausa. The steps are as follows:\n",
                "\n",
                "1. Extract Top n-grams: First, I'll extract the top 10 most frequent n-grams from the training data for both Swahili and Hausa.\n",
                "\n",
                "2. Analyze Common n-grams: I'll then compare the n-grams from both languages to identify how many of these n-grams are shared between the two languages.\n",
                "\n",
                "3. Interpret the Results: If we find significant overlap in the n-grams between Swahili and Hausa, this would support the hypothesis that the model struggles to differentiate between the two due to shared features.\n",
                "\n",
                "## Suitability of the Experiment:\n",
                "This experiment is suitable for testing the hypothesis because:\n",
                "\n",
                "1. Linguistic Similarity: Both Swahili and Hausa have historically borrowed words from common sources, like Arabic, so it’s reasonable to expect some overlap in n-gram patterns. By quantifying this overlap, we can test if this linguistic similarity contributes to the classifier’s confusion.\n",
                "\n",
                "2. Model Behavior: In multinomial logistic regression, n-grams are treated as features that help classify languages. If Swahili and Hausa share a large number of top n-grams, it would make it harder for the model to draw distinct decision boundaries. Thus, analyzing n-gram overlap directly helps explain the model's behavior and why misclassifications might occur."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.8.13 ('nlphw03')",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.3"
        },
        "orig_nbformat": 4,
        "vscode": {
            "interpreter": {
                "hash": "9b5c567e963c4c8a4f65edac5dbbc9d8f011f564e4ce8a416ba289a7b299ae77"
            }
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
