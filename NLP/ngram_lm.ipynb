{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "25d624c1",
      "metadata": {
        "id": "25d624c1"
      },
      "source": [
        "# Homework 2: Language Modeling\n",
        "11-411/611 Natural Language Processing (Fall 2024)\n",
        "\n",
        "- RELEASED: Tuesday, Oct 1, 2024\n",
        "- DUE: Thursday, October 24 2024 11:59 pm EDT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8710365b",
      "metadata": {
        "id": "8710365b"
      },
      "source": [
        "Whether for transcribing spoken utterances as correct word sequences or generating coherent human-like text, language models are extremely useful.\n",
        "\n",
        "In this assignment, you will be building your own language models powered by n-grams and RNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e485bc65",
      "metadata": {
        "id": "e485bc65"
      },
      "source": [
        "### Submission Guidelines\n",
        "**Programming:** \n",
        "- This notebook contains helpful test cases and additional information about the programming part of the HW. However, you are only required to submit `ngram_lm.py` and `rnn_lm.py` on Gradescope.\n",
        "- We recommended that you first code in the notebook and then copy the corresponding methods/classes to `ngram_lm.py` and `rnn_lm.py`.\n",
        "\n",
        "**Written:**\n",
        "- Analysis questions would require you to run your code.\n",
        "- You need to write your answers in a document and upload it alongside the programming components"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wjvLk7XPMxnl",
      "metadata": {
        "id": "wjvLk7XPMxnl"
      },
      "source": [
        "### Upload (if using Colab) main.py and utils.py, and the data.zip file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bmEQByNzNI4d",
      "metadata": {
        "id": "bmEQByNzNI4d"
      },
      "outputs": [],
      "source": [
        "!unzip data.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f84134d",
      "metadata": {
        "id": "1f84134d"
      },
      "source": [
        "## Part 1: Language Models [60 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0b3c6b",
      "metadata": {
        "id": "4d0b3c6b"
      },
      "source": [
        "### Step 0: Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "30667bf4",
      "metadata": {
        "id": "30667bf4"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3a6eedf",
      "metadata": {
        "id": "b3a6eedf"
      },
      "source": [
        "We provide you with a few functions in `utils.py` to read and preprocess your input data. Do not edit this file!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "195db6a3",
      "metadata": {
        "id": "195db6a3"
      },
      "outputs": [],
      "source": [
        "from utils import *"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4ea47b0",
      "metadata": {
        "id": "f4ea47b0"
      },
      "source": [
        "We have performed a round of preprocessing on the datasets.\n",
        "\n",
        "- Each file contains one sentence per line.\n",
        "- All punctuation marks have been removed.\n",
        "- Each line is a sequences of tokens separated by whitespace."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fbe98cc",
      "metadata": {
        "id": "5fbe98cc"
      },
      "source": [
        "#### Special Symbols ( Already defined in `utils.py` )\n",
        "The start and end tokens will act as padding to the given sentences, to make sure they are correctly defined, print them here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "9ed9e54f",
      "metadata": {
        "id": "9ed9e54f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence START symbol: <s>\n",
            "Sentence END symbol: </s>\n",
            "Unknown word symbol: <UNK>\n"
          ]
        }
      ],
      "source": [
        "print(\"Sentence START symbol: {}\".format(START))\n",
        "print(\"Sentence END symbol: {}\".format(EOS))\n",
        "print(\"Unknown word symbol: {}\".format(UNK))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b1f4a6f",
      "metadata": {
        "id": "6b1f4a6f"
      },
      "source": [
        "#### Reading and processing an example file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "d60ce7c2",
      "metadata": {
        "id": "d60ce7c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['We are never ever ever ever ever getting back together\\n', 'We are the ones together we are back']\n"
          ]
        }
      ],
      "source": [
        "# Read the sample file\n",
        "sample = read_file(\"data/sample.txt\")\n",
        "print(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "4ec373cc",
      "metadata": {
        "id": "4ec373cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<s>', '<s>', 'we', 'are', 'never', 'ever', 'ever', 'ever', 'ever', 'getting', 'back', 'together', '</s>']\n",
            "['<s>', '<s>', 'we', 'are', 'the', 'ones', 'together', 'we', 'are', 'back', '</s>']\n"
          ]
        }
      ],
      "source": [
        "# Preprocess the content to add corresponding number of start and end tokens. Try out the method with n = 3 and n = 4 as well.\n",
        "# Preprocessing example for bigrams (n=2)\n",
        "sample = preprocess(sample, n=3)\n",
        "for s in sample:\n",
        "    print(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "36a2a96e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<s>', '<s>', 'we', 'are', 'never', 'ever', 'ever', 'ever', 'ever', 'getting', 'back', 'together', '</s>', '<s>', '<s>', 'we', 'are', 'the', 'ones', 'together', 'we', 'are', 'back', '</s>']\n"
          ]
        }
      ],
      "source": [
        "# Flattens a nested list into a 1D list.\n",
        "flattened = flatten(sample)\n",
        "print(flattened)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9165b404",
      "metadata": {
        "id": "9165b404"
      },
      "source": [
        "### Step 1: N-Gram Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a4f73b0",
      "metadata": {
        "id": "5a4f73b0"
      },
      "source": [
        "#### TODO: Defining `get_ngrams()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "138c35b6",
      "metadata": {
        "id": "138c35b6"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TODO: get_ngrams()\n",
        "#######################################\n",
        "def get_ngrams(list_of_words, n):\n",
        "    \"\"\"\n",
        "    Returns a list of n-grams for a list of words.\n",
        "    Args\n",
        "    ----\n",
        "    list_of_words: List[str]\n",
        "        List of already preprocessed and flattened (1D) list of tokens e.g. [\"<s>\", \"hello\", \"</s>\", \"<s>\", \"bye\", \"</s>\"]\n",
        "    n: int\n",
        "        n-gram order e.g. 1, 2, 3\n",
        "    \n",
        "    Returns:\n",
        "        n_grams: List[Tuple]\n",
        "            Returns a list containing n-gram tuples\n",
        "    \"\"\"\n",
        "    n_grams = []\n",
        "    for i in range(len(list_of_words) - n + 1):\n",
        "        n_gram = tuple(list_of_words[i:i + n])\n",
        "        n_grams.append(n_gram)\n",
        "    return n_grams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "5fdab35a",
      "metadata": {
        "id": "5fdab35a"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TEST: get_ngrams()\n",
        "#######################################\n",
        "sample = preprocess(read_file(\"data/sample.txt\"), n=3)\n",
        "flattened = flatten(sample)\n",
        "\n",
        "assert get_ngrams(flattened, 3) == [('<s>', '<s>', 'we'),\n",
        "        ('<s>', 'we', 'are'),\n",
        "        ('we', 'are', 'never'),\n",
        "        ('are', 'never', 'ever'),\n",
        "        ('never', 'ever', 'ever'),\n",
        "        ('ever', 'ever', 'ever'),\n",
        "        ('ever', 'ever', 'ever'),\n",
        "        ('ever', 'ever', 'getting'),\n",
        "        ('ever', 'getting', 'back'),\n",
        "        ('getting', 'back', 'together'),\n",
        "        ('back', 'together', '</s>'),\n",
        "        ('together', '</s>', '<s>'),\n",
        "        ('</s>', '<s>', '<s>'),\n",
        "        ('<s>', '<s>', 'we'),\n",
        "        ('<s>', 'we', 'are'),\n",
        "        ('we', 'are', 'the'),\n",
        "        ('are', 'the', 'ones'),\n",
        "        ('the', 'ones', 'together'),\n",
        "        ('ones', 'together', 'we'),\n",
        "        ('together', 'we', 'are'),\n",
        "        ('we', 'are', 'back'),\n",
        "        ('are', 'back', '</s>')]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bbb4a88",
      "metadata": {
        "id": "7bbb4a88"
      },
      "source": [
        "#### **TODO:** Class `NGramLanguageModel()`\n",
        "\n",
        "*Now*, we will define our LanguageModel class.\n",
        "\n",
        "**Some Useful Variables:**\n",
        "- self.model: `dict` of n-grams and their corresponding probabilities, keys being the tuple containing the n-gram, and the value being the probability of the n-gram.\n",
        "- self.vocab: `dict` of unigram vocabulary with counts, keys being the words themselves and the values being their frequency.\n",
        "- self.n: `int` value for n-gram order (e.g. 1, 2, 3).\n",
        "- self.train_data: `List[List]` containing preprocessed **unflattened** train sentences. You will have to flatten it to use in the language model\n",
        "- self.smoothing: `float` flag signifying the smoothing parameter.\n",
        "\n",
        "In `lm.py`, we will be taking most of these argumemts from the command line using this command:\n",
        "\n",
        "`python3 lm.py --train data/sample.txt --test data/sample.txt --n 3 --smoothing 0 --min_freq 1`\n",
        "\n",
        "Note that we will not be using log probabilities in this section. Store the probabilities as they are, not in log space.\n",
        "\n",
        "**Laplace Smoothing**\n",
        "\n",
        "There are two ways to perform this:\n",
        "- Either you calculate all possible n-grams at train time and calculate smooth probabilities for all of them, hence inflating the model (eager emoothing). You then use the probabilities as when required at test time. **OR**\n",
        "- You calculate the probabilities for the **observed n-grams** at train time, using the smoothed likelihood formula, then if any unseen n-gram is observed at test time, you calculate the probability using the smoothed likelihood formula and store it in the model for future use (lazy smoothing).\n",
        "\n",
        "You will be implementing lazy smoothing\n",
        "\n",
        "**Perplexity**\n",
        "\n",
        "Steps:\n",
        "1. Flatten the test data.\n",
        "2. Extract ngrams from the flattened data.\n",
        "3. Calculate perplexity according to given formula. For unseen n-grams, calculate using smoothed likelihood and store the unseen n-gram probability in the labguage model `model` attribute:\n",
        "\n",
        "$ppl(W_{test}) = ppl(W_1W_2 ... W_n)^{-1/n} $\n",
        "\n",
        "Tips:\n",
        "- Remember that product changes to summation under `log`. Take the log of probabilities, sum them up, and then exponentiate it to get back to the original scale.\n",
        "- Make sure to `flatten()` your data before creating the n_grams using `get_ngrams()`.\n",
        "- The test suite provided is **not exhaustive**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "24f2ce5c",
      "metadata": {
        "id": "24f2ce5c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class NGramLanguageModel():\n",
        "    def __init__(self, n, train_data, alpha=1):\n",
        "        self.n = n\n",
        "        self.train_data = train_data\n",
        "        self.alpha = alpha\n",
        "        self.tokens = []  # 단어 토큰 리스트\n",
        "        self.vocab = {}   # 단어 집합 (단어별 등장 횟수)\n",
        "        self.model = {}   # n-gram 모델 (n-gram -> 확률)\n",
        "        self.n_grams_counts = {}  # n-gram 빈도수\n",
        "        self.prefix_counts = {}   # (n-1)-gram 빈도수\n",
        "        self.build()  # 모델을 초기화할 때 build 함수 호출\n",
        "\n",
        "    def build(self):\n",
        "        flattened_data = flatten(self.train_data)\n",
        "        self.tokens = flattened_data\n",
        "\n",
        "        # 단어 등장 횟수 계산\n",
        "        for word in flattened_data:\n",
        "            self.vocab[word] = self.vocab.get(word, 0) + 1\n",
        "\n",
        "        # n-grams 및 (n-1)-grams 생성 및 빈도수 계산\n",
        "        ngrams = get_ngrams(flattened_data, self.n)\n",
        "        for ngram in ngrams:\n",
        "            self.n_grams_counts[ngram] = self.n_grams_counts.get(ngram, 0) + 1\n",
        "            prefix = ngram[:-1]  # n-gram의 (n-1)-gram 부분\n",
        "            self.prefix_counts[prefix] = self.prefix_counts.get(prefix, 0) + 1\n",
        "\n",
        "        # 모델 확률 계산\n",
        "        for ngram in self.n_grams_counts:\n",
        "            self.model[ngram] = self.get_prob(ngram)\n",
        "\n",
        "    def get_prob(self, ngram):\n",
        "        \"\"\"\n",
        "        Returns the probability of the n-gram using Laplace Smoothing.\n",
        "        \"\"\"\n",
        "        if self.n == 1:\n",
        "            return (self.n_grams_counts.get(ngram, 0) + self.alpha) / (len(self.tokens) + self.alpha * len(self.vocab))\n",
        "        else:\n",
        "            prefix = ngram[:-1]\n",
        "            prefix_count = self.prefix_counts.get(prefix, 0)\n",
        "            ngram_count = self.n_grams_counts.get(ngram, 0)\n",
        "            \n",
        "            # 관찰되지 않은 n-gram에 대해 smoothing 적용\n",
        "            prob = (ngram_count + self.alpha) / (prefix_count + self.alpha * len(self.vocab))\n",
        "            \n",
        "            # 확률이 너무 작으면 최소 값을 보정하여 설정\n",
        "            return max(prob, 1e-3)\n",
        "\n",
        "    def perplexity(self, test_data):\n",
        "        \"\"\"\n",
        "        Calculates perplexity on the test data.\n",
        "        \"\"\"\n",
        "        flattened_test = flatten(test_data)  # 데이터를 평탄화\n",
        "        test_ngrams = get_ngrams(flattened_test, self.n)  # 테스트 데이터에서 n-gram 추출\n",
        "        \n",
        "        log_prob_sum = 0\n",
        "        N = len(test_ngrams)  # n-gram의 총 개수\n",
        "        \n",
        "        # n-gram이 충분한지 체크 (너무 적으면 퍼플렉서티 값이 왜곡될 수 있음)\n",
        "        if N == 0:\n",
        "            raise ValueError(\"Test data is too small to calculate perplexity.\")\n",
        "        \n",
        "        for ngram in test_ngrams:\n",
        "            prob = self.get_prob(ngram)  # 각 n-gram의 확률 가져오기\n",
        "            \n",
        "            # 확률이 0인 경우를 방지하고 로그 계산\n",
        "            if prob > 0:\n",
        "                log_prob_sum += np.log(prob)\n",
        "            else:\n",
        "                log_prob_sum += np.log(1e-5)  # 확률이 0인 경우 최소값으로 보정\n",
        "        \n",
        "        # perplexity 계산\n",
        "        perplexity = np.exp(-log_prob_sum / N)\n",
        "        return perplexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "909b9c4a",
      "metadata": {
        "id": "909b9c4a"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TEST: NGramLanguageModel()\n",
        "#######################################\n",
        "# For the sake of understanding we will pass alpha as 0 (no smoothing), so that you gain intuition about the probabilities\n",
        "sample = preprocess(read_file(\"data/sample.txt\"), n=2)\n",
        "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=0)\n",
        "\n",
        "assert test_lm.vocab == Counter({'<s>': 2,\n",
        "        'we': 3,\n",
        "        'are': 3,\n",
        "        'never': 1,\n",
        "        'ever': 4,\n",
        "        'getting': 1,\n",
        "        'back': 2,\n",
        "        'together': 2,\n",
        "        '</s>': 2,\n",
        "        'the': 1,\n",
        "        'ones': 1})\n",
        "\n",
        "assert test_lm.model =={('<s>', 'we'): 1.0,\n",
        "        ('we', 'are'): 1.0,\n",
        "        ('are', 'never'): 0.3333333333333333,\n",
        "        ('never', 'ever'): 1.0,\n",
        "        ('ever', 'ever'): 0.75,\n",
        "        ('ever', 'getting'): 0.25,\n",
        "        ('getting', 'back'): 1.0,\n",
        "        ('back', 'together'): 0.5,\n",
        "        ('together', '</s>'): 0.5,\n",
        "        ('</s>', '<s>'): 1.0,\n",
        "        ('are', 'the'): 0.3333333333333333,\n",
        "        ('the', 'ones'): 1.0,\n",
        "        ('ones', 'together'): 1.0,\n",
        "        ('together', 'we'): 0.5,\n",
        "        ('are', 'back'): 0.3333333333333333,\n",
        "        ('back', '</s>'): 0.5}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "501ac225",
      "metadata": {
        "id": "501ac225"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TEST smoothing: NGramLanguageModel()\n",
        "#######################################\n",
        "sample = preprocess(read_file(\"data/sample.txt\"), n=2)\n",
        "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=1)\n",
        "\n",
        "assert test_lm.vocab == Counter({'<s>': 2,\n",
        "        'we': 3,\n",
        "        'are': 3,\n",
        "        'never': 1,\n",
        "        'ever': 4,\n",
        "        'getting': 1,\n",
        "        'back': 2,\n",
        "        'together': 2,\n",
        "        '</s>': 2,\n",
        "        'the': 1,\n",
        "        'ones': 1})\n",
        "\n",
        "assert test_lm.model =={('<s>', 'we'): 0.23076923076923078,\n",
        "        ('we', 'are'): 0.2857142857142857,\n",
        "        ('are', 'never'): 0.14285714285714285,\n",
        "        ('never', 'ever'): 0.16666666666666666,\n",
        "        ('ever', 'ever'): 0.26666666666666666,\n",
        "        ('ever', 'getting'): 0.13333333333333333,\n",
        "        ('getting', 'back'): 0.16666666666666666,\n",
        "        ('back', 'together'): 0.15384615384615385,\n",
        "        ('together', '</s>'): 0.15384615384615385,\n",
        "        ('</s>', '<s>'): 0.16666666666666666,\n",
        "        ('are', 'the'): 0.14285714285714285,\n",
        "        ('the', 'ones'): 0.16666666666666666,\n",
        "        ('ones', 'together'): 0.16666666666666666,\n",
        "        ('together', 'we'): 0.15384615384615385,\n",
        "        ('are', 'back'): 0.14285714285714285,\n",
        "        ('back', '</s>'): 0.15384615384615385}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "157b0749",
      "metadata": {
        "id": "157b0749"
      },
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TEST unigram: NGramLanguageModel()\n",
        "#######################################\n",
        "sample = preprocess(read_file(\"data/sample.txt\"), n=1)\n",
        "test_lm = NGramLanguageModel(n=1, train_data=sample, alpha=1)\n",
        "\n",
        "assert test_lm.vocab == Counter({'<s>': 2,\n",
        "        'we': 3,\n",
        "        'are': 3,\n",
        "        'never': 1,\n",
        "        'ever': 4,\n",
        "        'getting': 1,\n",
        "        'back': 2,\n",
        "        'together': 2,\n",
        "        '</s>': 2,\n",
        "        'the': 1,\n",
        "        'ones': 1})\n",
        "\n",
        "assert test_lm.model == {('<s>',): 0.09090909090909091,\n",
        "        ('we',): 0.12121212121212122,\n",
        "        ('are',): 0.12121212121212122,\n",
        "        ('never',): 0.06060606060606061,\n",
        "        ('ever',): 0.15151515151515152,\n",
        "        ('getting',): 0.06060606060606061,\n",
        "        ('back',): 0.09090909090909091,\n",
        "        ('together',): 0.09090909090909091,\n",
        "        ('</s>',): 0.09090909090909091,\n",
        "        ('the',): 0.06060606060606061,\n",
        "        ('ones',): 0.06060606060606061}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "2927e9aa",
      "metadata": {
        "id": "2927e9aa"
      },
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m test_lm \u001b[38;5;241m=\u001b[39m NGramLanguageModel(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, train_data\u001b[38;5;241m=\u001b[39msample, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     10\u001b[0m test_ppl \u001b[38;5;241m=\u001b[39m test_lm\u001b[38;5;241m.\u001b[39mperplexity(sample)\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m test_ppl \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m5.0\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m test_ppl \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#######################################\n",
        "# TEST: perplexity()\n",
        "#######################################\n",
        "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=0)\n",
        "test_ppl = test_lm.perplexity(sample)\n",
        "assert test_ppl < 1.7\n",
        "assert test_ppl > 0\n",
        "\n",
        "test_lm = NGramLanguageModel(n=2, train_data=sample, alpha=1)\n",
        "test_ppl = test_lm.perplexity(sample)\n",
        "assert test_ppl < 5.0\n",
        "assert test_ppl > 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a552b291",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculated perplexity: 5.283124177782943\n"
          ]
        }
      ],
      "source": [
        "# Perplexity 값 출력\n",
        "test_ppl = test_lm.perplexity(sample)\n",
        "print(\"Calculated perplexity:\", test_ppl)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06fda0b2",
      "metadata": {},
      "source": [
        "### Step 2: RNN Language Model\n",
        "Recurrent Neural Networks (RNNs) are a class of neural networks designed to handle sequential data. Unlike traditional neural networks, which assume independence among inputs, RNNs utilize their internal state (memory) to process sequences of inputs. This makes them particularly well-suited for tasks where context and order matter.\n",
        "\n",
        "Before diving into building RNN Language Models using PyTorch, it's essential to have a solid foundation in the following areas:\n",
        ". We assume you have had a basic understanding of PyTorch and its core concepts, including tensors, autograd, modules (nn.Module), and how to construct simple neural networks using PyTorch. For more comprehensive learning, refer to the [PyTorch official tutorials](https://pytorch.org/tutorials/) and documentation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "454617c7",
      "metadata": {},
      "source": [
        "#### Preparing the Data\n",
        "The following Python code is used for loading and processing [GloVe (Global Vectors for Word Representation) embeddings](https://nlp.stanford.edu/projects/glove/). GloVe is an unsupervised learning algorithm for obtaining vector representations for words. These embeddings can be used in various natural language processing and machine learning tasks. You can download the 50d embeddings for this assignment from [Canvas](https://canvas.cmu.edu/courses/39596/files/10855662?module_item_id=5748476).\n",
        "\n",
        "The `load_glove_embeddings(path)` function is used to load the GloVe embeddings from a file. The function takes a file path as an argument, reads the file line by line, and for each line, it splits the line into words and their corresponding embeddings, and stores them in a dictionary. The dictionary, embeddings_dict, maps words to their corresponding vector representations.\n",
        "\n",
        "The `create_embedding_matrix(word_to_ix, embeddings_dict, embedding_dim)` function is used to create an embedding matrix from the loaded GloVe embeddings. This function takes a dictionary mapping words to their indices (`word_to_ix`), the dictionary of GloVe embeddings (`embeddings_dict`), and the dimension of the embeddings (`embedding_dim`) as arguments. It creates a zero matrix of size (vocab_size, embedding_dim) and then for each word in  `word_to_ix`, it checks if the word is in `embeddings_dict`. If it is, it assigns the corresponding GloVe vector to the word's index in the embedding matrix. If the word is not in the embeddings_dict, it assigns a random vector to the word's index in the embedding matrix.\n",
        "\n",
        "The `glove_path` variable is the path to the GloVe file, and `glove_embeddings` is the dictionary of GloVe embeddings loaded using the `load_glove_embeddings` function. The `embedding_dim` variable is the dimension of the embeddings, and `embedding_matrix` is the embedding matrix created using the create_embedding_matrix function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a4411999",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the data\n",
        "vocab, word_to_ix, ix_to_word, dataloader = loadfile(\"data/sample.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9e9b6d54",
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_glove_embeddings(path):\n",
        "    embeddings_dict = {}\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = torch.tensor([float(val) for val in values[1:]], dtype=torch.float)\n",
        "            embeddings_dict[word] = vector\n",
        "    return embeddings_dict\n",
        "\n",
        "# Path to the GloVe file\n",
        "glove_path = 'glove.6B.50d.txt'  # Update this path\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n",
        "\n",
        "def create_embedding_matrix(word_to_ix, embeddings_dict, embedding_dim):\n",
        "    vocab_size = len(word_to_ix)\n",
        "    embedding_matrix = torch.zeros((vocab_size, embedding_dim))\n",
        "    for word, ix in word_to_ix.items():\n",
        "        if word in embeddings_dict:\n",
        "            embedding_matrix[ix] = embeddings_dict[word]\n",
        "        else:\n",
        "            embedding_matrix[ix] = torch.rand(embedding_dim)  # Random initialization for words not in GloVe\n",
        "    return embedding_matrix\n",
        "\n",
        "# Create the embedding matrix\n",
        "embedding_dim = 50\n",
        "embedding_matrix = create_embedding_matrix(word_to_ix, glove_embeddings, embedding_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cbade19",
      "metadata": {},
      "source": [
        "#### TODO: Defining the RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bc88721e",
      "metadata": {},
      "outputs": [],
      "source": [
        "#######################################\n",
        "# TODO: RNNLanguageModel()\n",
        "#######################################\n",
        "class RNNLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n",
        "        super(RNNLanguageModel, self).__init__()\n",
        "        \n",
        "        # 임베딩 레이어: 사전 학습된 GloVe 임베딩으로 초기화\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight.data.copy_(embedding_matrix)  # GloVe 임베딩으로 초기화\n",
        "        self.embedding.weight.requires_grad = False  # GloVe 임베딩 고정\n",
        "        \n",
        "        # RNN 레이어: hidden_dim 크기의 RNN 레이어 정의\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
        "        \n",
        "        # Fully connected 레이어: RNN 출력에서 vocabulary 크기의 출력으로 변환\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "    \n",
        "    def forward(self, x, hidden=None):\n",
        "        \"\"\"\n",
        "        RNN 모델의 순방향 패스.\n",
        "        \n",
        "        Args\n",
        "        ____\n",
        "        x: torch.Tensor (batch_size, sequence_length)\n",
        "        hidden: torch.Tensor (num_layers, batch_size, hidden_dim)\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        out: torch.Tensor (batch_size, sequence_length, vocab_size)\n",
        "        hidden: torch.Tensor (num_layers, batch_size, hidden_dim)\n",
        "        \"\"\"\n",
        "        # 1. 입력을 임베딩으로 변환\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # 2. RNN 레이어를 통과\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        \n",
        "        # 3. RNN 출력값을 Fully connected 레이어를 통해 vocab 크기의 출력으로 변환\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out, hidden\n",
        "    \n",
        "    def generate_sentence(self, sequence, word_to_ix, ix_to_word, num_words, mode='max'):\n",
        "        \"\"\"\n",
        "        주어진 문장에서 다음 단어들을 예측하는 함수.\n",
        "        \n",
        "        Args\n",
        "        ____\n",
        "        sequence: str\n",
        "            입력 문장\n",
        "        word_to_ix: dict\n",
        "            단어 -> 인덱스 사전\n",
        "        ix_to_word: dict\n",
        "            인덱스 -> 단어 사전\n",
        "        num_words: int\n",
        "            예측할 단어의 최대 개수\n",
        "        mode: str\n",
        "            'max' 또는 'multinomial'\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        predicted_sequence: List[str]\n",
        "            예측된 단어 리스트\n",
        "        \"\"\"\n",
        "        # 입력 문장을 인덱스로 변환\n",
        "        input_idx = [word_to_ix[word] for word in sequence.split() if word in word_to_ix]\n",
        "        input_tensor = torch.tensor(input_idx).unsqueeze(0)  # (1, sequence_length) 크기의 텐서로 변환\n",
        "        \n",
        "        # 숨겨진 상태 초기화 (배치 크기 1로 설정)\n",
        "        hidden = None\n",
        "        \n",
        "        predicted_sequence = sequence.split()\n",
        "        \n",
        "        for _ in range(num_words):\n",
        "            # 순방향 패스: 단어를 예측\n",
        "            output, hidden = self.forward(input_tensor, hidden)\n",
        "            \n",
        "            # 마지막 단어의 출력에서 다음 단어 예측\n",
        "            last_word_logits = output[0, -1, :]\n",
        "            \n",
        "            if mode == 'max':\n",
        "                # 확률이 가장 높은 단어 선택\n",
        "                predicted_idx = torch.argmax(last_word_logits).item()\n",
        "            elif mode == 'multinomial':\n",
        "                # 확률 분포에서 단어 샘플링\n",
        "                probs = torch.softmax(last_word_logits, dim=0)\n",
        "                predicted_idx = torch.multinomial(probs, 1).item()\n",
        "            else:\n",
        "                raise ValueError(\"Unknown mode: choose 'max' or 'multinomial'\")\n",
        "            \n",
        "            predicted_word = ix_to_word[predicted_idx]\n",
        "            predicted_sequence.append(predicted_word)\n",
        "            \n",
        "            # 다음 예측을 위해 새로운 입력으로 업데이트\n",
        "            input_tensor = torch.tensor([predicted_idx]).unsqueeze(0)  # (1, 1)\n",
        "        \n",
        "        return predicted_sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe6c516d",
      "metadata": {},
      "source": [
        "#### Training the Model\n",
        "The following code snippet provided is responsible for training the RNN language model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "96135209",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Loss: 2.6141231060028076, Perplexity: 13.655236931013835\n",
            "Epoch 2/10, Loss: 2.4388041496276855, Perplexity: 11.459328903039383\n",
            "Epoch 3/10, Loss: 2.288909673690796, Perplexity: 9.864176644404472\n",
            "Epoch 4/10, Loss: 2.1654279232025146, Perplexity: 8.718331895227367\n",
            "Epoch 5/10, Loss: 2.0620663166046143, Perplexity: 7.86219882938823\n",
            "Epoch 6/10, Loss: 1.972102165222168, Perplexity: 7.185766290151172\n",
            "Epoch 7/10, Loss: 1.8897345066070557, Perplexity: 6.617611515661019\n",
            "Epoch 8/10, Loss: 1.8105781078338623, Perplexity: 6.113980951038627\n",
            "Epoch 9/10, Loss: 1.7322709560394287, Perplexity: 5.653478141611421\n",
            "Epoch 10/10, Loss: 1.6541872024536133, Perplexity: 5.228828215779611\n"
          ]
        }
      ],
      "source": [
        "#######################################\n",
        "# TEST: RNNLanguageModel() and training\n",
        "#######################################\n",
        "torch.manual_seed(11411)\n",
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 50\n",
        "hidden_dim = 32\n",
        "num_epochs = 10\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "RNN = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(RNN.parameters(), lr=0.005)\n",
        "\n",
        "lines = \"\"\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        RNN.zero_grad()\n",
        "        output, _ = RNN(inputs)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    line = f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Perplexity: {np.exp(loss.item())}'\n",
        "    lines += line + \"\\n\"\n",
        "    print(line)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14ba8069",
      "metadata": {
        "id": "14ba8069"
      },
      "source": [
        "## Part 2: Written [40 points]. We have given some code for some of the written parts to make it easier for you."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53431996",
      "metadata": {
        "id": "53431996"
      },
      "source": [
        "### **Written 4.2** – Song Attribution [8 points]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8cc310ab",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique 2-grams in business dataset: 83819\n",
            "Unique 3-grams in business dataset: 141220\n",
            "Unique 2-grams in sports dataset: 77398\n",
            "Unique 3-grams in sports dataset: 135644\n",
            "Possible 2-grams: 309126724\n",
            "Possible 3-grams: 5435066061368\n"
          ]
        }
      ],
      "source": [
        "# Load the datasets\n",
        "business_data = read_file('data/bbc/business.txt')\n",
        "sports_data = read_file('data/bbc/sport.txt')\n",
        "\n",
        "# Preprocess the data\n",
        "business_tokens = preprocess(business_data, n=2)  # For 2-grams\n",
        "sports_tokens = preprocess(sports_data, n=2)\n",
        "\n",
        "# Flatten the token lists for easier n-gram extraction\n",
        "flat_business = flatten(business_tokens)\n",
        "flat_sports = flatten(sports_tokens)\n",
        "\n",
        "# Generate 2-grams and 3-grams\n",
        "business_2grams = get_ngrams(flat_business, n=2)\n",
        "business_3grams = get_ngrams(flat_business, n=3)\n",
        "sports_2grams = get_ngrams(flat_sports, n=2)\n",
        "sports_3grams = get_ngrams(flat_sports, n=3)\n",
        "\n",
        "# Count unique n-grams\n",
        "unique_business_2grams = len(set(business_2grams))\n",
        "unique_business_3grams = len(set(business_3grams))\n",
        "unique_sports_2grams = len(set(sports_2grams))\n",
        "unique_sports_3grams = len(set(sports_3grams))\n",
        "\n",
        "# Output results\n",
        "print(f\"Unique 2-grams in business dataset: {unique_business_2grams}\")\n",
        "print(f\"Unique 3-grams in business dataset: {unique_business_3grams}\")\n",
        "print(f\"Unique 2-grams in sports dataset: {unique_sports_2grams}\")\n",
        "print(f\"Unique 3-grams in sports dataset: {unique_sports_3grams}\")\n",
        "\n",
        "# Vocabulary size\n",
        "vocab_size = len(set(flat_business + flat_sports))  # Combine both vocabularies for comparison\n",
        "\n",
        "# Possible n-grams\n",
        "possible_2grams = vocab_size ** 2\n",
        "possible_3grams = vocab_size ** 3\n",
        "\n",
        "print(f\"Possible 2-grams: {possible_2grams}\")\n",
        "print(f\"Possible 3-grams: {possible_3grams}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "4751ea5b",
      "metadata": {
        "id": "4751ea5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "99.16627687793007\n"
          ]
        }
      ],
      "source": [
        "# Example code for Taylor Swift N-Gram LM\n",
        "n = 3\n",
        "smoothing = 0.1\n",
        "min_freq = 1\n",
        "\n",
        "train = read_file(\"data/lyrics/taylor_swift.txt\")\n",
        "test = read_file(\"data/lyrics/test_lyrics.txt\")\n",
        "\n",
        "train = preprocess(train, n)\n",
        "test = preprocess(test, n)\n",
        "lm = NGramLanguageModel(n, train, smoothing)\n",
        "\n",
        "ppl = lm.perplexity(test)\n",
        "print(ppl)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "749a49a0",
      "metadata": {
        "id": "749a49a0"
      },
      "source": [
        "### **Written 4.3.1** –  Intro to Decoding [8 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6949f6a5",
      "metadata": {},
      "source": [
        "Please take a look at and understand the functions: `best_candidate()`, `top_k_best_candidates()` and `generate_sentences_from_phrase()` in `utils.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c8ed9b85",
      "metadata": {
        "id": "c8ed9b85"
      },
      "outputs": [],
      "source": [
        "n = 3\n",
        "smoothing = 0.1\n",
        "min_freq = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "96b3d000",
      "metadata": {
        "id": "96b3d000"
      },
      "outputs": [],
      "source": [
        "train = read_file(\"data/lyrics/taylor_swift.txt\")\n",
        "train = preprocess(train, n)\n",
        "lm = NGramLanguageModel(n, train, smoothing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "17794ab7",
      "metadata": {
        "id": "17794ab7"
      },
      "outputs": [],
      "source": [
        "s1 = (\"the\", \"tortured\", \"poets\", \"department\")\n",
        "\n",
        "s2 = (\"so\", \"long\", \"london\")\n",
        "\n",
        "s3 = (\"down\", \"bad\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "69ef66a2",
      "metadata": {
        "id": "69ef66a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('</s>', 1)\n"
          ]
        }
      ],
      "source": [
        "print(top_k_best_candidates(lm, s1, 5, without=['<s>', '</s>']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "5b6f2563",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Taylor Swift perplexity: 99.16627687793007\n",
            "Green Day perplexity: 235.17079902124422\n",
            "Ed Sheeran perplexity: 225.217257812529\n",
            "The lyricist is most likely Taylor Swift.\n"
          ]
        }
      ],
      "source": [
        "# 주어진 가수별 데이터 파일을 학습\n",
        "taylor_swift_lyrics = read_file(\"data/lyrics/taylor_swift.txt\")\n",
        "green_day_lyrics = read_file(\"data/lyrics/green_day.txt\")\n",
        "ed_sheeran_lyrics = read_file(\"data/lyrics/ed_sheeran.txt\")\n",
        "\n",
        "# 익명의 테스트 가사 파일을 읽어옴\n",
        "test_lyrics = read_file(\"data/lyrics/test_lyrics.txt\")\n",
        "\n",
        "# 각 가수의 n-gram 언어 모델을 생성\n",
        "n = 3\n",
        "alpha = 0.1\n",
        "\n",
        "# Taylor Swift 언어 모델 학습\n",
        "taylor_model = NGramLanguageModel(n, preprocess(taylor_swift_lyrics, n), alpha)\n",
        "\n",
        "# Green Day 언어 모델 학습\n",
        "green_day_model = NGramLanguageModel(n, preprocess(green_day_lyrics, n), alpha)\n",
        "\n",
        "# Ed Sheeran 언어 모델 학습\n",
        "ed_sheeran_model = NGramLanguageModel(n, preprocess(ed_sheeran_lyrics, n), alpha)\n",
        "\n",
        "# 익명의 가사에 대해 perplexity 계산\n",
        "taylor_perplexity = taylor_model.perplexity(preprocess(test_lyrics, n))\n",
        "green_day_perplexity = green_day_model.perplexity(preprocess(test_lyrics, n))\n",
        "ed_sheeran_perplexity = ed_sheeran_model.perplexity(preprocess(test_lyrics, n))\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"Taylor Swift perplexity: {taylor_perplexity}\")\n",
        "print(f\"Green Day perplexity: {green_day_perplexity}\")\n",
        "print(f\"Ed Sheeran perplexity: {ed_sheeran_perplexity}\")\n",
        "\n",
        "# 가장 낮은 perplexity를 가지는 가수 출력\n",
        "if min(taylor_perplexity, green_day_perplexity, ed_sheeran_perplexity) == taylor_perplexity:\n",
        "    print(\"The lyricist is most likely Taylor Swift.\")\n",
        "elif min(taylor_perplexity, green_day_perplexity, ed_sheeran_perplexity) == green_day_perplexity:\n",
        "    print(\"The lyricist is most likely Green Day.\")\n",
        "else:\n",
        "    print(\"The lyricist is most likely Ed Sheeran.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ee4e956",
      "metadata": {
        "id": "8ee4e956"
      },
      "source": [
        "### **Written 4.3.2** – Text Generation [8 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "205129a7",
      "metadata": {
        "id": "205129a7"
      },
      "source": [
        "For this subtask, train an RNN LM using `data/taylor_swift.txt`\n",
        "\n",
        "In this part, we will try the first two approaches to generate sentences.\n",
        "\n",
        "Q1. Use `predict_next_words()` method to generate sentences after the provided phrases from `s1` to `s3`. Use modes `max` and `multinomial`. Report one of your favorite generations (for any strategy or phrase).\n",
        "\n",
        "Q2. Which decoding strategy did you like better and why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7cb85208",
      "metadata": {},
      "outputs": [],
      "source": [
        "s1 = \"the tortured poets department\"\n",
        "\n",
        "s2 = \"so long, london\"\n",
        "\n",
        "s3 = \"down bad\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3c108f15",
      "metadata": {},
      "outputs": [],
      "source": [
        "# <UNK> 토큰 추가 (필요한 경우)\n",
        "if \"<UNK>\" not in word_to_ix:\n",
        "    word_to_ix[\"<UNK>\"] = len(word_to_ix)\n",
        "    ix_to_word[len(ix_to_word)] = \"<UNK>\"\n",
        "\n",
        "def predict_next_words(RNN, phrase, word_to_ix, ix_to_word, num_words=5, mode='max'):\n",
        "    \"\"\"\n",
        "    주어진 문장 뒤에 예측된 단어들을 생성하는 함수.\n",
        "    \n",
        "    Args:\n",
        "        RNN: 학습된 RNN 언어 모델.\n",
        "        phrase: str, 주어진 문장 (예: 'the tortured poets department').\n",
        "        word_to_ix: dict, 단어 -> 인덱스 사전.\n",
        "        ix_to_word: dict, 인덱스 -> 단어 사전.\n",
        "        num_words: int, 예측할 단어의 개수.\n",
        "        mode: str, 'max' 또는 'multinomial' 모드.\n",
        "    \n",
        "    Returns:\n",
        "        predicted_sentence: List[str], 예측된 단어들로 이루어진 리스트.\n",
        "    \"\"\"\n",
        "    RNN.eval()  # 예측 모드로 전환\n",
        "    \n",
        "    # 입력 문장을 인덱스로 변환\n",
        "    input_idx = []\n",
        "    for word in phrase.split():\n",
        "        if word in word_to_ix:\n",
        "            input_idx.append(word_to_ix[word])\n",
        "        else:\n",
        "            input_idx.append(word_to_ix[\"<UNK>\"])  # 사전에 없는 단어는 <UNK>로 처리\n",
        "\n",
        "    # 입력 인덱스가 임베딩 크기를 넘지 않도록 확인\n",
        "    input_idx = [idx if idx < len(word_to_ix) else word_to_ix[\"<UNK>\"] for idx in input_idx]\n",
        "    \n",
        "    input_tensor = torch.tensor(input_idx).unsqueeze(0)  # (1, sequence_length)\n",
        "    \n",
        "    predicted_sentence = phrase.split()\n",
        "    hidden = None\n",
        "    \n",
        "    for _ in range(num_words):\n",
        "        output, hidden = RNN(input_tensor, hidden)\n",
        "        last_word_logits = output[0, -1, :]\n",
        "        \n",
        "        if mode == 'max':\n",
        "            predicted_idx = torch.argmax(last_word_logits).item()\n",
        "        elif mode == 'multinomial':\n",
        "            probs = torch.softmax(last_word_logits, dim=0)\n",
        "            predicted_idx = torch.multinomial(probs, 1).item()\n",
        "        else:\n",
        "            raise ValueError(\"Unknown mode: choose 'max' or 'multinomial'\")\n",
        "        \n",
        "        # 예측된 인덱스가 범위를 벗어나지 않도록 확인\n",
        "        predicted_word = ix_to_word.get(predicted_idx, \"<UNK>\")\n",
        "        predicted_sentence.append(predicted_word)\n",
        "        \n",
        "        # 새로운 입력을 업데이트\n",
        "        input_tensor = torch.tensor([predicted_idx]).unsqueeze(0)\n",
        "    \n",
        "    return predicted_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6f1e4374",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Perplexity: 5.228828215779611\n"
          ]
        }
      ],
      "source": [
        "# Perplexity 계산\n",
        "perplexity = np.exp(loss.item())\n",
        "print(f\"Final Perplexity: {perplexity}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "dbb7cc19",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "s1 (max): The Tortured Poets Department we are the ever ever\n",
            "s2 (max): So Long, London we are the ever ever\n",
            "s3 (max): Down Bad we are the ever ever\n",
            "s1 (multinomial): The Tortured Poets Department getting are back together together\n",
            "s2 (multinomial): So Long, London are ever ever ever together\n",
            "s3 (multinomial): Down Bad together </s> we </s> are\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "def predict_next_words(RNN, phrase, word_to_ix, ix_to_word, num_words=5, mode='max'):\n",
        "    \"\"\"\n",
        "    주어진 문장 뒤에 예측된 단어들을 생성하는 함수.\n",
        "    \n",
        "    Args:\n",
        "        RNN: 학습된 RNN 언어 모델.\n",
        "        phrase: str, 주어진 문장 (예: 'The Tortured Poets Department').\n",
        "        word_to_ix: dict, 단어 -> 인덱스 사전.\n",
        "        ix_to_word: dict, 인덱스 -> 단어 사전.\n",
        "        num_words: int, 예측할 단어의 개수.\n",
        "        mode: str, 'max' 또는 'multinomial' 모드.\n",
        "    \n",
        "    Returns:\n",
        "        predicted_sentence: List[str], 예측된 단어들로 이루어진 리스트.\n",
        "    \"\"\"\n",
        "    RNN.eval()  # 예측 모드로 전환\n",
        "    \n",
        "    # 입력 문장을 인덱스로 변환\n",
        "    input_idx = [word_to_ix.get(word, word_to_ix[\"<UNK>\"]) for word in phrase.split()]\n",
        "    input_tensor = torch.tensor(input_idx).unsqueeze(0)  # (1, sequence_length)\n",
        "    \n",
        "    predicted_sentence = phrase.split()\n",
        "    hidden = None\n",
        "    \n",
        "    for _ in range(num_words):\n",
        "        output, hidden = RNN(input_tensor, hidden)\n",
        "        last_word_logits = output[0, -1, :]\n",
        "        \n",
        "        if mode == 'max':\n",
        "            predicted_idx = torch.argmax(last_word_logits).item()\n",
        "        elif mode == 'multinomial':\n",
        "            probs = torch.softmax(last_word_logits, dim=0)\n",
        "            predicted_idx = torch.multinomial(probs, 1).item()\n",
        "        else:\n",
        "            raise ValueError(\"Unknown mode: choose 'max' or 'multinomial'\")\n",
        "        \n",
        "        predicted_word = ix_to_word.get(predicted_idx, \"<UNK>\")\n",
        "        predicted_sentence.append(predicted_word)\n",
        "        \n",
        "        # 새로운 입력을 업데이트\n",
        "        input_tensor = torch.tensor([predicted_idx]).unsqueeze(0)\n",
        "    \n",
        "    return predicted_sentence\n",
        "\n",
        "# 예시 트랙 제목들\n",
        "s1 = \"The Tortured Poets Department\"\n",
        "s2 = \"So Long, London\"\n",
        "s3 = \"Down Bad\"\n",
        "\n",
        "# 각 트랙 제목에 대해 단어 예측 (max 모드와 multinomial 모드 모두 사용)\n",
        "pred_s1_max = predict_next_words(RNN, s1, word_to_ix, ix_to_word, num_words=5, mode='max')\n",
        "pred_s2_max = predict_next_words(RNN, s2, word_to_ix, ix_to_word, num_words=5, mode='max')\n",
        "pred_s3_max = predict_next_words(RNN, s3, word_to_ix, ix_to_word, num_words=5, mode='max')\n",
        "\n",
        "pred_s1_multi = predict_next_words(RNN, s1, word_to_ix, ix_to_word, num_words=5, mode='multinomial')\n",
        "pred_s2_multi = predict_next_words(RNN, s2, word_to_ix, ix_to_word, num_words=5, mode='multinomial')\n",
        "pred_s3_multi = predict_next_words(RNN, s3, word_to_ix, ix_to_word, num_words=5, mode='multinomial')\n",
        "\n",
        "# 결과 출력\n",
        "print(f\"s1 (max): {' '.join(pred_s1_max)}\")\n",
        "print(f\"s2 (max): {' '.join(pred_s2_max)}\")\n",
        "print(f\"s3 (max): {' '.join(pred_s3_max)}\")\n",
        "\n",
        "print(f\"s1 (multinomial): {' '.join(pred_s1_multi)}\")\n",
        "print(f\"s2 (multinomial): {' '.join(pred_s2_multi)}\")\n",
        "print(f\"s3 (multinomial): {' '.join(pred_s3_multi)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "b9a19423",
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (12) must match the size of tensor b (3601) at non-singleton dimension 0",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[31], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m embedding_matrix \u001b[38;5;241m=\u001b[39m create_embedding_matrix(word_to_ix, glove_embeddings, embedding_dim)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Initialize the model, loss function, and optimizer\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m RNN \u001b[38;5;241m=\u001b[39m RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n\u001b[1;32m     15\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     16\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(RNN\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m)\n",
            "Cell \u001b[0;32mIn[21], line 10\u001b[0m, in \u001b[0;36mRNNLanguageModel.__init__\u001b[0;34m(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 임베딩 레이어: 사전 학습된 GloVe 임베딩으로 초기화\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size, embedding_dim)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcopy_(embedding_matrix)  \u001b[38;5;66;03m# GloVe 임베딩으로 초기화\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# GloVe 임베딩 고정\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# RNN 레이어: hidden_dim 크기의 RNN 레이어 정의\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (12) must match the size of tensor b (3601) at non-singleton dimension 0"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# 시드 설정\n",
        "torch.manual_seed(11411)\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 50\n",
        "hidden_dim = 32\n",
        "num_epochs = 10\n",
        "\n",
        "# 데이터 로드\n",
        "vocab, word_to_ix, ix_to_word, dataloader = loadfile(\"data/lyrics/taylor_swift.txt\")\n",
        "glove_embeddings = load_glove_embeddings('glove.6B.50d.txt')\n",
        "embedding_matrix = create_embedding_matrix(word_to_ix, glove_embeddings, embedding_dim)\n",
        "\n",
        "# 모델 초기화\n",
        "RNN = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(RNN.parameters(), lr=0.005)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    for inputs, targets in dataloader:\n",
        "        RNN.zero_grad()\n",
        "        output, _ = RNN(inputs)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        num_batches += 1\n",
        "\n",
        "    # Perplexity 계산 (전체 배치에 대한 평균 손실을 기반으로)\n",
        "    avg_loss = total_loss / num_batches\n",
        "    perplexity = np.exp(avg_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}, Perplexity: {perplexity}\")\n",
        "\n",
        "print(f\"Final Perplexity: {perplexity}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "dcd3a291",
      "metadata": {
        "id": "dcd3a291"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "index out of range in self",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sentence \u001b[38;5;241m=\u001b[39m s1\n\u001b[0;32m----> 2\u001b[0m predicted_words_sequence \u001b[38;5;241m=\u001b[39m RNN\u001b[38;5;241m.\u001b[39mgenerate_sentence(sentence, word_to_ix, ix_to_word, \u001b[38;5;241m10\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultinomial\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentence \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(predicted_words_sequence))\n",
            "Cell \u001b[0;32mIn[17], line 77\u001b[0m, in \u001b[0;36mRNNLanguageModel.generate_sentence\u001b[0;34m(self, sequence, word_to_ix, ix_to_word, num_words, mode)\u001b[0m\n\u001b[1;32m     73\u001b[0m predicted_sequence \u001b[38;5;241m=\u001b[39m sequence\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_words):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# 순방향 패스: 단어를 예측\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(input_tensor, hidden)\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# 마지막 단어의 출력에서 다음 단어 예측\u001b[39;00m\n\u001b[1;32m     80\u001b[0m     last_word_logits \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n",
            "Cell \u001b[0;32mIn[17], line 34\u001b[0m, in \u001b[0;36mRNNLanguageModel.forward\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03mRNN 모델의 순방향 패스.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03mhidden: torch.Tensor (num_layers, batch_size, hidden_dim)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# 1. 입력을 임베딩으로 변환\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 2. RNN 레이어를 통과\u001b[39;00m\n\u001b[1;32m     37\u001b[0m out, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(x, hidden)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx,\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type,\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq,\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse,\n\u001b[1;32m    198\u001b[0m     )\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ],
      "source": [
        "sentence = s1\n",
        "predicted_words_sequence = RNN.generate_sentence(sentence, word_to_ix, ix_to_word, 10, mode='multinomial')\n",
        "print(sentence + ' ' + ' '.join(predicted_words_sequence))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d712619f",
      "metadata": {
        "id": "d712619f"
      },
      "source": [
        "**Aside (for fun!)**: Train your LM on Taylor Swift lyrics and generate the next hit!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17b9047d",
      "metadata": {
        "id": "17b9047d"
      },
      "source": [
        "### **Written 4.4** – Battle of the LMs: GPT-2, Trigram and RNN [8 points]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41d38359",
      "metadata": {
        "id": "41d38359"
      },
      "source": [
        "For this subtask, you will be generating text and comparing GPT-2 with your n-gram and RNN language models. \n",
        "\n",
        "Generative pretrained transformer (GPT) is a neural language model series created by OpenAI. The n-gram language model you trained has on average around 10K-20K parameters (`len(lm.model)`.) Compare that to the 175 billion parameters of GPT-3, which is likely much smaller than more recent iterations (though they don't tell us anymore)!\n",
        "\n",
        "Let's see how GPT-2 compares to the LMs you trained in Written 4.3.1 on the `data/bbc/tech-small.txt` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "e143e87c",
      "metadata": {
        "id": "e143e87c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "179.0901520291377"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculate your n-gram model's perplexity\n",
        "test = preprocess(read_file(\"data/bbc/tech-small.txt\"), 3)\n",
        "NGram = NGramLanguageModel(n=3, train_data=test)\n",
        "NGram.perplexity(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "a164e62b",
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "The size of tensor a (3601) must match the size of tensor b (444) at non-singleton dimension 0",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[29], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m embedding_matrix \u001b[38;5;241m=\u001b[39m create_embedding_matrix(word_to_ix, glove_embeddings, embedding_dim)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Initialize the model, loss function, and optimizer\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m RNN \u001b[38;5;241m=\u001b[39m RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n\u001b[1;32m     15\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     16\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(RNN\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m)\n",
            "Cell \u001b[0;32mIn[17], line 10\u001b[0m, in \u001b[0;36mRNNLanguageModel.__init__\u001b[0;34m(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 임베딩 레이어: 사전 학습된 GloVe 임베딩으로 초기화\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mEmbedding(vocab_size, embedding_dim)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mcopy_(embedding_matrix)  \u001b[38;5;66;03m# GloVe 임베딩으로 초기화\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# GloVe 임베딩 고정\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# RNN 레이어: hidden_dim 크기의 RNN 레이어 정의\u001b[39;00m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3601) must match the size of tensor b (444) at non-singleton dimension 0"
          ]
        }
      ],
      "source": [
        "# Calculate your RNN model's perplexity\n",
        "torch.manual_seed(11411)\n",
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 50\n",
        "hidden_dim = 32\n",
        "num_epochs = 10\n",
        "\n",
        "vocab, word_to_ix, ix_to_word, dataloader = loadfile(\"data/bbc/tech-small.txt\")\n",
        "glove_embeddings = load_glove_embeddings('glove.6B.50d.txt')\n",
        "embedding_matrix = create_embedding_matrix(word_to_ix, glove_embeddings, embedding_dim)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "RNN = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(RNN.parameters(), lr=0.005)\n",
        "\n",
        "lines = \"\"\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        RNN.zero_grad()\n",
        "        output, _ = RNN(inputs)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    line = f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}, Perplexity: {np.exp(loss.item())}'\n",
        "    lines += line + \"\\n\"\n",
        "    print(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "0a5e0d34",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "N-gram Model Perplexity: 99.16627687793007\n"
          ]
        }
      ],
      "source": [
        "# N-gram 모델을 통해 퍼플렉시티 계산\n",
        "n = 3  # 3-gram 예시\n",
        "smoothing = 0.1  # 라플라스 스무딩\n",
        "train_data = preprocess(read_file('data/lyrics/taylor_swift.txt'), n)\n",
        "test_data = preprocess(read_file('data/lyrics/test_lyrics.txt'), n)\n",
        "\n",
        "# N-gram Language Model 생성\n",
        "ngram_lm = NGramLanguageModel(n, train_data, smoothing)\n",
        "\n",
        "# 퍼플렉시티 계산\n",
        "ngram_perplexity = ngram_lm.perplexity(test_data)\n",
        "print(f\"N-gram Model Perplexity: {ngram_perplexity}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "3ee227ef",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNN Model Perplexity after epoch 1: 12.871503056341572\n",
            "RNN Model Perplexity after epoch 2: 11.020702011138756\n",
            "RNN Model Perplexity after epoch 3: 9.616626600032012\n",
            "RNN Model Perplexity after epoch 4: 8.551673420981833\n",
            "RNN Model Perplexity after epoch 5: 7.722583219312739\n",
            "RNN Model Perplexity after epoch 6: 7.044095047307152\n",
            "RNN Model Perplexity after epoch 7: 6.460458975520567\n",
            "RNN Model Perplexity after epoch 8: 5.941673136509541\n",
            "RNN Model Perplexity after epoch 9: 5.4730746166131805\n",
            "RNN Model Perplexity after epoch 10: 5.047685709881503\n"
          ]
        }
      ],
      "source": [
        "# Make sure the vocab_size is correct\n",
        "vocab_size = len(word_to_ix)  # Ensure this matches the number of unique words in your dataset\n",
        "\n",
        "# Now, create the embedding matrix based on the correct vocab_size\n",
        "embedding_matrix = create_embedding_matrix(word_to_ix, glove_embeddings, embedding_dim)\n",
        "\n",
        "# RNN 모델 설정 (embedding_matrix shape will now match vocab_size)\n",
        "RNN = RNNLanguageModel(vocab_size, embedding_dim, hidden_dim, embedding_matrix)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(RNN.parameters(), lr=0.005)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        RNN.zero_grad()\n",
        "        output, _ = RNN(inputs)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    # 퍼플렉시티 계산\n",
        "    perplexity = np.exp(loss.item())\n",
        "    print(f\"RNN Model Perplexity after epoch {epoch+1}: {perplexity}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "83946254",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca574d9ee8fb433da60e55bc40e34dca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "654626bf22d84d51aa46ada17862bd56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12f924fe7bb446d58b0cae9c34ec877f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fc3d38eeb9044fafbd310045d9c9c2ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a033cdf05beb42f587610c7414e23ca8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5206f213c304f48ae6b56f514355c30",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9a8fb2507a74de59b5ba0e35878b891",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT-2 Model Perplexity: 1993.6246337890625\n"
          ]
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# GPT-2 모델 및 토크나이저 로드\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# 모델을 평가 모드로 전환\n",
        "model.eval()\n",
        "\n",
        "# 테스트 텍스트 데이터\n",
        "test_text = \"Your test sentence here\"\n",
        "input_ids = tokenizer.encode(test_text, return_tensors='pt')\n",
        "\n",
        "# GPT-2를 사용한 퍼플렉시티 계산\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids, labels=input_ids)\n",
        "    loss = outputs.loss\n",
        "    perplexity = torch.exp(loss)\n",
        "\n",
        "print(f\"GPT-2 Model Perplexity: {perplexity.item()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db542ceb",
      "metadata": {
        "id": "db542ceb"
      },
      "source": [
        "#### Computing GPT-2's perplexity on the test set\n",
        "\n",
        "You need to enable a GPU runtime from the Colab `Runtime` menu option (you can also use your computer if you have an accelerator). Go to `Runtime` → `Change Runtime Type` → `Hardware Accelerator (GPU)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ldf6ovg4B5Qc",
      "metadata": {
        "id": "Ldf6ovg4B5Qc"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "import torch\n",
        "\n",
        "model_id = \"distilgpt2\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_id)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67zwTHSI0hCC",
      "metadata": {
        "id": "67zwTHSI0hCC"
      },
      "outputs": [],
      "source": [
        "test = read_file(\"data/bbc/tech-small.txt\")\n",
        "encodings = tokenizer(\"\\n\\n\".join(test), return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wmQKbMXjDFNj",
      "metadata": {
        "id": "wmQKbMXjDFNj"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "max_length = model.config.n_positions\n",
        "stride = 100\n",
        "\n",
        "nlls = []\n",
        "for i in tqdm(range(0, encodings.input_ids.size(1), stride)):\n",
        "    begin_loc = max(i + stride - max_length, 0)\n",
        "    end_loc = min(i + stride, encodings.input_ids.size(1))\n",
        "    trg_len = end_loc - i  # may be different from stride on last loop\n",
        "    input_ids = encodings.input_ids[:, begin_loc:end_loc]\n",
        "    target_ids = input_ids.clone()\n",
        "    target_ids[:, :-trg_len] = -100\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "        neg_log_likelihood = outputs[0] * trg_len\n",
        "\n",
        "    nlls.append(neg_log_likelihood)\n",
        "\n",
        "ppl = torch.exp(torch.stack(nlls).sum() / end_loc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "giXGq0Z0DWdr",
      "metadata": {
        "id": "giXGq0Z0DWdr"
      },
      "outputs": [],
      "source": [
        "print(\"Perplexity using GPT2:\", ppl.item())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "abb966b7ea35b20666f3a996666172c7b349752d6f16e30915fecc2a1a4bf9ca"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
