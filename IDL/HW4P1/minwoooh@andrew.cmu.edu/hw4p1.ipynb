{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSLkT0qL3jgl"
   },
   "source": [
    "# HW4P1: Language Modelling\n",
    "\n",
    "Welcome to the final part 1 hw of this course. This is the only part 1 in which you have PyTorch training (Yay). You will be working on training language models and evaluating them on the task of prediction and generation.<br>\n",
    "The model which you will be coding in this HW very similar to the Speller module from HW4P2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The language model was trained using a Transformer-based architecture with causal masking, positional encoding, and multi-head self-attention mechanisms. The dataset consisted of character-level tokenized transcripts from speech recognition datasets. A custom data loader was implemented to handle sequence-to-sequence learning. The training process utilized Adam optimizer, cross-entropy loss, and a learning rate scheduler. The model was validated on a separate validation set, and the final test perplexity was calculated as 4.8656, indicating the model’s ability to generate predictions with reasonable accuracy. The result suggests the model effectively learned the dataset’s patterns and dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EB2bOV3bzYLR"
   },
   "source": [
    "# Get modules and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QZNwme4320LW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/work/jupyter/minwoo/CMU/Intro_deep_learning/HW4P1/handout_4/hw4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/anaconda3/envs/minwoo/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "path = '/home/work/jupyter/minwoo/CMU/Intro_deep_learning/HW4P1/handout_4/hw4' # TODO: Add path to handout. For example ~/IDL/hw4/hw4p1_handout\n",
    "\n",
    "sys.path.append(path)\n",
    "%cd {path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "INh9p3v3zbF_"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oxiZ42B4SwQ-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:1\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "import os\n",
    "\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import torchsummaryX\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import gc\n",
    "import glob\n",
    "import wandb\n",
    "import yaml\n",
    "import json\n",
    "import random\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "DEVICE = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device: \", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "loUYgkv8moMl"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ybSqv89zmoMm"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"token_type\":  \"1k\", # TODO: select a tokenzier from [\"char\", \"1k\", \"10k\"]\n",
    "    \"d_model\":     512,  \n",
    "    \"num_layers\":  4,    \n",
    "    \"num_heads\":   8,    \n",
    "    \"d_ff\":        2048,  \n",
    "    \"dropout\":     0.25,  \n",
    "    \"max_length\":  128,  \n",
    "    \"lr\":          1e-4, \n",
    "    \"batch_size\":  128,   \n",
    "    \"num_epochs\":  1,   \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdG352aFYdyz"
   },
   "source": [
    "# Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9D39sk7AYdy0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary    : 31\n",
      "VOCAB                   : ['<sos>', '<eos>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', \"'\", ' ', '<pad>']\n",
      "PAD_TOKEN               : 30\n",
      "SOS_TOKEN               : 0\n",
      "EOS_TOKEN               : 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>transcripts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>CHAPTER TEN SHAGGY MAN TO THE RESCUE THEY HAD ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AT ONCE THEY HURRIED FORWARD TO SEE WHAT THIS ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>AND PICKED OUT THE EASIEST PLACES TO GO ALL IT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>LEAVING HOLES THAT MIGHT CAUSE THE UNWARY TO S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>WITH MY HEART RENDING GROWL MY HORRIBLE SHUDDE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        transcripts\n",
       "0           0  CHAPTER TEN SHAGGY MAN TO THE RESCUE THEY HAD ...\n",
       "1           1  AT ONCE THEY HURRIED FORWARD TO SEE WHAT THIS ...\n",
       "2           2  AND PICKED OUT THE EASIEST PLACES TO GO ALL IT...\n",
       "3           3  LEAVING HOLES THAT MIGHT CAUSE THE UNWARY TO S...\n",
       "4           4  WITH MY HEART RENDING GROWL MY HORRIBLE SHUDDE..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the vocabulary. Try printing and see\n",
    "VOCAB = [\n",
    "   \"<sos>\", \"<eos>\",\n",
    "    \"A\",   \"B\",    \"C\",    \"D\",\n",
    "    \"E\",   \"F\",    \"G\",    \"H\",\n",
    "    \"I\",   \"J\",    \"K\",    \"L\",\n",
    "    \"M\",   \"N\",    \"O\",    \"P\",\n",
    "    \"Q\",   \"R\",    \"S\",    \"T\",\n",
    "    \"U\",   \"V\",    \"W\",    \"X\",\n",
    "    \"Y\",   \"Z\",    \"'\",    \" \", \"<pad>\"\n",
    "]\n",
    "\n",
    "VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n",
    "# We have also included <sos> and <eos> in the vocabulary for you\n",
    "# However in real life, you include it explicitly if not provided\n",
    "PAD_TOKEN =  VOCAB_MAP[\"<pad>\"]\n",
    "SOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n",
    "EOS_TOKEN = VOCAB_MAP[\"<eos>\"]\n",
    "\n",
    "print(f\"Length of Vocabulary    : {len(VOCAB)}\")\n",
    "print(f\"VOCAB                   : {VOCAB}\")\n",
    "print(f\"PAD_TOKEN               : {PAD_TOKEN}\")\n",
    "print(f\"SOS_TOKEN               : {SOS_TOKEN}\")\n",
    "print(f\"EOS_TOKEN               : {EOS_TOKEN}\")\n",
    "\n",
    "df_train = pd.read_csv(\"/home/work/jupyter/minwoo/CMU/Intro_deep_learning/HW4P1/handout_4/dataset/train-clean-100/transcripts.csv\")\n",
    "df_val = pd.read_csv(\"/home/work/jupyter/minwoo/CMU/Intro_deep_learning/HW4P1/handout_4/dataset/dev-clean/transcripts.csv\")\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9XKR8hhmFslQ"
   },
   "outputs": [],
   "source": [
    "class CharTokenizer():\n",
    "    ''' A wrapper around character tokenization to have a consistent interface with other tokeization strategies'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.eos_token = \"<|endoftext|>\"  # Same as EOS_TOKEN\n",
    "        self.pad_token = \"<|padding|>\"\n",
    "        self.unk_token = \"<|unknown|>\"\n",
    "\n",
    "        characters = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ '\")\n",
    "\n",
    "        # Create vocabulary mapping\n",
    "        self.vocab = {\n",
    "            self.eos_token: 0,\n",
    "            self.pad_token: 1,  # Same ID as EOS_TOKEN\n",
    "            self.unk_token: 2,\n",
    "        }\n",
    "\n",
    "        for idx, char in enumerate(characters, start=3):\n",
    "            self.vocab[char] = idx\n",
    "\n",
    "        self.inv_vocab = {v: k for k, v in self.vocab.items()}\n",
    "\n",
    "        self.eos_token_id = self.vocab[self.eos_token]\n",
    "        self.bos_token_id = self.vocab[self.eos_token]\n",
    "        self.pad_token_id = self.vocab[self.pad_token]\n",
    "        self.unk_token_id = self.vocab[self.unk_token]\n",
    "\n",
    "        self.vocab_size = len(self.vocab)\n",
    "\n",
    "    def tokenize(self, data):\n",
    "        return [char for char in data]\n",
    "\n",
    "    def encode(self, data, return_tensors=None):\n",
    "        e = [self.vocab.get(char.upper(), self.unk_token) for char in data]\n",
    "        if return_tensors == 'pt':\n",
    "            return torch.tensor(e).unsqueeze(0)\n",
    "        return e\n",
    "\n",
    "    def decode(self, data):\n",
    "        try:\n",
    "            return ''.join([self.inv_vocab.get(j) for j in data])\n",
    "        except:\n",
    "            data = data.cpu().tolist()\n",
    "            return ''.join([self.inv_vocab.get(j) for j in data])\n",
    "\n",
    "    def convert_tokens_to_ids(self, token):\n",
    "        return self.vocab[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6kdTvuPfmoMo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1k vocab tokenizer loaded\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer\n",
    "if config[\"token_type\"] == \"1k\":\n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(\"alexgichamba/hw4_tokenizer_1k\")\n",
    "    print(\"1k vocab tokenizer loaded\")\n",
    "elif config[\"token_type\"] == \"10k\":\n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(\"alexgichamba/hw4_tokenizer_10k\")\n",
    "    print(\"10k vocab tokenizer loaded\")\n",
    "elif config[\"token_type\"] == \"char\":\n",
    "    TOKENIZER = CharTokenizer()\n",
    "    print(\"character tokenizer loaded\")\n",
    "else:\n",
    "    raise ValueError(\"Invalid token type\")\n",
    "\n",
    "\n",
    "UNK_TOKEN = TOKENIZER.unk_token_id\n",
    "EOS_TOKEN = TOKENIZER.eos_token_id\n",
    "SOS_TOKEN = TOKENIZER.bos_token_id\n",
    "PAD_TOKEN = TOKENIZER.convert_tokens_to_ids('<|padding|>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "csVEvmoc3pAI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281241\n",
      "2433\n"
     ]
    }
   ],
   "source": [
    "train_transcripts = [np.array([i for i in row['transcripts'].replace(\"<sos>\", \"\").replace(\"<eos>\", \"\") ]) for index, row in df_train.iterrows()]\n",
    "train_dataset = []\n",
    "for files in train_transcripts:\n",
    "    tokenized = \"\".join(files)\n",
    "    tokenized = TOKENIZER.encode(tokenized)\n",
    "    train_dataset.append(tokenized)\n",
    "\n",
    "val_transcripts = [np.array([i for i in row['transcripts'].replace(\"<sos>\", \"\").replace(\"<eos>\", \"\") ]) for index, row in df_val.iterrows()]\n",
    "val_dataset = []\n",
    "for files in val_transcripts:\n",
    "    tokenized = \"\".join(files)\n",
    "    tokenized = TOKENIZER.encode(tokenized)\n",
    "    val_dataset.append(tokenized)\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "umQYdUCjmoMo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 06:06:09.679081: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-22 06:06:09.700653: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-22 06:06:09.723754: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-22 06:06:09.730208: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-22 06:06:09.744983: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HE', 'LL', 'O', 'ĠDE', 'EP', 'ĠLE', 'AR', 'N', 'ERS']\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = TOKENIZER.vocab_size\n",
    "\n",
    "# test the tokenizer\n",
    "if TOKENIZER is not None:\n",
    "    TOKENIZER.decode([EOS_TOKEN, SOS_TOKEN, PAD_TOKEN, UNK_TOKEN])\n",
    "    print(TOKENIZER.tokenize(\"HELLO DEEP LEARNERS\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dHjYhXAOzkrP"
   },
   "source": [
    "# Custom DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "gH_fPT-fYdy1"
   },
   "outputs": [],
   "source": [
    "class DataLoaderForLanguageModeling(torch.utils.data.DataLoader): # Inherit from torch.utils.data.DataLoader\n",
    "    \"\"\"\n",
    "        TODO: Define data loader logic here\n",
    "    \"\"\"\n",
    "    # TODO: You can probably add more parameters as well. Eg. sequence length\n",
    "    def __init__(self, dataset, batch_size, seq_len=3, shuffle= True, drop_last= False):\n",
    "\n",
    "        # If you remember, these are the standard things which you give while defining a dataloader.\n",
    "        # Now you are just customizing your dataloader\n",
    "        self.dataset    = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle    = shuffle\n",
    "        self.drop_last  = drop_last\n",
    "        self.sequence_length = seq_len\n",
    "        \n",
    "        self.inputs, self.targets = self._prepare_sequences()\n",
    "        self.num_batches = len(self.inputs) // self.batch_size\n",
    "        if not self.drop_last and len(self.inputs) % self.batch_size != 0:\n",
    "            self.num_batches += 1\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        # What output do you get when you print len(loader)? You get the number of batches\n",
    "        # Your dataset has (579, ) articles and each article has a specified amount of words.\n",
    "        # You concatenate the dataset and then batch parts of it according to the sequence length\n",
    "        # TODO: return the number of batches\n",
    "        # If you are using variable sequence_length, the length might not be fixed\n",
    "        return self.num_batches\n",
    "    \n",
    "    def _prepare_sequences(self):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "\n",
    "        for seq in self.dataset:\n",
    "            input_seq = [SOS_TOKEN] + seq\n",
    "            target_seq = seq + [EOS_TOKEN]\n",
    "\n",
    "            for i in range(0, len(input_seq), self.sequence_length):\n",
    "                input_chunk = input_seq[i:i + self.sequence_length]\n",
    "                target_chunk = target_seq[i:i + self.sequence_length]\n",
    "\n",
    "                input_chunk += [PAD_TOKEN] * (self.sequence_length - len(input_chunk))\n",
    "                target_chunk += [PAD_TOKEN] * (self.sequence_length - len(target_chunk))\n",
    "\n",
    "                inputs.append(input_chunk)\n",
    "                targets.append(target_chunk)\n",
    "\n",
    "        return inputs, targets\n",
    "    \n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            combined = list(zip(self.inputs, self.targets))\n",
    "            random.shuffle(combined)\n",
    "            self.inputs[:], self.targets[:] = zip(*combined)\n",
    "\n",
    "        for i in range(0, len(self.inputs), self.batch_size):\n",
    "            batch_inputs = self.inputs[i:i + self.batch_size]\n",
    "            batch_targets = self.targets[i:i + self.batch_size]\n",
    "\n",
    "            # Convert to tensors\n",
    "            batch_inputs = torch.tensor(batch_inputs, dtype=torch.long)\n",
    "            batch_targets = torch.tensor(batch_targets, dtype=torch.long)\n",
    "\n",
    "            yield batch_inputs, batch_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "t7IyJOYqYdy1"
   },
   "outputs": [],
   "source": [
    "dl = DataLoaderForLanguageModeling(\n",
    "    dataset = train_dataset,\n",
    "    batch_size = config[\"batch_size\"],\n",
    "    shuffle = True,\n",
    "    drop_last = False,\n",
    "    seq_len = config[\"max_length\"]\n",
    "    # Input Extra parameters here if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fBZSzmy10M9M"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128]) torch.Size([128, 128])\n",
      "x:  <|endoftext|>AND NOT ALTOGETHER A SLEEPING ONE WHEN IT CAME TO A DIVISION OF PROFITS WHICH AT TIMES WERE CONSIDERABLE IN ORDER TO BE NEARER HIS NEW FRIEND EDISON BOARDED WITH POPE AT ELIZABETH NEW JERSEY FOR SOME TIME<|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|>\n",
      "y:  AND NOT ALTOGETHER A SLEEPING ONE WHEN IT CAME TO A DIVISION OF PROFITS WHICH AT TIMES WERE CONSIDERABLE IN ORDER TO BE NEARER HIS NEW FRIEND EDISON BOARDED WITH POPE AT ELIZABETH NEW JERSEY FOR SOME TIME<|endoftext|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|><|padding|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Some sanity checks\n",
    "\n",
    "inputs, targets = next(iter(dl))\n",
    "print(inputs.shape, targets.shape)\n",
    "for x, y in dl:\n",
    "    transcript = TOKENIZER.decode(x[0].tolist())\n",
    "    transcript_y = TOKENIZER.decode(y[0].tolist())\n",
    "    print(\"x: \", transcript)\n",
    "    print(\"y: \", transcript_y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcWU0YlnzmVM"
   },
   "source": [
    "# Causal Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7wwkDXlV3xf"
   },
   "source": [
    "Causal language models predict the probability of a word based on the preceding words in the sentence. This differs from bidirectional models, which consider both previous and following context. Here, we use a Transformer-based decoder, leveraging its attention mechanism to focus only on earlier parts of the sequence to predict the next word. This type of modeling is suitable for tasks such as text generation where the sequence order is crucial.\n",
    "\n",
    "\n",
    "**Link to HuggingFace Documentation**: [Causal Language Model](https://huggingface.co/docs/transformers/en/tasks/language_modeling)\n",
    "\n",
    "The following image can be a helpful aid in visualizing the flow of information in a causal language model, highlighting how each word in a sequence is used to predict the next word.\n",
    "\n",
    "<img src=\"https://github.com/christianversloot/machine-learning-articles/blob/main/images/causal-1024x445.png?raw=true\" width=\"60%\">\n",
    "\n",
    "This figure shows three matrices: the attention scores between sequence elements, the causal mask with zeros allowing attention and negative infinity blocking future attention, and the resultant matrix after applying the causal mask. The negative infinity values in the causal mask prevent the model from using future tokens in its predictions, reinforcing the sequence's order. This visualization shows how transformers can be used for causal language modeling where future input information must not influence current predictions.\n",
    "\n",
    "<img src=\"https://github.com/christianversloot/machine-learning-articles/raw/main/images/Diagram-20-1024x282.png\" width=\"80%\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dps6lGqiYdy1"
   },
   "source": [
    "## 2. `create_mask`: Mask for Preventing Attention to Subsequent Positions\n",
    "\n",
    "\n",
    "```python\n",
    "def create_mask(seq, pad_idx=None)\n",
    "```\n",
    "\n",
    "\n",
    "## Purpose:\n",
    "This function creates a **subsequent mask** that prevents attention from attending to future positions in the sequence. It ensures that each position can only attend to previous positions (as in causal language modeling).\n",
    "\n",
    "## Usage:\n",
    "- **Input:**\n",
    "  - `seq`: Tensor of shape `(batch_size, sequence_length)` representing the input sequence.\n",
    "  - `pad_idx`: (Optional) Padding index for masking padding positions.\n",
    "\n",
    "- **Output:**\n",
    "  - A mask of shape `(batch_size, sequence_length, sequence_length)` where the upper triangular portion is filled with 1s to prevent attention to future positions.\n",
    "\n",
    "### The Expected mask should look like the image below:\n",
    "\n",
    "<img src=\"https://i.imgur.com/AJdqMGx.png\" alt=\"drawing\" width=\"400\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "v49yRYF7Ydy1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_mask(seq, pad_idx=None):\n",
    "    \"\"\" Create a mask to prevent positions from attending to subsequent positions.\n",
    "\n",
    "    Args:\n",
    "        seq: The input sequence tensor, shape (batch_size, sequence_length).\n",
    "\n",
    "    Returns:\n",
    "        A mask tensor with shape (batch_size, sequence_length, sequence_length),\n",
    "            where positions are allowed to attend to previous positions but not to subsequent positions.\n",
    "    \"\"\"\n",
    "\n",
    "    sz_b, len_s = seq.size()\n",
    "\n",
    "    # Create an upper triangular matrix with zeros on the diagonal and below (indicating allowed positions)\n",
    "    #   and ones above the diagonal (indicating disallowed positions)\n",
    "\n",
    "    subsequent_mask = torch.triu(torch.ones((len_s, len_s)), diagonal=1).to(seq.device) # TODO\n",
    "\n",
    "    # Expand the mask to match the batch size, resulting in a mask for each sequence in the batch.\n",
    "    mask = subsequent_mask.unsqueeze(0).expand(sz_b, -1, -1)  # b x ls x ls\n",
    "\n",
    "\n",
    "    ''' Create a mask to ignore padding positions in the key sequence during attention calculation. '''\n",
    "\n",
    "    # Expanding to fit the shape of key query attention matrix.\n",
    "    if pad_idx != None:\n",
    "        len_q = seq.size(1)\n",
    "\n",
    "          # Create a mask where padding positions in the key sequence are marked with 1.\n",
    "        padding_mask  = seq.eq(pad_idx)\n",
    "\n",
    "          # Expand the mask to match the dimensions of the key-query attention matrix.\n",
    "        padding_mask  = padding_mask.unsqueeze(1).expand(-1, len_q, -1)  # b x lq x lk\n",
    "\n",
    "\n",
    "        mask          = (padding_mask + mask).gt(0)\n",
    "\n",
    "    else:\n",
    "        mask = mask.gt(0)\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dec_causal_mask         = create_mask( torch.randn(4, 10)  , pad_idx=0)\n",
    "\n",
    "# Black portions are attended to\n",
    "fig, axs = plt.subplots(1, 1, figsize=(5, 5))\n",
    "\n",
    "axs.imshow(dec_causal_mask[0], cmap=\"gray\", aspect='auto')\n",
    "axs.set_title(\"Decoder Causal Self-Attn Mask\")\n",
    "\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1GobVvxYdy2"
   },
   "source": [
    "# Transformer Decoder Components\n",
    "\n",
    "We will use these components in the Transformer decoder. These include positional encoding, feed-forward networks, scaled dot-product attention, and multi-head attention. Each of these components plays a vital role in processing input sequences and computing attention in the Transformer model.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. **Positional Encoding (`PositionalEncoding`)**\n",
    "Transformers do not inherently capture the order of sequences, so positional encodings are used to introduce sequence order into the model.\n",
    "\n",
    "- **Purpose**: Adds information about the position of each token in the input sequence.\n",
    "- **Mechanism**: Uses a combination of sine and cosine functions of different frequencies to generate positional encodings.\n",
    "- **Parameters**:\n",
    "  - `projection_size`: The size of the input embeddings (i.e., `d_model`).\n",
    "  - `max_seq_len`: The maximum length of the input sequence (default: 1000).\n",
    "- **Output**: The input embedding enriched with positional information, which is passed through a dropout layer for regularization.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wixQHpDaYdy2"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, projection_size, max_seq_len= 1000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout                = torch.nn.Dropout(dropout)\n",
    "\n",
    "        pe              = torch.zeros(max_seq_len, projection_size)\n",
    "        position        = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term        = torch.exp(torch.arange(0, projection_size, 2).float() * (-math.log(10000.0) / projection_size))\n",
    "        pe[:, 0::2]     = torch.sin(position * div_term)\n",
    "        pe[:, 1::2]     = torch.cos(position * div_term)\n",
    "        pe              = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[:, :x.size(1)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DUt8AjEEYdy2"
   },
   "source": [
    "\n",
    "## 2. **Feed-Forward Network (`FeedForward`)**\n",
    "The feed-forward network is a fully connected layer applied independently to each position in the sequence after the attention layers.\n",
    "\n",
    "- **Purpose**: Projects the intermediate representations to a higher-dimensional space and back to the original model dimension.\n",
    "- **Mechanism**: Consists of two linear layers with a GeLU activation function and dropout in between.\n",
    "- **Parameters**:\n",
    "  - `d_model`: The input and output dimensionality of the model.\n",
    "  - `d_ff`: The dimensionality of the hidden layer in the feed-forward network (default: 2048).\n",
    "  - `dropout`: Dropout rate applied after the GeLU activation (default: 0.1).\n",
    "- **Output**: The transformed input sequence passed through two linear transformations with non-linear activation in between.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "f5UzjJmRYdy2"
   },
   "outputs": [],
   "source": [
    "\n",
    "class FeedForward(torch.nn.Module):\n",
    "    ''' Projection Layer (Fully Connected Layers) '''\n",
    "\n",
    "    def __init__(self, d_model, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear_1   = torch.nn.Linear(d_model, d_ff)\n",
    "        self.dropout    = torch.nn.Dropout(dropout)\n",
    "        self.linear_2   = torch.nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Apply the first linear layer, GeLU activation, and then dropout\n",
    "        x = self.dropout(torch.nn.functional.gelu(self.linear_1(x)))\n",
    "\n",
    "        # Apply the second linear layer to project the dimension back to d_model\n",
    "        x = self.linear_2(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vofvMG5lYdy7"
   },
   "source": [
    "\n",
    "\n",
    "## 3. **Scaled Dot-Product Attention (`ScaledDotProductAttention`)**\n",
    "This module computes the attention score for each query-key pair in the input sequence using the scaled dot-product mechanism.\n",
    "\n",
    "- **Purpose**: To compute attention scores and generate weighted outputs based on the input query, key, and value matrices.\n",
    "- **Mechanism**:\n",
    "  - Calculates the dot product of queries and keys, scales by the square root of the dimension, and applies a softmax to generate attention weights.\n",
    "  - Uses dropout for regularization.\n",
    "- **Parameters**:\n",
    "  - `temperature`: Scaling factor for the dot product.\n",
    "  - `attn_dropout`: Dropout rate for attention weights (default: 0.1).\n",
    "- **Output**: Returns the weighted sum of the values and the attention weights.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "wMB9Jb45Ydy7"
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(torch.nn.Module):\n",
    "    ''' Scaled Dot-Product Attention '''\n",
    "\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.temperature    = temperature                       # Scaling factor for the dot product\n",
    "        self.dropout        = torch.nn.Dropout(attn_dropout)    # Dropout layer for attention weights\n",
    "        self.softmax        = torch.nn.Softmax(dim=-1)           # Softmax layer along the attention dimension\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        # Calculate the dot product between queries and keys.\n",
    "        # attn = torch.bmm(q, k.transpose(1, 2))\n",
    "        attn = (q @ k.transpose(-2, -1))\n",
    "\n",
    "        # Scale the dot product by the temperature.\n",
    "        attn = attn / self.temperature\n",
    "\n",
    "        if mask is not None:\n",
    "            # Apply the mask by setting masked positions to a large negative value.\n",
    "            # This ensures they have a softmax score close to zero.\n",
    "            attn = attn.masked_fill(mask, float('-inf'))\n",
    "\n",
    "        # Apply softmax to obtain attention weights.\n",
    "        attn    = self.softmax(attn)\n",
    "\n",
    "        # Apply dropout to the attention weights.\n",
    "        # Compute the weighted sum of values based on the attention weights.\n",
    "        # output  = torch.bmm(self.dropout(attn), v)\n",
    "        attn = self.dropout(attn)\n",
    "        output = attn @ v\n",
    "\n",
    "        return output, attn # Return the attention output and the attention weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjSRj_hjYdy7"
   },
   "source": [
    "## 4. **Multi-Head Attention (`MultiHeadAttention`)**\n",
    "This module implements multi-head attention, where multiple sets of attention heads are computed in parallel, and their outputs are concatenated.\n",
    "\n",
    "- **Purpose**: To allow the model to jointly attend to different positions in the input sequence from different representation subspaces.\n",
    "- **Mechanism**:\n",
    "  - Projects the input query, key, and value matrices into multiple smaller subspaces (heads).\n",
    "  - Computes scaled dot-product attention for each head in parallel.\n",
    "  - Concatenates the outputs of all heads and applies a final linear transformation to project the result back to the original model dimension.\n",
    "- **Parameters**:\n",
    "  - `n_head`: Number of attention heads.\n",
    "  - `d_model`: Dimensionality of the input and output representations.\n",
    "  - `dropout`: Dropout rate applied to the attention output (default: 0.1).\n",
    "- **Output**: Returns the concatenated output of all attention heads and the averaged attention weights.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "TRa-PPXrYdy7"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    ''' Multi-Head Attention Module '''\n",
    "\n",
    "    def __init__(self, n_head, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = n_head # Number of attention heads\n",
    "        self.d_k    = d_model // n_head\n",
    "        self.d_v    = d_model // n_head\n",
    "\n",
    "\n",
    "        # Linear layers for projecting the input query, key, and value to multiple heads\n",
    "        self.w_qs   = torch.nn.Linear(d_model, n_head * self.d_k)\n",
    "        self.w_ks   = torch.nn.Linear(d_model, n_head * self.d_k)\n",
    "        self.w_vs   = torch.nn.Linear(d_model, n_head * self.d_v)\n",
    "\n",
    "        torch.nn.init.normal_(self.w_qs.weight, mean=0, std=np.sqrt(2.0 / (d_model + self.d_k)))\n",
    "        torch.nn.init.normal_(self.w_ks.weight, mean=0, std=np.sqrt(2.0 / (d_model + self.d_k)))\n",
    "        torch.nn.init.normal_(self.w_vs.weight, mean=0, std=np.sqrt(2.0 / (d_model + self.d_v)))\n",
    "\n",
    "        # Initialize the weights of the linear layers\n",
    "        self.attention = ScaledDotProductAttention(\n",
    "            temperature=np.power(self.d_k, 0.5), attn_dropout=dropout)\n",
    "\n",
    "        # Final linear layer to project the concatenated outputs of the attention heads back to the model dimension\n",
    "        self.fc = torch.nn.Linear(n_head * self.d_v, d_model)\n",
    "        torch.nn.init.normal_(self.fc.weight)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "\n",
    "        # following key, value, query standard computation\n",
    "        d_k, d_v, n_head    = self.d_k, self.d_v, self.n_head\n",
    "        sz_b, len_q, _      = q.size()\n",
    "        sz_b, len_k, _      = k.size()\n",
    "        sz_b, len_v, _      = v.size()\n",
    "\n",
    "        # Project the input query, key, and value to multiple heads\n",
    "        q = self.w_qs(q).view(sz_b, len_q, n_head, d_k)\n",
    "        k = self.w_ks(k).view(sz_b, len_k, n_head, d_k)\n",
    "        v = self.w_vs(v).view(sz_b, len_v, n_head, d_v)\n",
    "\n",
    "        # Rearrange the dimensions to group the heads together for parallel processing\n",
    "        q = q.transpose(1, 2)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "\n",
    "        # Repeat the mask for each attention head if a mask is provided\n",
    "        if mask is not None:\n",
    "              # print(mask.shape)\n",
    "              mask = mask.unsqueeze(1).repeat(1, n_head, 1, 1)\n",
    "\n",
    "        # Apply scaled dot-product attention to the projected query, key, and value\n",
    "        output, attn    = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # Rearrange the output back to the original order and concatenate the heads\n",
    "        output = output.transpose(1, 2).contiguous().view(sz_b, len_v, -1)\n",
    "\n",
    "        output          = self.dropout(self.fc(output))\n",
    "\n",
    "        attn_weights = attn.mean(dim=(0, 1))\n",
    "\n",
    "        return output, attn_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rlvl41QtYdy7"
   },
   "source": [
    "# Transformer Decoder Layers\n",
    "\n",
    "The `DecoderLayer1` and `DecoderLayer3` are modular components of the Transformer decoder. Each layer is designed to handle a specific function: self-attention, cross-attention, and feed-forward processing.\n",
    "\n",
    "## 1. `DecoderLayer1`: Self-Attention Layer\n",
    "- **Purpose**: Implements self-attention, where the decoder attends to its own inputs, combined with residual connections and layer normalization.\n",
    "- **Components**:\n",
    "  - `MultiHeadAttention`: Applies self-attention to the target sequence.\n",
    "  - `LayerNorm`: Normalizes the output after the residual connection.\n",
    "  - `Dropout`: Regularization to prevent overfitting.\n",
    "\n",
    "## 3. `DecoderLayer3`: Feed-Forward Layer\n",
    "- **Purpose**: Implements a feed-forward neural network for further transformation of the decoder's intermediate representations.\n",
    "- **Components**:\n",
    "  - `FeedForward`: A two-layer fully connected network with non-linearity.\n",
    "  - `LayerNorm`: Applied after the residual connection.\n",
    "  - `Dropout`: Regularization to avoid overfitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5hX2kBrAYdy7"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer1(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        DecoderLayer (attention and layer norm) in the Transformer architecture.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The number of expected features in the input (embedding dimension).\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Dimension of the feedforward network model.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super(DecoderLayer1, self).__init__()\n",
    "\n",
    "        # TODO: fill in the blanks appropriately (given the modules above)\n",
    "        self.self_attn = MultiHeadAttention(n_head=num_heads, d_model=d_model, dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, attn_mask=None, key_padding_mask=None):\n",
    "        # TODO: apply layer norm to input\n",
    "        # TODO: call self attention with mask\n",
    "        output, attn_weights = self.self_attn(q=self.layer_norm(tgt), k=self.layer_norm(tgt), v=self.layer_norm(tgt), mask=attn_mask) \n",
    "        \n",
    "        # TODO: apply dropout\n",
    "        output = self.dropout(output) \n",
    "        \n",
    "        # TODO: add skip connection\n",
    "        tgt = tgt + output \n",
    "        \n",
    "        # raise NotImplemented\n",
    "        return tgt, attn_weights\n",
    "\n",
    "\n",
    "class DecoderLayer3(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        Feedforward layer with layer normalization in the Transformer decoder.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): Embedding dimension.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Dimension of the feedforward network.\n",
    "            dropout (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super(DecoderLayer3, self).__init__()\n",
    "\n",
    "        # TODO: fill in the blanks appropriately (given the modules above)\n",
    "        self.ffn = FeedForward(d_model=d_model, d_ff=d_ff, dropout=dropout)\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt):\n",
    "        # TODO: apply layer norm to input\n",
    "        # TODO: call feed forward layer\n",
    "        output = self.ffn(self.layer_norm(tgt))\n",
    "        # TODO: apply dropout\n",
    "        output = self.dropout(output)\n",
    "        # TODO: add skip connection\n",
    "        tgt = tgt + output\n",
    "        # raise NotImplemented\n",
    "        return tgt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnMbHHLcYdy7"
   },
   "source": [
    "# Causal Language Model\n",
    "\n",
    "This module implements a Transformer-based decoder for causal language modeling (CLM). It consists of several components, including embedding layers, positional encoding, self-attention layers, and feed-forward layers. It supports various generation strategies such as beam search and sampling.\n",
    "\n",
    "### Key Components:\n",
    "- **Embedding Layer**: Converts input tokens into dense vector representations.\n",
    "- **Positional Encoding**: Adds position information to input tokens, helping the model understand the order of tokens.\n",
    "- **Decoder Layers**: Composed of:\n",
    "  - `DecoderLayer1`: Implements self-attention and layer normalization.\n",
    "  - `DecoderLayer3`: Implements a feed-forward network with residual connections.\n",
    "- **Output Linear Layer**: Projects the hidden states to the vocabulary size to generate output probabilities.\n",
    "\n",
    "### Key Methods:\n",
    "- **`forward`**: Runs the input through the decoder layers and generates output probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "cebwoorWttWe"
   },
   "outputs": [],
   "source": [
    "class CausalLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size=31, d_model=256, num_layers=2, num_heads=2, d_ff=512, dropout=0.1, max_length=1000):\n",
    "\n",
    "        \"\"\"\n",
    "        Decoder module in the Transformer architecture.\n",
    "        Initializes embeddings, multiple decoder layers, and an output linear layer.\n",
    "\n",
    "        Args:\n",
    "            vocab_size (int): Size of the vocabulary.\n",
    "            d_model (int): The number of expected features in the input (embedding dimension).\n",
    "            num_layers (int): Number of decoder layers.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Dimension of the feedforward network model.\n",
    "            dropout (float): Dropout probability.\n",
    "            max_length (int): Maximum length of input sequences.\n",
    "        \"\"\"\n",
    "        super(CausalLanguageModel, self).__init__()\n",
    "\n",
    "        # TODO: fill in the blanks appropriately (given the modules above)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len=max_length, dropout=dropout)\n",
    "        self.num_layers= num_layers\n",
    "        self.dec_layers1 = nn.ModuleList([DecoderLayer1(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        self.dec_layers3 = nn.ModuleList([DecoderLayer3(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.fully_connected = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # TODO: generate the causal mask using the given function\n",
    "        attn_mask = create_mask(inp, pad_idx=PAD_TOKEN)\n",
    "\n",
    "        # TODO: convert input to embeddings\n",
    "        # TODO: apply positional encoding\n",
    "        inp = self.embedding(inp)\n",
    "        inp = self.pos_encoder(inp)\n",
    "\n",
    "        attention_weights_list = []\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # TODO: apply decoder layer\n",
    "            inp, attn_weights = self.dec_layers1[i](inp, attn_mask=attn_mask)\n",
    "            inp = self.dec_layers3[i](inp)\n",
    "            attention_weights_list.append(attn_weights)\n",
    "\n",
    "        # TODO: apply layernorm and the fully connected layer for classification\n",
    "        output = self.fully_connected(self.layer_norm(inp))\n",
    "\n",
    "        stacked_attention_weights = torch.stack(attention_weights_list, dim=0)\n",
    "\n",
    "        return output, stacked_attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DelhoytAQWQa"
   },
   "source": [
    "# Model, Loss, Optimizer, and Scheduler Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "DbHH6zXTSwRa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CausalLanguageModel(\n",
      "  (embedding): Embedding(1000, 512)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.25, inplace=False)\n",
      "  )\n",
      "  (dec_layers1): ModuleList(\n",
      "    (0-3): 4 x DecoderLayer1(\n",
      "      (self_attn): MultiHeadAttention(\n",
      "        (w_qs): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_ks): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (w_vs): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (attention): ScaledDotProductAttention(\n",
      "          (dropout): Dropout(p=0.25, inplace=False)\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "        (fc): Linear(in_features=512, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (dec_layers3): ModuleList(\n",
      "    (0-3): 4 x DecoderLayer3(\n",
      "      (ffn): FeedForward(\n",
      "        (linear_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.25, inplace=False)\n",
      "        (linear_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "      )\n",
      "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.25, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (fully_connected): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1364812/1985671330.py:40: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# TODO: Define the model\n",
    "model = CausalLanguageModel(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    d_model=config[\"d_model\"],\n",
    "    num_layers=config[\"num_layers\"],\n",
    "    num_heads=config[\"num_heads\"],\n",
    "    d_ff=config[\"d_ff\"],\n",
    "    dropout=config[\"dropout\"],\n",
    "    max_length=config[\"max_length\"]\n",
    ").to(DEVICE)\n",
    "\n",
    "# TODO: Define the dataloader\n",
    "train_loader = DataLoaderForLanguageModeling(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    seq_len=config[\"max_length\"],\n",
    "    shuffle=True,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "val_loader = DataLoaderForLanguageModeling(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    seq_len=config[\"max_length\"],\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "# TODO: Define the criterion\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "# TODO: Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "# TODO: Define the learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)\n",
    "\n",
    "# Optional TODO: Define the scaler for mixed precision training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "# Print the model architecture and parameter summary\n",
    "print(model)\n",
    "\n",
    "# Optionally, if you want to summarize the model, make sure `torchsummaryX` is installed\n",
    "# summary = torchsummaryX.summary(model.to(DEVICE), x=torch.tensor(inputs).to(DEVICE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlWF_bpLznup"
   },
   "source": [
    "\n",
    "\n",
    "# Trainer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "jkFMwb095P5w"
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, optimizer, criterion, scheduler, scaler, max_epochs= 1):\n",
    "        \"\"\"\n",
    "            Use this class to train your model\n",
    "        \"\"\"\n",
    "        # feel free to add any other parameters here\n",
    "        self.model      = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader   = val_loader\n",
    "        self.optimizer  = optimizer\n",
    "        self.criterion  = criterion\n",
    "        self.scheduler  = scheduler\n",
    "        self.scaler     = scaler\n",
    "\n",
    "        self.train_losses           = []\n",
    "        self.val_losses             = []\n",
    "        self.prediction_probs       = []\n",
    "        self.prediction_probs_test  = []\n",
    "        self.generated_texts_test   = []\n",
    "        self.generated_texts_test_beam = []\n",
    "        self.generated_texts_test_beam_random = []\n",
    "        self.generated_texts_validation = []\n",
    "\n",
    "        self.log_likelihood_beam = []\n",
    "        self.log_likelihood_beam_random = []\n",
    "\n",
    "        self.epochs                 = 0\n",
    "        self.max_epochs             = max_epochs\n",
    "\n",
    "\n",
    "    def calculate_loss(self, out, target):\n",
    "        # output: (B, T, Vocab_size) - probability distributions\n",
    "        # target: (B, T)\n",
    "        # Read the documentation of CrossEntropyLoss and try to understand how it takes inputs\n",
    "\n",
    "        # Tip: If your target is of shape (B, T) it means that you have B batches with T words.\n",
    "        # Tip: What is the total number of words in this batch?\n",
    "        # Tip: Crossentropy calculates the loss between a label and its probability distribution.\n",
    "\n",
    "        out     = out.view(-1, out.size(-1)) # TODO\n",
    "        targets = target.view(-1) # TODO\n",
    "        loss    = self.criterion(out, targets)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        self.model.train() # set to training mode\n",
    "        epoch_loss  = 0\n",
    "        num_batches = 0\n",
    "        attn_weights = None\n",
    "\n",
    "        for batch_num, (inputs, targets) in enumerate(tqdm(self.train_loader)):\n",
    "\n",
    "            # TODO: Complete the loop. You should be able to complete this without any helper comments after 3 HWs\n",
    "            # Tip: Use Mixed Precision Training\n",
    "            # For loss calculation, use the calculate_loss function. You need to complete it before using.\n",
    "            # pass\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            \n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs, attn_weights = self.model(inputs)\n",
    "                loss = self.calculate_loss(outputs, targets)\n",
    "                \n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        self.epochs += 1\n",
    "        print('[TRAIN] \\tEpoch [%d/%d] \\tLoss: %.4f \\tLr: %.6f'\n",
    "                     % (self.epochs, self.max_epochs, epoch_loss, self.optimizer.param_groups[0]['lr']))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "\n",
    "        return (epoch_loss, self.optimizer.param_groups[0]['lr'], attn_weights)\n",
    "\n",
    "    def validate(self):\n",
    "\n",
    "        self.model.eval() # set to eval mode\n",
    "        epoch_loss  = 0\n",
    "        num_batches = 0\n",
    "        attn_weights = None\n",
    "\n",
    "        for batch_num, (inputs, targets) in enumerate(tqdm(self.val_loader)):\n",
    "\n",
    "            # TODO: Complete the loop. You should be able to complete this without any helper comments after 3 HWs\n",
    "            # Tip: you don't need gradients for inference\n",
    "            # For loss calculation, use the calculate_loss function. You need to complete it before using it.\n",
    "            # pass\n",
    "            \n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            \n",
    "            outputs, _ = self.model(inputs)\n",
    "            loss = self.calculate_loss(outputs, targets)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        self.epochs += 1\n",
    "        print('[VAL] \\tEpoch [%d/%d] \\tLoss: %.4f \\tLr: %.6f'\n",
    "                     % (self.epochs, self.max_epochs, epoch_loss, self.optimizer.param_groups[0]['lr']))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "\n",
    "        return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "2HCVG5YISwRW"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model       = model,\n",
    "    train_loader = train_loader,\n",
    "    val_loader  = val_loader,\n",
    "    optimizer   = optimizer,\n",
    "    criterion   = criterion,\n",
    "    scheduler   = scheduler,\n",
    "    scaler      = scaler,\n",
    "    max_epochs  = config[\"num_epochs\"], # TODO: set the number of epochs\n",
    ")\n",
    "\n",
    "trainer.epochs = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dfrf1FoSoAI0"
   },
   "source": [
    "# Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "91s_RQaQYdy8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgapbu\u001b[0m (\u001b[33mgapbu-carnegie-mellon-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/work/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/work/jupyter/minwoo/CMU/Intro_deep_learning/HW4P1/handout_4/hw4/wandb/run-20241122_060629-dmvx47pn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/gapbu-carnegie-mellon-university/hw4p1-f24/runs/dmvx47pn' target=\"_blank\">final</a></strong> to <a href='https://wandb.ai/gapbu-carnegie-mellon-university/hw4p1-f24' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/gapbu-carnegie-mellon-university/hw4p1-f24' target=\"_blank\">https://wandb.ai/gapbu-carnegie-mellon-university/hw4p1-f24</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/gapbu-carnegie-mellon-university/hw4p1-f24/runs/dmvx47pn' target=\"_blank\">https://wandb.ai/gapbu-carnegie-mellon-university/hw4p1-f24/runs/dmvx47pn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use wandb? Resume Training?\n",
    "USE_WANDB = True\n",
    "RESUME_LOGGING = False\n",
    "\n",
    "# Create your wandb run\n",
    "\n",
    "run_name = 'final' # TODO: pick a run name you like\n",
    "\n",
    "if USE_WANDB:\n",
    "\n",
    "    wandb.login(key='f20506dbcca14d82ab5324952dad2c08e99085e2') # your wandb key\n",
    "\n",
    "    if RESUME_LOGGING:\n",
    "        run_id = ''\n",
    "        run = wandb.init(\n",
    "            settings=wandb.Settings(symlink=False),\n",
    "            id     = run_id, ### Insert specific run id here if you want to resume a previous run\n",
    "            resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "            project = \"hw4p1-f24\", ### Project should be created in your wandb account\n",
    "        )\n",
    "    else:\n",
    "        run = wandb.init(\n",
    "            name    = run_name, ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
    "            reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "            project = \"hw4p1-f24\", ### Project should be created in your wandb account\n",
    "            config  = config ### Wandb Config for your run\n",
    "        )\n",
    "\n",
    "        ### Save your model architecture as a string with str(model)\n",
    "        model_arch  = str(model)\n",
    "        ### Save it in a txt file\n",
    "        arch_file   = open(\"model_arch.txt\", \"w\")\n",
    "        file_write  = arch_file.write(model_arch)\n",
    "        arch_file.close()\n",
    "\n",
    "\n",
    "        ### log it in your wandb run with wandb.save()\n",
    "        # wandb.save('model_arch.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fcxKXL0hrxX"
   },
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-pLh5_LCpVxw",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974c5b3175584cb6931723bb85991317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2198 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1364812/2238103942.py:66: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN] \tEpoch [1/1] \tLoss: 5.1820 \tLr: 0.000100\n",
      "torch.Size([128, 128])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e94e5fb31cd4f49b3215a8243af9239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VAL] \tEpoch [2/1] \tLoss: 4.6037 \tLr: 0.000100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/anaconda3/envs/minwoo/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:232: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87e485293d3404f8f528504095b37eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>▁</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>learning_rate</td><td>0.0001</td></tr><tr><td>test_loss</td><td>4.60368</td></tr><tr><td>train_loss</td><td>5.18197</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">final</strong> at: <a href='https://wandb.ai/gapbu-carnegie-mellon-university/hw4p1-f24/runs/dmvx47pn' target=\"_blank\">https://wandb.ai/gapbu-carnegie-mellon-university/hw4p1-f24/runs/dmvx47pn</a><br/> View project at: <a href='https://wandb.ai/gapbu-carnegie-mellon-university/hw4p1-f24' target=\"_blank\">https://wandb.ai/gapbu-carnegie-mellon-university/hw4p1-f24</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241122_060629-dmvx47pn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.18.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "423"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run the experiments loop.\n",
    "# Each epoch wont take more than 2-3min. If its taking more time, it might be due to (but not limited to) the following:\n",
    "#   * You might be overlapping batches\n",
    "#       Eg. Input: \"I had biryani for lunch today\" and sequence length = 3,\n",
    "#           --> \"I had biryani\", \"for lunch today\" are ideal examples for inputs\n",
    "#           --> \"I had biryani\", \"had biryani for\", \"biryani for lunch\", ... is just redundant info :')\n",
    "#   * Your length calculation in the dataloader might be wrong\n",
    "# If you haven't had biryani, try it :D\n",
    "\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    train_loss, curr_lr,  attn_weights = trainer.train()\n",
    "    print(attn_weights[-1].shape)\n",
    "\n",
    "    test_loss = trainer.validate()\n",
    "\n",
    "    wandb.log({\"train_loss\":train_loss,\n",
    "                \"test_loss\": test_loss,\n",
    "                \"learning_rate\": curr_lr\n",
    "               })\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "### Finish your wandb run\n",
    "run.finish()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EScl6rumoMv"
   },
   "source": [
    "# Testing Your Model's Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "g7-MtnkWiHcj"
   },
   "outputs": [],
   "source": [
    "def log_softmax(x, axis):\n",
    "    ret = x - np.max(x, axis=axis, keepdims=True)\n",
    "    lsm = np.log(np.sum(np.exp(ret), axis=axis, keepdims=True))\n",
    "    return ret - lsm\n",
    "\n",
    "def get_prediction_nll_single_for_test(out, targ):\n",
    "    out = log_softmax(out, 1)\n",
    "    nlls = out[np.arange(out.shape[0]), targ]\n",
    "    nll = -np.sum(nlls)\n",
    "    return nll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "AWrNI6Ny2cyQ"
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"/home/work/jupyter/minwoo/CMU/Intro_deep_learning/HW4P1/handout_4/dataset/test-clean/transcripts.csv\")\n",
    "test_transcripts  = []\n",
    "test_transcripts = [np.array([i for i in row['transcripts'].replace(\"<sos>\", \"\").replace(\"<eos>\", \"\") ]) for index, row in test_df.iterrows()]\n",
    "\n",
    "test_dataset = []\n",
    "for files in test_transcripts:\n",
    "    tokenized = \"\".join(files)\n",
    "    tokenized = TOKENIZER.encode(tokenized)\n",
    "    test_dataset.append(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "g-tmR17aiL63"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97fc755c57f64a5894d3a7274896f21e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1364812/3389907590.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  inputs = torch.tensor(inputs).long().to(DEVICE)\n",
      "/tmp/ipykernel_1364812/3389907590.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = torch.tensor(targets).long().to(DEVICE)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_perplexity: 3.6266320215026684\n"
     ]
    }
   ],
   "source": [
    "test_dl = DataLoaderForLanguageModeling(\n",
    "    dataset         = test_dataset,\n",
    "    batch_size      = 1,\n",
    "    shuffle         = False,\n",
    "    drop_last       = False,\n",
    "    seq_len = 100\n",
    ")\n",
    "\n",
    "nnls = []\n",
    "model.eval()\n",
    "\n",
    "for batch_num, (inputs, targets) in enumerate(tqdm(test_dl)):\n",
    "    inputs = torch.tensor(inputs).long().to(DEVICE)\n",
    "    targets = torch.tensor(targets).long().to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(inputs)\n",
    "    nnl = get_prediction_nll_single_for_test(output[0][0].to('cpu').numpy(), targets[0].to('cpu').numpy())\n",
    "    if TOKENIZER != None:\n",
    "        text_len = len(TOKENIZER.decode(targets.flatten().to('cpu')).replace(\"<|endoftext|>\", \"\")) + 1\n",
    "    else:\n",
    "        text_len = len(targets[0])\n",
    "    nnls.append(nnl / text_len)\n",
    "\n",
    "test_ppl = np.exp(sum(nnls) / len(nnls))\n",
    "print(f'test_perplexity: {test_ppl}')\n",
    "with open('/home/work/jupyter/minwoo/CMU/Intro_deep_learning/HW4P1/handout_4/hw4/test_perplexity_2.txt', 'w') as f:\n",
    "    f.write(str(test_ppl))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "989a3c3836794109ac641230122845a3",
  "kernelspec": {
   "display_name": "minwoo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
