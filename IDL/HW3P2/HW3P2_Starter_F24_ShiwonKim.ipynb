{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UR4qfYrVoO4v"
   },
   "source": [
    "# Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "mA9qZoIDcx-h"
   },
   "outputs": [],
   "source": [
    "# %pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchtext==0.14.1 torchaudio==0.13.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu117 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ONgAWhqdoYy-"
   },
   "source": [
    "\n",
    "This may take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "SS7a7xeEoaV9"
   },
   "outputs": [],
   "source": [
    "# !pip install torchsummaryX==1.3.0\n",
    "# !pip install pandas==1.5.2\n",
    "# !pip install wandb --quiet\n",
    "# !pip install python-Levenshtein -q\n",
    "# !git clone --recursive https://github.com/parlance/ctcdecode.git\n",
    "# !pip install wget -q\n",
    "# %cd ctcdecode\n",
    "# !pip install . -q\n",
    "# %cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWVONJxCobPc"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "78ZTCIXoof2f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shiwon/miniconda3/envs/idl_hw3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummaryX import summary\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import torchaudio.transforms as tat\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gc\n",
    "\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "# imports for decoding and distance calculation\n",
    "import ctcdecode\n",
    "import Levenshtein\n",
    "from ctcdecode import CTCBeamDecoder\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gg3-yJ8tok34"
   },
   "source": [
    "# Kaggle Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AdUelfGhom1m"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8 -q\n",
    "# !mkdir /home/shiwon/.kaggle\n",
    "\n",
    "# with open(\"/home/shiwon/.kaggle/kaggle.json\", \"w+\") as f:\n",
    "#     f.write('{\"username\":\"shiwonkim\",\"key\":\"8eeeb84558070cf64f4a51d1897ead6a\"}')\n",
    "\n",
    "# !chmod 600 /home/shiwon/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dSjBwfXeoq4B"
   },
   "outputs": [],
   "source": [
    "# !kaggle competitions download -c 11-785-hw3p2-f24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_ruxWP60LCQA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis will take a couple minutes, but you should see at least the following:\\n11-785-f24-hw3p2  ctcdecode  hw3p2asr-f24.zip  sample_data\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This will take a couple minutes, but you should see at least the following:\n",
    "11-785-f24-hw3p2  ctcdecode  hw3p2asr-f24.zip  sample_data\n",
    "'''\n",
    "# !unzip -q hw3p2-785-f24.zip\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ORNHnSFroP0"
   },
   "source": [
    "# Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "k0v7wHRWrqH6"
   },
   "outputs": [],
   "source": [
    "# ARPABET PHONEME MAPPING\n",
    "# DO NOT CHANGE\n",
    "\n",
    "CMUdict_ARPAbet = {\n",
    "    \"\" : \" \",\n",
    "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\",\n",
    "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\",\n",
    "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\",\n",
    "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\",\n",
    "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\",\n",
    "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\",\n",
    "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\",\n",
    "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
    "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"\n",
    "}\n",
    "\n",
    "CMUdict = list(CMUdict_ARPAbet.keys())\n",
    "ARPAbet = list(CMUdict_ARPAbet.values())\n",
    "\n",
    "\n",
    "PHONEMES = CMUdict[:-2]\n",
    "LABELS = ARPAbet[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "eN2kcxwXLLBb"
   },
   "outputs": [],
   "source": [
    "# You might want to play around with the mapping as a sanity check here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agmNBKf4JrLV"
   },
   "source": [
    "### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "afd0_vlbJmr_"
   },
   "outputs": [],
   "source": [
    "class AudioDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    # For this homework, we give you full flexibility to design your data set class.\n",
    "    # Hint: The data from HW1 is very similar to this HW\n",
    "\n",
    "    def __init__(self, root, partition=\"train-clean-100\", phonemes=PHONEMES):\n",
    "        '''\n",
    "        Initializes the dataset.\n",
    "\n",
    "        INPUTS: What inputs do you need here?\n",
    "        '''\n",
    "\n",
    "        # Load the directory and all files in them\n",
    "\n",
    "        self.mfcc_dir = os.path.join(root, partition, \"mfcc\")\n",
    "        self.transcript_dir = os.path.join(root, partition, \"transcript\")\n",
    "\n",
    "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
    "        self.transcript_files = sorted(os.listdir(self.transcript_dir))\n",
    "\n",
    "        self.PHONEMES = phonemes\n",
    "\n",
    "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
    "        self.length = len(self.mfcc_files)\n",
    "        assert self.length == len(self.transcript_files)\n",
    "\n",
    "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
    "        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n",
    "\n",
    "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
    "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
    "        '''\n",
    "        You may decide to do this in __getitem__ if you wish.\n",
    "        However, doing this here will make the __init__ function take the load of\n",
    "        loading the data, and shift it away from training.\n",
    "        '''\n",
    "        self.mfccs, self.transcripts = [], []\n",
    "        \n",
    "        for i in range(self.length):\n",
    "            mfcc = np.load(os.path.join(self.mfcc_dir, self.mfcc_files[i]))\n",
    "            mfcc = (mfcc - np.mean(mfcc, axis=0)) / (np.std(mfcc, axis=0) + 1e-8) # Cepstral normalization\n",
    "            self.mfccs.append(mfcc)\n",
    "\n",
    "            transcript = np.load(os.path.join(self.transcript_dir, self.transcript_files[i]))[1:-1] # Remove SOS and EOS\n",
    "            self.transcripts.append([self.PHONEMES.index(p) for p in transcript])\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        '''\n",
    "        What do we return here?\n",
    "        '''\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        '''\n",
    "        RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
    "\n",
    "        If you didn't do the loading and processing of the data in __init__,\n",
    "        do that here.\n",
    "\n",
    "        Once done, return a tuple of features and labels.\n",
    "        '''\n",
    "        mfcc = torch.FloatTensor(self.mfccs[ind])\n",
    "        transcript = torch.LongTensor(self.transcripts[ind])\n",
    "        \n",
    "        return mfcc, transcript\n",
    "\n",
    "    def collate_fn(self,batch):\n",
    "        '''\n",
    "        TODO:\n",
    "        1.  Extract the features and labels from 'batch'\n",
    "        2.  We will additionally need to pad both features and labels,\n",
    "            look at pytorch's docs for pad_sequence\n",
    "        3.  This is a good place to perform transforms, if you so wish.\n",
    "            Performing them on batches will speed the process up a bit.\n",
    "        4.  Return batch of features, labels, lenghts of features,\n",
    "            and lengths of labels.\n",
    "        '''\n",
    "        # batch of input mfcc coefficients\n",
    "        batch_mfcc = [item[0] for item in batch]\n",
    "        # batch of output phonemes\n",
    "        batch_transcript = [item[1] for item in batch]\n",
    "\n",
    "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
    "        # Also be sure to check the input format (batch_first)\n",
    "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True)\n",
    "        lengths_mfcc = [mfcc.shape[0] for mfcc in batch_mfcc]\n",
    "\n",
    "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True)\n",
    "        lengths_transcript = [transcript.shape[0] for transcript in batch_transcript]\n",
    "\n",
    "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
    "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
    "        #                  -> Would we apply transformation on the validation set as well?\n",
    "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
    "\n",
    "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
    "        return batch_mfcc_pad, batch_transcript_pad, torch.LongTensor(lengths_mfcc), torch.LongTensor(lengths_transcript)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqDrxeHfJw4g"
   },
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HrLS1wfVJppA"
   },
   "outputs": [],
   "source": [
    "# Test Dataloader\n",
    "#TODO\n",
    "\n",
    "class AudioDatasetTest(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, root, partition=\"test-clean\"):\n",
    "\n",
    "        self.mfcc_dir = os.path.join(root, partition, \"mfcc\")\n",
    "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
    "        self.length = len(self.mfcc_files)\n",
    "        \n",
    "        self.mfccs = []\n",
    "\n",
    "        for mfcc_name in self.mfcc_files:\n",
    "            mfcc = np.load(os.path.join(self.mfcc_dir, mfcc_name))\n",
    "            mfcc = (mfcc - np.mean(mfcc, axis=0)) / (np.std(mfcc, axis=0) + 1e-8)\n",
    "            self.mfccs.append(mfcc)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        mfcc = torch.FloatTensor(self.mfccs[ind])\n",
    "\n",
    "        return mfcc\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        batch_mfcc_pad = pad_sequence(batch, batch_first=True)\n",
    "        lengths_mfcc = [mfcc.shape[0] for mfcc in batch]\n",
    "\n",
    "        return batch_mfcc_pad, torch.LongTensor(lengths_mfcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pt-veYcdL6Fe"
   },
   "source": [
    "### Config - Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "MN82c3KpLup8"
   },
   "outputs": [],
   "source": [
    "root = '/home/shiwon/2024_CMU/IntroDL/HW3/P2/11785-f24-hw3p2'\n",
    "\n",
    "# Feel free to add more items here\n",
    "config = {\n",
    "    \"beam_width\" : 10,\n",
    "    \"lr\"         : 2e-3,\n",
    "    \"epochs\"     : 10,\n",
    "    \"batch_size\" : 64  # Increase if your device can handle it\n",
    "}\n",
    "\n",
    "# You may pass this as a parameter to the dataset class above\n",
    "# This will help modularize your implementation\n",
    "transforms = [] # set of tranformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NmuPk9J6L8dz"
   },
   "source": [
    "### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3_kG0gU2x4hH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get me RAMMM!!!!\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "4mzoYfTKu14s"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size:  64\n",
      "Train dataset samples = 28539, batches = 446\n",
      "Val dataset samples = 2703, batches = 43\n",
      "Test dataset samples = 2620, batches = 41\n"
     ]
    }
   ],
   "source": [
    "# Create objects for the dataset class\n",
    "train_data = AudioDataset(root, \"train-clean-100\")\n",
    "val_data = AudioDataset(root, \"dev-clean\")\n",
    "test_data = AudioDatasetTest(root, \"test-clean\")\n",
    "\n",
    "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_data,\n",
    "    num_workers=8,\n",
    "    batch_size=config['batch_size'],\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    "    collate_fn=train_data.collate_fn\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset=val_data,\n",
    "    num_workers=2,\n",
    "    batch_size=config['batch_size'],\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    collate_fn=val_data.collate_fn\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_data,\n",
    "    num_workers=2,\n",
    "    batch_size=config['batch_size'],\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    collate_fn=test_data.collate_fn\n",
    ")\n",
    "\n",
    "print(\"Batch size: \", config['batch_size'])\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "cXMtwyviKaxK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1673, 28]) torch.Size([64, 214]) torch.Size([64]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "for data in train_loader:\n",
    "    x, y, lx, ly = data\n",
    "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSexxhdfMUzx"
   },
   "source": [
    "# NETWORK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-qb7wnAzCZl"
   },
   "source": [
    "## ASR Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PB6eh3gnMUzy"
   },
   "source": [
    "### Pyramid Bi-LSTM (pBLSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qd4BEX_yMUzz"
   },
   "outputs": [],
   "source": [
    "# Utils for network\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "class PermuteBlock(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OmdyXI6KMUzz"
   },
   "outputs": [],
   "source": [
    "class pBLSTM(torch.nn.Module):\n",
    "\n",
    "    '''\n",
    "    Pyramidal BiLSTM\n",
    "    Read the write up/paper and understand the concepts and then write your implementation here.\n",
    "\n",
    "    At each step,\n",
    "    1. Pad your input if it is packed (Unpack it)\n",
    "    2. Reduce the input length dimension by concatenating feature dimension\n",
    "        (Tip: Write down the shapes and understand)\n",
    "        (i) How should  you deal with odd/even length input?\n",
    "        (ii) How should you deal with input length array (x_lens) after truncating the input?\n",
    "    3. Pack your input\n",
    "    4. Pass it into LSTM layer\n",
    "\n",
    "    To make our implementation modular, we pass 1 layer at a time.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(pBLSTM, self).__init__()\n",
    "        \n",
    "        self.blstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, x_packed): # x_packed is a PackedSequence\n",
    "\n",
    "        x, x_lens = nn.utils.rnn.pad_packed_sequence(x_packed, batch_first=True)\n",
    "        x, x_lens = self.trunc_reshape(x, x_lens)\n",
    "\n",
    "        # Call self.trunc_reshape() which downsamples the time steps of x and increases the feature dimensions as mentioned above\n",
    "        # self.trunc_reshape will return 2 outputs. What are they? Think about what quantites are changing.\n",
    "        # Pack Padded Sequence. What output(s) would you get?\n",
    "        x_packed = nn.utils.rnn.pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Pass the sequence through bLSTM\n",
    "        lstm_out, _ = self.blstm1(x_packed)\n",
    "\n",
    "        return lstm_out\n",
    "\n",
    "    def trunc_reshape(self, x, x_lens):\n",
    "        # If you have odd number of timesteps, how can you handle it? (Hint: You can exclude them)\n",
    "        if x.size(1) % 2 != 0:\n",
    "            x_lens = x_lens - 1\n",
    "            max_len = x_lens.max().item()\n",
    "            x = x[:, :max_len, :]\n",
    "        \n",
    "        # Reshape x. When reshaping x, you have to reduce number of timesteps by a downsampling factor while increasing number of features by the same factor\n",
    "        batch_size, time_steps, feature_dim = x.size()\n",
    "        x = x.contiguous().view(batch_size, time_steps // 2, feature_dim * 2)\n",
    "        \n",
    "        # Reduce lengths by the same downsampling factor\n",
    "        x_lens = x_lens // 2\n",
    "\n",
    "        return x, x_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3ZQ75OcMUz0"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "GEzw5_xmMUz0"
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    '''\n",
    "    The Encoder takes utterances as inputs and returns latent feature representations\n",
    "    '''\n",
    "    def __init__(self, input_size, encoder_hidden_size, emb_size=256):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "\n",
    "        self.embedding = nn.Conv1d(input_size, emb_size, kernel_size=3, padding=1)\n",
    "\n",
    "        self.pBLSTMs = torch.nn.Sequential( # How many pBLSTMs are required?\n",
    "            # Fill this up with pBLSTMs - What should the input_size be?\n",
    "            # Hint: You are downsampling timesteps by a factor of 2, upsampling features by a factor of 2 and the LSTM is bidirectional)\n",
    "            # Optional: Dropout/Locked Dropout after each pBLSTM (Not needed for early submission)\n",
    "            # https://github.com/salesforce/awd-lstm-lm/blob/dfd3cb0235d2caf2847a4d53e1cbd495b781b5d2/locked_dropout.py#L5\n",
    "            pBLSTM(input_size=emb_size * 2, hidden_size=emb_size),\n",
    "            pBLSTM(input_size=emb_size * 4, hidden_size=encoder_hidden_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, x_lens):\n",
    "        # Where are x and x_lens coming from? The dataloader\n",
    "        # Call the embedding layer\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.embedding(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        # Pack Padded Sequence\n",
    "        x_packed = nn.utils.rnn.pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        # Pass Sequence through the pyramidal Bi-LSTM layer\n",
    "        encoder_out = self.pBLSTMs(x_packed)\n",
    "\n",
    "        # Pad Packed Sequence\n",
    "        encoder_out, encoder_lens = nn.utils.rnn.pad_packed_sequence(encoder_out, batch_first=True)\n",
    "\n",
    "        return encoder_out, encoder_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kg82HXa3MUz1"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "PQIRxdNTMUz1"
   },
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, output_size=41):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mlp = torch.nn.Sequential(\n",
    "            PermuteBlock(), torch.nn.BatchNorm1d(embed_size), PermuteBlock(),\n",
    "            # Define your MLP arch. Refer HW1P2\n",
    "            # Use Permute Block before and after BatchNorm1d() to match the size\n",
    "            nn.Linear(embed_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_size)\n",
    "        )\n",
    "\n",
    "        self.softmax = torch.nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, encoder_out):\n",
    "        out = self.mlp(encoder_out)\n",
    "        out = self.softmax(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "qmHf6pFiMUz1"
   },
   "outputs": [],
   "source": [
    "class ASRModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, embed_size= 192, output_size= len(PHONEMES)):\n",
    "        super().__init__()\n",
    "\n",
    "        self.augmentations  = torch.nn.Sequential(\n",
    "            # Add Time Masking/ Frequency Masking\n",
    "            # Hint: See how to use PermuteBlock() function defined above\n",
    "            PermuteBlock(),\n",
    "            tat.TimeMasking(time_mask_param=30),\n",
    "            tat.FrequencyMasking(freq_mask_param=30),\n",
    "            PermuteBlock(),\n",
    "        )\n",
    "        self.encoder = Encoder(input_size=input_size, encoder_hidden_size=embed_size//2)\n",
    "        self.decoder = Decoder(embed_size=embed_size, output_size=output_size)\n",
    "    \n",
    "    def forward(self, x, lengths_x):\n",
    "\n",
    "        if self.training:\n",
    "            x = self.augmentations(x)\n",
    "\n",
    "        encoder_out, encoder_lens   = self.encoder(x, lengths_x)\n",
    "        decoder_out                 = self.decoder(encoder_out)\n",
    "\n",
    "        return decoder_out, encoder_lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EV7DMPDoMUz2"
   },
   "source": [
    "## Initialize ASR Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oaaDsnnLMUz2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASRModel(\n",
      "  (augmentations): Sequential(\n",
      "    (0): PermuteBlock()\n",
      "    (1): TimeMasking()\n",
      "    (2): FrequencyMasking()\n",
      "    (3): PermuteBlock()\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Conv1d(28, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (pBLSTMs): Sequential(\n",
      "      (0): pBLSTM(\n",
      "        (blstm1): LSTM(512, 256, batch_first=True, bidirectional=True)\n",
      "      )\n",
      "      (1): pBLSTM(\n",
      "        (blstm1): LSTM(1024, 96, batch_first=True, bidirectional=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (mlp): Sequential(\n",
      "      (0): PermuteBlock()\n",
      "      (1): BatchNorm1d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): PermuteBlock()\n",
      "      (3): Linear(in_features=192, out_features=512, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Linear(in_features=512, out_features=41, bias=True)\n",
      "    )\n",
      "    (softmax): LogSoftmax(dim=2)\n",
      "  )\n",
      ")\n",
      "========================================================================================\n",
      "                                    Kernel Shape     Output Shape    Params  \\\n",
      "Layer                                                                         \n",
      "0_augmentations.PermuteBlock_0                 -   [64, 28, 1673]         -   \n",
      "1_augmentations.TimeMasking_1                  -   [64, 28, 1673]         -   \n",
      "2_augmentations.FrequencyMasking_2             -   [64, 28, 1673]         -   \n",
      "3_augmentations.PermuteBlock_3                 -   [64, 1673, 28]         -   \n",
      "4_encoder.Conv1d_embedding          [28, 256, 3]  [64, 256, 1673]    21.76k   \n",
      "5_encoder.pBLSTMs.0.LSTM_blstm1                -     [41194, 512]  1.57696M   \n",
      "6_encoder.pBLSTMs.1.LSTM_blstm1                -     [20584, 192]  861.696k   \n",
      "7_decoder.mlp.PermuteBlock_0                   -   [64, 192, 418]         -   \n",
      "8_decoder.mlp.BatchNorm1d_1                [192]   [64, 192, 418]     384.0   \n",
      "9_decoder.mlp.PermuteBlock_2                   -   [64, 418, 192]         -   \n",
      "10_decoder.mlp.Linear_3               [192, 512]   [64, 418, 512]   98.816k   \n",
      "11_decoder.mlp.ReLU_4                          -   [64, 418, 512]         -   \n",
      "12_decoder.mlp.Linear_5                [512, 41]    [64, 418, 41]   21.033k   \n",
      "13_decoder.LogSoftmax_softmax                  -    [64, 418, 41]         -   \n",
      "\n",
      "                                     Mult-Adds  \n",
      "Layer                                           \n",
      "0_augmentations.PermuteBlock_0               -  \n",
      "1_augmentations.TimeMasking_1                -  \n",
      "2_augmentations.FrequencyMasking_2           -  \n",
      "3_augmentations.PermuteBlock_3               -  \n",
      "4_encoder.Conv1d_embedding          35.976192M  \n",
      "5_encoder.pBLSTMs.0.LSTM_blstm1      1.572864M  \n",
      "6_encoder.pBLSTMs.1.LSTM_blstm1        860.16k  \n",
      "7_decoder.mlp.PermuteBlock_0                 -  \n",
      "8_decoder.mlp.BatchNorm1d_1              192.0  \n",
      "9_decoder.mlp.PermuteBlock_2                 -  \n",
      "10_decoder.mlp.Linear_3                98.304k  \n",
      "11_decoder.mlp.ReLU_4                        -  \n",
      "12_decoder.mlp.Linear_5                20.992k  \n",
      "13_decoder.LogSoftmax_softmax                -  \n",
      "----------------------------------------------------------------------------------------\n",
      "                          Totals\n",
      "Total params           2.580649M\n",
      "Trainable params       2.580649M\n",
      "Non-trainable params         0.0\n",
      "Mult-Adds             38.528704M\n",
      "========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kernel Shape</th>\n",
       "      <th>Output Shape</th>\n",
       "      <th>Params</th>\n",
       "      <th>Mult-Adds</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_augmentations.PermuteBlock_0</th>\n",
       "      <td>-</td>\n",
       "      <td>[64, 28, 1673]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_augmentations.TimeMasking_1</th>\n",
       "      <td>-</td>\n",
       "      <td>[64, 28, 1673]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2_augmentations.FrequencyMasking_2</th>\n",
       "      <td>-</td>\n",
       "      <td>[64, 28, 1673]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3_augmentations.PermuteBlock_3</th>\n",
       "      <td>-</td>\n",
       "      <td>[64, 1673, 28]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4_encoder.Conv1d_embedding</th>\n",
       "      <td>[28, 256, 3]</td>\n",
       "      <td>[64, 256, 1673]</td>\n",
       "      <td>21760.0</td>\n",
       "      <td>35976192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5_encoder.pBLSTMs.0.LSTM_blstm1</th>\n",
       "      <td>-</td>\n",
       "      <td>[41194, 512]</td>\n",
       "      <td>1576960.0</td>\n",
       "      <td>1572864.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6_encoder.pBLSTMs.1.LSTM_blstm1</th>\n",
       "      <td>-</td>\n",
       "      <td>[20584, 192]</td>\n",
       "      <td>861696.0</td>\n",
       "      <td>860160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7_decoder.mlp.PermuteBlock_0</th>\n",
       "      <td>-</td>\n",
       "      <td>[64, 192, 418]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8_decoder.mlp.BatchNorm1d_1</th>\n",
       "      <td>[192]</td>\n",
       "      <td>[64, 192, 418]</td>\n",
       "      <td>384.0</td>\n",
       "      <td>192.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9_decoder.mlp.PermuteBlock_2</th>\n",
       "      <td>-</td>\n",
       "      <td>[64, 418, 192]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10_decoder.mlp.Linear_3</th>\n",
       "      <td>[192, 512]</td>\n",
       "      <td>[64, 418, 512]</td>\n",
       "      <td>98816.0</td>\n",
       "      <td>98304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11_decoder.mlp.ReLU_4</th>\n",
       "      <td>-</td>\n",
       "      <td>[64, 418, 512]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12_decoder.mlp.Linear_5</th>\n",
       "      <td>[512, 41]</td>\n",
       "      <td>[64, 418, 41]</td>\n",
       "      <td>21033.0</td>\n",
       "      <td>20992.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13_decoder.LogSoftmax_softmax</th>\n",
       "      <td>-</td>\n",
       "      <td>[64, 418, 41]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Kernel Shape     Output Shape     Params  \\\n",
       "Layer                                                                          \n",
       "0_augmentations.PermuteBlock_0                 -   [64, 28, 1673]        NaN   \n",
       "1_augmentations.TimeMasking_1                  -   [64, 28, 1673]        NaN   \n",
       "2_augmentations.FrequencyMasking_2             -   [64, 28, 1673]        NaN   \n",
       "3_augmentations.PermuteBlock_3                 -   [64, 1673, 28]        NaN   \n",
       "4_encoder.Conv1d_embedding          [28, 256, 3]  [64, 256, 1673]    21760.0   \n",
       "5_encoder.pBLSTMs.0.LSTM_blstm1                -     [41194, 512]  1576960.0   \n",
       "6_encoder.pBLSTMs.1.LSTM_blstm1                -     [20584, 192]   861696.0   \n",
       "7_decoder.mlp.PermuteBlock_0                   -   [64, 192, 418]        NaN   \n",
       "8_decoder.mlp.BatchNorm1d_1                [192]   [64, 192, 418]      384.0   \n",
       "9_decoder.mlp.PermuteBlock_2                   -   [64, 418, 192]        NaN   \n",
       "10_decoder.mlp.Linear_3               [192, 512]   [64, 418, 512]    98816.0   \n",
       "11_decoder.mlp.ReLU_4                          -   [64, 418, 512]        NaN   \n",
       "12_decoder.mlp.Linear_5                [512, 41]    [64, 418, 41]    21033.0   \n",
       "13_decoder.LogSoftmax_softmax                  -    [64, 418, 41]        NaN   \n",
       "\n",
       "                                     Mult-Adds  \n",
       "Layer                                           \n",
       "0_augmentations.PermuteBlock_0             NaN  \n",
       "1_augmentations.TimeMasking_1              NaN  \n",
       "2_augmentations.FrequencyMasking_2         NaN  \n",
       "3_augmentations.PermuteBlock_3             NaN  \n",
       "4_encoder.Conv1d_embedding          35976192.0  \n",
       "5_encoder.pBLSTMs.0.LSTM_blstm1      1572864.0  \n",
       "6_encoder.pBLSTMs.1.LSTM_blstm1       860160.0  \n",
       "7_decoder.mlp.PermuteBlock_0               NaN  \n",
       "8_decoder.mlp.BatchNorm1d_1              192.0  \n",
       "9_decoder.mlp.PermuteBlock_2               NaN  \n",
       "10_decoder.mlp.Linear_3                98304.0  \n",
       "11_decoder.mlp.ReLU_4                      NaN  \n",
       "12_decoder.mlp.Linear_5                20992.0  \n",
       "13_decoder.LogSoftmax_softmax              NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ASRModel(\n",
    "    input_size  = 28,\n",
    "    embed_size  = 192,\n",
    "    output_size = len(PHONEMES)\n",
    ").to(device)\n",
    "print(model)\n",
    "summary(model, x.to(device), lx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IBwunYpyugFg"
   },
   "source": [
    "# Training Config\n",
    "Initialize Loss Criterion, Optimizer, CTC Beam Decoder, Scheduler, Scaler (Mixed-Precision), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "iGoozH2nd6KB"
   },
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
    "# Refer to the handout for hints\n",
    "\n",
    "optimizer =  torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-5) # What goes in here?\n",
    "\n",
    "# Declare the decoder. Use the CTC Beam Decoder to decode phonemes\n",
    "# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\n",
    "decoder = CTCBeamDecoder(labels=PHONEMES, beam_width=config['beam_width'], blank_id=0, log_probs_input=True)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Mixed Precision, if you need it\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jmc6_4eWL2Xp"
   },
   "source": [
    "# Decode Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "KHjnCDddL36E"
   },
   "outputs": [],
   "source": [
    "def decode_prediction(output, output_lens, decoder, PHONEME_MAP= LABELS):\n",
    "\n",
    "    # Look at docs for CTC.decoder and find out what is returned here. Check the shape of output and expected shape in decode.\n",
    "    beam_results, beam_scores, timesteps, out_lens = decoder.decode(output, seq_lens=output_lens) #lengths - list of lengths\n",
    "\n",
    "    pred_strings                    = []\n",
    "\n",
    "    for i in range(output_lens.shape[0]):\n",
    "        # Create the prediction from the output of decoder.decode. Don't forget to map it using PHONEMES_MAP.\n",
    "        pred_indices = beam_results[i][0][:out_lens[i][0]].tolist()\n",
    "        pred_string = ''.join([PHONEME_MAP[idx] for idx in pred_indices])\n",
    "        pred_strings.append(pred_string)\n",
    "\n",
    "    return pred_strings\n",
    "\n",
    "def calculate_levenshtein(output, label, output_lens, label_lens, decoder, PHONEME_MAP= LABELS): # y - sequence of integers\n",
    "\n",
    "    dist            = 0\n",
    "    batch_size      = label.shape[0]\n",
    "\n",
    "    pred_strings    = decode_prediction(output, output_lens, decoder, PHONEME_MAP)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        # Get predicted string and label string for each element in the batch\n",
    "        pred_string = pred_strings[i]\n",
    "        label_indices = label[i][:label_lens[i]].tolist()\n",
    "        label_string = ''.join([PHONEME_MAP[idx] for idx in label_indices])\n",
    "        dist += Levenshtein.distance(pred_string, label_string)\n",
    "\n",
    "    dist /= batch_size\n",
    "    \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Qk9iZud1LXT"
   },
   "source": [
    "# Test Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "GnTLL-5gMBrY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 734, 41])\n",
      "torch.Size([734, 64, 41]) torch.Size([64, 265])\n",
      "tensor(7.6756, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "205.890625\n"
     ]
    }
   ],
   "source": [
    "# test code to check shapes\n",
    "\n",
    "model.eval()\n",
    "for i, data in enumerate(val_loader, 0):\n",
    "    x, y, lx, ly = data\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    h, lh = model(x, lx)\n",
    "    print(h.shape)\n",
    "    h = torch.permute(h, (1, 0, 2))\n",
    "    print(h.shape, y.shape)\n",
    "    loss = criterion(h, y, lh, ly)\n",
    "    print(loss)\n",
    "\n",
    "    print(calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lx, ly, decoder, LABELS))\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rd5aNaLVoR_g"
   },
   "source": [
    "# WandB\n",
    "\n",
    "You will need to fetch your api key from wandb.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "PiDduMaDIARE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshiwon1998\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/shiwon/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"930ceec05ed04b6998ffe53abfbc75f287963822\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4s52yBOvICPZ"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/shiwon/2024_CMU/IntroDL/HW3/P2/wandb/run-20241101_215820-kl44t8jr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/shiwon1998/hw3p2-ablations/runs/kl44t8jr' target=\"_blank\">early-submission</a></strong> to <a href='https://wandb.ai/shiwon1998/hw3p2-ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/shiwon1998/hw3p2-ablations' target=\"_blank\">https://wandb.ai/shiwon1998/hw3p2-ablations</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/shiwon1998/hw3p2-ablations/runs/kl44t8jr' target=\"_blank\">https://wandb.ai/shiwon1998/hw3p2-ablations/runs/kl44t8jr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(\n",
    "    name = \"early-submission\", ## Wandb creates random run names if you skip this field\n",
    "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
    "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"hw3p2-ablations\", ### Project should be created in your wandb account\n",
    "    config = config ### Wandb Config for your run\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6fLLj5KIMMOe"
   },
   "source": [
    "# Train Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "ri87MAdhMUz5"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, criterion, optimizer):\n",
    "\n",
    "    model.train()\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, data in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        x, y, lx, ly = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with torch.cuda.amp.autocast():\n",
    "            h, lh = model(x, lx)\n",
    "            h = torch.permute(h, (1, 0, 2))\n",
    "            loss = criterion(h, y, lh, ly)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "\n",
    "        batch_bar.update() # Update tqdm bar\n",
    "\n",
    "        # Another couple things you need for FP16.\n",
    "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
    "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
    "        scaler.update() # This is something added just for FP16\n",
    "\n",
    "        del x, y, lx, ly, h, lh, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close() # You need this to close the tqdm bar\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
    "\n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "\n",
    "    total_loss = 0\n",
    "    vdist = 0\n",
    "\n",
    "    for i, data in enumerate(val_loader):\n",
    "\n",
    "        x, y, lx, ly = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            h, lh = model(x, lx)\n",
    "            h = torch.permute(h, (1, 0, 2))\n",
    "            loss = criterion(h, y, lh, ly)\n",
    "\n",
    "        total_loss += float(loss)\n",
    "        vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
    "\n",
    "        batch_bar.update()\n",
    "\n",
    "        del x, y, lx, ly, h, lh, loss\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    batch_bar.close()\n",
    "    total_loss = total_loss/len(val_loader)\n",
    "    val_dist = vdist/len(val_loader)\n",
    "    return total_loss, val_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpYExu4vT4_g"
   },
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "husa5_EYMUz6"
   },
   "outputs": [],
   "source": [
    "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
    "    torch.save(\n",
    "        {'model_state_dict'         : model.state_dict(),\n",
    "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
    "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
    "         metric[0]                  : metric[1],\n",
    "         'epoch'                    : epoch},\n",
    "         path\n",
    "    )\n",
    "\n",
    "def load_model(path, model, metric= 'valid_acc', optimizer= None, scheduler= None):\n",
    "\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    if optimizer != None:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if scheduler != None:\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "\n",
    "    epoch   = checkpoint['epoch']\n",
    "    metric  = checkpoint[metric]\n",
    "\n",
    "    return [model, optimizer, scheduler, epoch, metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "tExvyl1BIdMC"
   },
   "outputs": [],
   "source": [
    "# This is for checkpointing, if you're doing it over multiple sessions\n",
    "\n",
    "last_epoch_completed = 0\n",
    "start = last_epoch_completed\n",
    "end = config[\"epochs\"]\n",
    "best_lev_dist = float(\"inf\") # if you're restarting from some checkpoint, use what you saw there.\n",
    "# epoch_model_path = # set the model path( Optional, you can just store best one. Make sure to make the changes below )\n",
    "best_model_path = '/home/shiwon/2024_CMU/IntroDL/HW3/P2/checkpoints/best_model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "JR43E28rM9Ak"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/446 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 2.2036\t Learning Rate 0.0020000\n",
      "\tVal Dist 25.8281%\t Val Loss 1.1604\n",
      "Saved best model\n",
      "\n",
      "Epoch: 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 1.5954\t Learning Rate 0.0000200\n",
      "\tVal Dist 24.0317%\t Val Loss 1.0792\n",
      "Saved best model\n",
      "\n",
      "Epoch: 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 1.7183\t Learning Rate 0.0000200\n",
      "\tVal Dist 23.9550%\t Val Loss 1.0743\n",
      "Saved best model\n",
      "\n",
      "Epoch: 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 1.6041\t Learning Rate 0.0000200\n",
      "\tVal Dist 23.1538%\t Val Loss 1.0438\n",
      "Saved best model\n",
      "\n",
      "Epoch: 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 1.5951\t Learning Rate 0.0000200\n",
      "\tVal Dist 23.0002%\t Val Loss 1.0336\n",
      "Saved best model\n",
      "\n",
      "Epoch: 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 1.5447\t Learning Rate 0.0000200\n",
      "\tVal Dist 22.7709%\t Val Loss 1.0216\n",
      "Saved best model\n",
      "\n",
      "Epoch: 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 1.5891\t Learning Rate 0.0000200\n",
      "\tVal Dist 22.6278%\t Val Loss 1.0150\n",
      "Saved best model\n",
      "\n",
      "Epoch: 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 1.5837\t Learning Rate 0.0000200\n",
      "\tVal Dist 22.4707%\t Val Loss 1.0101\n",
      "Saved best model\n",
      "\n",
      "Epoch: 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 1.5534\t Learning Rate 0.0000200\n",
      "\tVal Dist 22.0293%\t Val Loss 0.9899\n",
      "Saved best model\n",
      "\n",
      "Epoch: 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss 1.5630\t Learning Rate 0.0000200\n",
      "\tVal Dist 22.2295%\t Val Loss 1.0028\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td></td></tr><tr><td>train_loss</td><td></td></tr><tr><td>valid_dist</td><td></td></tr><tr><td>valid_loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>2e-05</td></tr><tr><td>train_loss</td><td>1.56295</td></tr><tr><td>valid_dist</td><td>22.22946</td></tr><tr><td>valid_loss</td><td>1.0028</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">early-submission</strong> at: <a href='https://wandb.ai/shiwon1998/hw3p2-ablations/runs/kl44t8jr' target=\"_blank\">https://wandb.ai/shiwon1998/hw3p2-ablations/runs/kl44t8jr</a><br/> View project at: <a href='https://wandb.ai/shiwon1998/hw3p2-ablations' target=\"_blank\">https://wandb.ai/shiwon1998/hw3p2-ablations</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241101_215820-kl44t8jr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "for epoch in range(0, config['epochs']):\n",
    "\n",
    "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
    "\n",
    "    curr_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "    train_loss = train_model(model, train_loader, criterion, optimizer)\n",
    "    valid_loss, valid_dist = validate_model(model, val_loader, decoder, LABELS)\n",
    "    scheduler.step(valid_dist)\n",
    "\n",
    "    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
    "    print(\"\\tVal Dist {:.04f}%\\t Val Loss {:.04f}\".format(valid_dist, valid_loss))\n",
    "\n",
    "\n",
    "    wandb.log({\n",
    "        'train_loss': train_loss,\n",
    "        'valid_dist': valid_dist,\n",
    "        'valid_loss': valid_loss,\n",
    "        'lr'        : curr_lr\n",
    "    })\n",
    "\n",
    "    # save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)\n",
    "    # wandb.save(epoch_model_path)\n",
    "    # print(\"Saved epoch model\")\n",
    "\n",
    "    if valid_dist <= best_lev_dist:\n",
    "        best_lev_dist = valid_dist\n",
    "        save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)\n",
    "        wandb.save(best_model_path)\n",
    "        print(\"Saved best model\")\n",
    "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2H4EEj-sD32"
   },
   "source": [
    "# Generate Predictions and Submit to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "2moYJhTWsOG-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 41/41 [00:13<00:00,  2.94it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "\n",
    "# Follow the steps below:\n",
    "# 1. Create a new object for CTCBeamDecoder with larger (why?) number of beams\n",
    "# 2. Get prediction string by decoding the results of the beam decoder\n",
    "\n",
    "TEST_BEAM_WIDTH = 20\n",
    "\n",
    "test_decoder    = CTCBeamDecoder(labels=PHONEMES, beam_width=TEST_BEAM_WIDTH, blank_id=0)\n",
    "results = []\n",
    "\n",
    "model.eval()\n",
    "print(\"Testing\")\n",
    "for data in tqdm(test_loader):\n",
    "\n",
    "    x, lx   = data\n",
    "    x       = x.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        h, lh = model(x, lx)\n",
    "\n",
    "    prediction_string= decode_prediction(h, lh, test_decoder, PHONEME_MAP=LABELS)\n",
    "    results.extend(prediction_string)\n",
    "\n",
    "    del x, lx, h, lh\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "d70dvu_lsMlv"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/shiwon/2024_CMU/IntroDL/HW3/P2/11785-f24-hw3p2/test-clean/random_submission.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/test-clean/random_submission.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;241m=\u001b[39m results\n\u001b[1;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubmission.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/idl_hw3/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/idl_hw3/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/idl_hw3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/idl_hw3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/envs/idl_hw3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/idl_hw3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/envs/idl_hw3/lib/python3.9/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/shiwon/2024_CMU/IntroDL/HW3/P2/11785-f24-hw3p2/test-clean/random_submission.csv'"
     ]
    }
   ],
   "source": [
    "data_dir = f\"{root}/test-clean/random_submission.csv\"\n",
    "df = pd.read_csv(data_dir)\n",
    "df.label = results\n",
    "df.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1sZmEIs4yIz"
   },
   "outputs": [],
   "source": [
    "!kaggle competitions submit -c hw3p2-785-f24 -f submission.csv -m \"I made it!\"\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "rd5aNaLVoR_g"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "idl_hw3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
