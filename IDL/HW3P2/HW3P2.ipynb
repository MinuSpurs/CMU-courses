{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR4qfYrVoO4v"
      },
      "source": [
        "# Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA9qZoIDcx-h",
        "outputId": "d92e33be-24f6-4ab2-f078-01ae5f680f3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 GB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.3/24.3 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install torch==1.13.1+cu117 torchvision==0.14.1+cu117 torchtext==0.14.1 torchaudio==0.13.1 torchdata==0.5.1 --extra-index-url https://download.pytorch.org/whl/cu117 -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONgAWhqdoYy-"
      },
      "source": [
        "\n",
        "This may take a while"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SS7a7xeEoaV9",
        "outputId": "0ad9f732-0cf8-4939-e2fd-6588d451ba51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchsummaryX==1.3.0\n",
            "  Downloading torchsummaryX-1.3.0-py3-none-any.whl.metadata (325 bytes)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchsummaryX==1.3.0) (1.13.1+cu117)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchsummaryX==1.3.0) (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from torchsummaryX==1.3.0) (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->torchsummaryX==1.3.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->torchsummaryX==1.3.0) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->torchsummaryX==1.3.0) (2024.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchsummaryX==1.3.0) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->torchsummaryX==1.3.0) (1.16.0)\n",
            "Downloading torchsummaryX-1.3.0-py3-none-any.whl (3.6 kB)\n",
            "Installing collected packages: torchsummaryX\n",
            "Successfully installed torchsummaryX-1.3.0\n",
            "Collecting pandas==1.5.2\n",
            "  Downloading pandas-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.2) (2024.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.2) (1.26.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.2) (1.16.0)\n",
            "Downloading pandas-1.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 1.25.0 requires pandas>=1.5.3, but you have pandas 1.5.2 which is incompatible.\n",
            "cudf-cu12 24.10.1 requires pandas<2.2.3dev0,>=2.0, but you have pandas 1.5.2 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.2 which is incompatible.\n",
            "ibis-framework 9.2.0 requires pandas<3,>=1.5.3, but you have pandas 1.5.2 which is incompatible.\n",
            "mizani 0.13.0 requires pandas>=2.2.0, but you have pandas 1.5.2 which is incompatible.\n",
            "plotnine 0.14.0 requires pandas>=2.2.0, but you have pandas 1.5.2 which is incompatible.\n",
            "xarray 2024.10.0 requires pandas>=2.1, but you have pandas 1.5.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.5.2\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCloning into 'ctcdecode'...\n",
            "remote: Enumerating objects: 1102, done.\u001b[K\n",
            "remote: Counting objects: 100% (39/39), done.\u001b[K\n",
            "remote: Compressing objects: 100% (25/25), done.\u001b[K\n",
            "remote: Total 1102 (delta 16), reused 32 (delta 14), pack-reused 1063 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1102/1102), 782.27 KiB | 2.38 MiB/s, done.\n",
            "Resolving deltas: 100% (529/529), done.\n",
            "Submodule 'third_party/ThreadPool' (https://github.com/progschj/ThreadPool.git) registered for path 'third_party/ThreadPool'\n",
            "Submodule 'third_party/kenlm' (https://github.com/kpu/kenlm.git) registered for path 'third_party/kenlm'\n",
            "Cloning into '/content/ctcdecode/third_party/ThreadPool'...\n",
            "remote: Enumerating objects: 82, done.        \n",
            "remote: Counting objects: 100% (26/26), done.        \n",
            "remote: Compressing objects: 100% (9/9), done.        \n",
            "remote: Total 82 (delta 19), reused 17 (delta 17), pack-reused 56 (from 1)        \n",
            "Receiving objects: 100% (82/82), 13.34 KiB | 6.67 MiB/s, done.\n",
            "Resolving deltas: 100% (36/36), done.\n",
            "Cloning into '/content/ctcdecode/third_party/kenlm'...\n",
            "remote: Enumerating objects: 14170, done.        \n",
            "remote: Counting objects: 100% (483/483), done.        \n",
            "remote: Compressing objects: 100% (337/337), done.        \n",
            "remote: Total 14170 (delta 167), reused 410 (delta 132), pack-reused 13687 (from 1)        \n",
            "Receiving objects: 100% (14170/14170), 5.91 MiB | 10.29 MiB/s, done.\n",
            "Resolving deltas: 100% (8047/8047), done.\n",
            "Submodule path 'third_party/ThreadPool': checked out '9a42ec1329f259a5f4881a291db1dcb8f2ad9040'\n",
            "Submodule path 'third_party/kenlm': checked out '35835f1ac4884126458ac89f9bf6dd9ccad561e0'\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/content/ctcdecode\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ctcdecode (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummaryX==1.3.0\n",
        "!pip install pandas==1.5.2\n",
        "!pip install wandb --quiet\n",
        "!pip install python-Levenshtein -q\n",
        "!git clone --recursive https://github.com/parlance/ctcdecode.git\n",
        "!pip install wget -q\n",
        "%cd ctcdecode\n",
        "!pip install . -q\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWVONJxCobPc"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78ZTCIXoof2f",
        "outputId": "473e41cc-8206-44eb-8f3b-da8544969abf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummaryX import summary\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "import gc\n",
        "\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "import ctcdecode\n",
        "import Levenshtein\n",
        "from ctcdecode import CTCBeamDecoder\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gg3-yJ8tok34"
      },
      "source": [
        "# Kaggle Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdUelfGhom1m",
        "outputId": "7025b3e2-1b5d-4597-e396-75a251e8f86d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/59.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8 -q\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"gapbu123\",\"key\":\"9aead84bcd4b2de7b74761101e6ffe3f\"}')\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSjBwfXeoq4B",
        "outputId": "aed2f77e-cc65-48c6-dde5-113d238b479d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 11-785-hw3p2-f24.zip to /content\n",
            "100% 3.97G/3.97G [00:24<00:00, 267MB/s]\n",
            "100% 3.97G/3.97G [00:24<00:00, 175MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c 11-785-hw3p2-f24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ruxWP60LCQA",
        "outputId": "d6033f03-2743-43d2-cfc4-ff56c1ba6b62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11785-f24-hw3p2  11-785-hw3p2-f24.zip  ctcdecode  sample_data\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "This will take a couple minutes, but you should see at least the following:\n",
        "11-785-f24-hw3p2  ctcdecode  hw3p2asr-f24.zip  sample_data\n",
        "'''\n",
        "!unzip -q /content/11-785-hw3p2-f24.zip\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ORNHnSFroP0"
      },
      "source": [
        "# Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "k0v7wHRWrqH6"
      },
      "outputs": [],
      "source": [
        "# ARPABET PHONEME MAPPING\n",
        "# DO NOT CHANGE\n",
        "\n",
        "CMUdict_ARPAbet = {\n",
        "    \"\" : \" \",\n",
        "    \"[SIL]\": \"-\", \"NG\": \"G\", \"F\" : \"f\", \"M\" : \"m\", \"AE\": \"@\",\n",
        "    \"R\"    : \"r\", \"UW\": \"u\", \"N\" : \"n\", \"IY\": \"i\", \"AW\": \"W\",\n",
        "    \"V\"    : \"v\", \"UH\": \"U\", \"OW\": \"o\", \"AA\": \"a\", \"ER\": \"R\",\n",
        "    \"HH\"   : \"h\", \"Z\" : \"z\", \"K\" : \"k\", \"CH\": \"C\", \"W\" : \"w\",\n",
        "    \"EY\"   : \"e\", \"ZH\": \"Z\", \"T\" : \"t\", \"EH\": \"E\", \"Y\" : \"y\",\n",
        "    \"AH\"   : \"A\", \"B\" : \"b\", \"P\" : \"p\", \"TH\": \"T\", \"DH\": \"D\",\n",
        "    \"AO\"   : \"c\", \"G\" : \"g\", \"L\" : \"l\", \"JH\": \"j\", \"OY\": \"O\",\n",
        "    \"SH\"   : \"S\", \"D\" : \"d\", \"AY\": \"Y\", \"S\" : \"s\", \"IH\": \"I\",\n",
        "    \"[SOS]\": \"[SOS]\", \"[EOS]\": \"[EOS]\"\n",
        "}\n",
        "\n",
        "CMUdict = list(CMUdict_ARPAbet.keys())\n",
        "ARPAbet = list(CMUdict_ARPAbet.values())\n",
        "\n",
        "\n",
        "PHONEMES = CMUdict[:-2]\n",
        "LABELS = ARPAbet[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eN2kcxwXLLBb",
        "outputId": "841da38f-ef97-4689-9645-54cb600d4d55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "print(sys.version)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agmNBKf4JrLV"
      },
      "source": [
        "### Train Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "afd0_vlbJmr_"
      },
      "outputs": [],
      "source": [
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    # For this homework, we give you full flexibility to design your data set class.\n",
        "    # Hint: The data from HW1 is very similar to this HW\n",
        "\n",
        "    def __init__(self, root, partition=\"train-clean-100\", phonemes=PHONEMES):\n",
        "        '''\n",
        "        Initializes the dataset.\n",
        "\n",
        "        INPUTS: What inputs do you need here?\n",
        "        '''\n",
        "\n",
        "        # Load the directory and all files in them\n",
        "\n",
        "        self.mfcc_dir = os.path.join(root, partition, \"mfcc\")\n",
        "        self.transcript_dir = os.path.join(root, partition, \"transcript\")\n",
        "\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "        self.transcript_files = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        self.PHONEMES = phonemes\n",
        "\n",
        "        # WHAT SHOULD THE LENGTH OF THE DATASET BE?\n",
        "        self.length = len(self.mfcc_files)\n",
        "        assert self.length == len(self.transcript_files)\n",
        "\n",
        "        # HOW CAN WE REPRESENT PHONEMES? CAN WE CREATE A MAPPING FOR THEM?\n",
        "        # HINT: TENSORS CANNOT STORE NON-NUMERICAL VALUES OR STRINGS\n",
        "\n",
        "        # CREATE AN ARRAY OF ALL FEATUERS AND LABELS\n",
        "        # WHAT NORMALIZATION TECHNIQUE DID YOU USE IN HW1? CAN WE USE IT HERE?\n",
        "        '''\n",
        "        You may decide to do this in __getitem__ if you wish.\n",
        "        However, doing this here will make the __init__ function take the load of\n",
        "        loading the data, and shift it away from training.\n",
        "        '''\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "\n",
        "        for i in range(self.length):\n",
        "            mfcc = np.load(os.path.join(self.mfcc_dir, self.mfcc_files[i]))\n",
        "            mfcc = (mfcc - np.mean(mfcc, axis=0)) / (np.std(mfcc, axis=0) + 1e-8) # Cepstral normalization\n",
        "            self.mfccs.append(mfcc)\n",
        "\n",
        "            transcript = np.load(os.path.join(self.transcript_dir, self.transcript_files[i]))[1:-1] # Remove SOS and EOS\n",
        "            self.transcripts.append([self.PHONEMES.index(p) for p in transcript])\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        '''\n",
        "        What do we return here?\n",
        "        '''\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        '''\n",
        "        RETURN THE MFCC COEFFICIENTS AND ITS CORRESPONDING LABELS\n",
        "\n",
        "        If you didn't do the loading and processing of the data in __init__,\n",
        "        do that here.\n",
        "\n",
        "        Once done, return a tuple of features and labels.\n",
        "        '''\n",
        "        mfcc = torch.FloatTensor(self.mfccs[ind])\n",
        "        transcript = torch.LongTensor(self.transcripts[ind])\n",
        "\n",
        "        return mfcc, transcript\n",
        "\n",
        "    def collate_fn(self,batch):\n",
        "        '''\n",
        "        TODO:\n",
        "        1.  Extract the features and labels from 'batch'\n",
        "        2.  We will additionally need to pad both features and labels,\n",
        "            look at pytorch's docs for pad_sequence\n",
        "        3.  This is a good place to perform transforms, if you so wish.\n",
        "            Performing them on batches will speed the process up a bit.\n",
        "        4.  Return batch of features, labels, lenghts of features,\n",
        "            and lengths of labels.\n",
        "        '''\n",
        "        # batch of input mfcc coefficients\n",
        "        batch_mfcc = [item[0] for item in batch]\n",
        "        # batch of output phonemes\n",
        "        batch_transcript = [item[1] for item in batch]\n",
        "\n",
        "        # HINT: CHECK OUT -> pad_sequence (imported above)\n",
        "        # Also be sure to check the input format (batch_first)\n",
        "        batch_mfcc_pad = pad_sequence(batch_mfcc, batch_first=True)\n",
        "        lengths_mfcc = [mfcc.shape[0] for mfcc in batch_mfcc]\n",
        "\n",
        "        batch_transcript_pad = pad_sequence(batch_transcript, batch_first=True)\n",
        "        lengths_transcript = [transcript.shape[0] for transcript in batch_transcript]\n",
        "\n",
        "        # You may apply some transformation, Time and Frequency masking, here in the collate function;\n",
        "        # Food for thought -> Why are we applying the transformation here and not in the __getitem__?\n",
        "        #                  -> Would we apply transformation on the validation set as well?\n",
        "        #                  -> Is the order of axes / dimensions as expected for the transform functions?\n",
        "\n",
        "        # Return the following values: padded features, padded labels, actual length of features, actual length of the labels\n",
        "        return batch_mfcc_pad, batch_transcript_pad, torch.LongTensor(lengths_mfcc), torch.LongTensor(lengths_transcript)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqDrxeHfJw4g"
      },
      "source": [
        "### Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HrLS1wfVJppA"
      },
      "outputs": [],
      "source": [
        "# Test Dataloader\n",
        "#TODO\n",
        "\n",
        "class AudioDatasetTest(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, partition=\"test-clean\"):\n",
        "\n",
        "        self.mfcc_dir = os.path.join(root, partition, \"mfcc\")\n",
        "        self.mfcc_files = sorted(os.listdir(self.mfcc_dir))\n",
        "        self.length = len(self.mfcc_files)\n",
        "\n",
        "        self.mfccs = []\n",
        "\n",
        "        for mfcc_name in self.mfcc_files:\n",
        "            mfcc = np.load(os.path.join(self.mfcc_dir, mfcc_name))\n",
        "            mfcc = (mfcc - np.mean(mfcc, axis=0)) / (np.std(mfcc, axis=0) + 1e-8)\n",
        "            self.mfccs.append(mfcc)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        mfcc = torch.FloatTensor(self.mfccs[ind])\n",
        "\n",
        "        return mfcc\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        batch_mfcc_pad = pad_sequence(batch, batch_first=True)\n",
        "        lengths_mfcc = [mfcc.shape[0] for mfcc in batch]\n",
        "\n",
        "        return batch_mfcc_pad, torch.LongTensor(lengths_mfcc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt-veYcdL6Fe"
      },
      "source": [
        "### Config - Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MN82c3KpLup8"
      },
      "outputs": [],
      "source": [
        "root = '/content/11785-f24-hw3p2'\n",
        "\n",
        "# Feel free to add more items here\n",
        "config = {\n",
        "    \"beam_width\" : 15,\n",
        "    \"lr\"         : 0.002,  \n",
        "    \"epochs\"     : 25,\n",
        "    \"batch_size\" : 64  \n",
        "}\n",
        "# You may pass this as a parameter to the dataset class above\n",
        "# This will help modularize your implementation\n",
        "transforms = [] # set of tranformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmuPk9J6L8dz"
      },
      "source": [
        "### Data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_kG0gU2x4hH",
        "outputId": "da1475b1-258b-4c24-db7f-78893c1bc644"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get me RAMMM!!!!\n",
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mzoYfTKu14s",
        "outputId": "4cb45685-4669-42bf-ccaf-80d9d1e9d1dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size:  64\n",
            "Train dataset samples = 28539, batches = 446\n",
            "Val dataset samples = 2703, batches = 43\n",
            "Test dataset samples = 2620, batches = 41\n"
          ]
        }
      ],
      "source": [
        "# Create objects for the dataset class\n",
        "train_data = AudioDataset(root, \"train-clean-100\")\n",
        "val_data = AudioDataset(root, \"dev-clean\")\n",
        "test_data = AudioDatasetTest(root, \"test-clean\")\n",
        "\n",
        "# Do NOT forget to pass in the collate function as parameter while creating the dataloader\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset=train_data,\n",
        "    num_workers=8,\n",
        "    batch_size=config['batch_size'],\n",
        "    pin_memory=True,\n",
        "    shuffle=True,\n",
        "    collate_fn=train_data.collate_fn\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset=val_data,\n",
        "    num_workers=2,\n",
        "    batch_size=config['batch_size'],\n",
        "    pin_memory=True,\n",
        "    shuffle=False,\n",
        "    collate_fn=val_data.collate_fn\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset=test_data,\n",
        "    num_workers=2,\n",
        "    batch_size=config['batch_size'],\n",
        "    pin_memory=True,\n",
        "    shuffle=False,\n",
        "    collate_fn=test_data.collate_fn\n",
        ")\n",
        "\n",
        "print(\"Batch size: \", config['batch_size'])\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Val dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXMtwyviKaxK",
        "outputId": "7b3cd977-fe44-46ac-e182-5740b58e4d99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 1653, 28]) torch.Size([64, 217]) torch.Size([64]) torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "# sanity check\n",
        "for data in train_loader:\n",
        "    x, y, lx, ly = data\n",
        "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSexxhdfMUzx"
      },
      "source": [
        "# NETWORK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB6eh3gnMUzy"
      },
      "source": [
        "### Pyramid Bi-LSTM (pBLSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ASf2ySFKnvGO",
        "outputId": "a80ad2a7-531d-43ff-ad2b-9373925e2996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ASRModel(\n",
            "  (augmentations): Sequential(\n",
            "    (0): PermuteBlock()\n",
            "    (1): TimeMasking()\n",
            "    (2): FrequencyMasking()\n",
            "    (3): PermuteBlock()\n",
            "  )\n",
            "  (encoder): Encoder(\n",
            "    (embedding): Conv1d(28, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
            "    (pblstm1): pBLSTM(\n",
            "      (blstm): LSTM(1024, 512, batch_first=True, bidirectional=True)\n",
            "      (dropout): LockedDropout()\n",
            "    )\n",
            "    (pblstm2): pBLSTM(\n",
            "      (blstm): LSTM(2048, 512, batch_first=True, bidirectional=True)\n",
            "      (dropout): LockedDropout()\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (mlp): Sequential(\n",
            "      (0): Linear(in_features=1024, out_features=1024, bias=False)\n",
            "      (1): PermuteBlock()\n",
            "      (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (3): PermuteBlock()\n",
            "      (4): GELU(approximate='none')\n",
            "      (5): Linear(in_features=1024, out_features=512, bias=False)\n",
            "      (6): PermuteBlock()\n",
            "      (7): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (8): PermuteBlock()\n",
            "      (9): GELU(approximate='none')\n",
            "      (10): Linear(in_features=512, out_features=256, bias=False)\n",
            "      (11): PermuteBlock()\n",
            "      (12): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (13): PermuteBlock()\n",
            "      (14): GELU(approximate='none')\n",
            "      (15): Linear(in_features=256, out_features=128, bias=False)\n",
            "      (16): PermuteBlock()\n",
            "      (17): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (18): PermuteBlock()\n",
            "      (19): GELU(approximate='none')\n",
            "      (20): Linear(in_features=128, out_features=41, bias=True)\n",
            "    )\n",
            "    (softmax): LogSoftmax(dim=2)\n",
            "  )\n",
            ")\n",
            "================================================================================================\n",
            "                                         Kernel Shape      Output Shape  \\\n",
            "Layer                                                                     \n",
            "0_augmentations.PermuteBlock_0                      -    [64, 28, 2936]   \n",
            "1_augmentations.TimeMasking_1                       -    [64, 28, 2936]   \n",
            "2_augmentations.FrequencyMasking_2                  -    [64, 28, 2936]   \n",
            "3_augmentations.PermuteBlock_3                      -    [64, 2936, 28]   \n",
            "4_encoder.Conv1d_embedding               [28, 512, 3]   [64, 512, 2936]   \n",
            "5_encoder.pblstm1.LockedDropout_dropout             -  [64, 1468, 1024]   \n",
            "6_encoder.pblstm1.LSTM_blstm                        -     [21060, 1024]   \n",
            "7_encoder.pblstm2.LockedDropout_dropout             -   [64, 734, 2048]   \n",
            "8_encoder.pblstm2.LSTM_blstm                        -     [10517, 1024]   \n",
            "9_decoder.mlp.Linear_0                   [1024, 1024]   [64, 734, 1024]   \n",
            "10_decoder.mlp.PermuteBlock_1                       -   [64, 1024, 734]   \n",
            "11_decoder.mlp.BatchNorm1d_2                   [1024]   [64, 1024, 734]   \n",
            "12_decoder.mlp.PermuteBlock_3                       -   [64, 734, 1024]   \n",
            "13_decoder.mlp.GELU_4                               -   [64, 734, 1024]   \n",
            "14_decoder.mlp.Linear_5                   [1024, 512]    [64, 734, 512]   \n",
            "15_decoder.mlp.PermuteBlock_6                       -    [64, 512, 734]   \n",
            "16_decoder.mlp.BatchNorm1d_7                    [512]    [64, 512, 734]   \n",
            "17_decoder.mlp.PermuteBlock_8                       -    [64, 734, 512]   \n",
            "18_decoder.mlp.GELU_9                               -    [64, 734, 512]   \n",
            "19_decoder.mlp.Linear_10                   [512, 256]    [64, 734, 256]   \n",
            "20_decoder.mlp.PermuteBlock_11                      -    [64, 256, 734]   \n",
            "21_decoder.mlp.BatchNorm1d_12                   [256]    [64, 256, 734]   \n",
            "22_decoder.mlp.PermuteBlock_13                      -    [64, 734, 256]   \n",
            "23_decoder.mlp.GELU_14                              -    [64, 734, 256]   \n",
            "24_decoder.mlp.Linear_15                   [256, 128]    [64, 734, 128]   \n",
            "25_decoder.mlp.PermuteBlock_16                      -    [64, 128, 734]   \n",
            "26_decoder.mlp.BatchNorm1d_17                   [128]    [64, 128, 734]   \n",
            "27_decoder.mlp.PermuteBlock_18                      -    [64, 734, 128]   \n",
            "28_decoder.mlp.GELU_19                              -    [64, 734, 128]   \n",
            "29_decoder.mlp.Linear_20                    [128, 41]     [64, 734, 41]   \n",
            "30_decoder.LogSoftmax_softmax                       -     [64, 734, 41]   \n",
            "\n",
            "                                             Params    Mult-Adds  \n",
            "Layer                                                             \n",
            "0_augmentations.PermuteBlock_0                    -            -  \n",
            "1_augmentations.TimeMasking_1                     -            -  \n",
            "2_augmentations.FrequencyMasking_2                -            -  \n",
            "3_augmentations.PermuteBlock_3                    -            -  \n",
            "4_encoder.Conv1d_embedding                   43.52k  126.271488M  \n",
            "5_encoder.pblstm1.LockedDropout_dropout           -            -  \n",
            "6_encoder.pblstm1.LSTM_blstm              6.299648M    6.291456M  \n",
            "7_encoder.pblstm2.LockedDropout_dropout           -            -  \n",
            "8_encoder.pblstm2.LSTM_blstm             10.493952M    10.48576M  \n",
            "9_decoder.mlp.Linear_0                    1.048576M    1.048576M  \n",
            "10_decoder.mlp.PermuteBlock_1                     -            -  \n",
            "11_decoder.mlp.BatchNorm1d_2                 2.048k       1.024k  \n",
            "12_decoder.mlp.PermuteBlock_3                     -            -  \n",
            "13_decoder.mlp.GELU_4                             -            -  \n",
            "14_decoder.mlp.Linear_5                    524.288k     524.288k  \n",
            "15_decoder.mlp.PermuteBlock_6                     -            -  \n",
            "16_decoder.mlp.BatchNorm1d_7                 1.024k        512.0  \n",
            "17_decoder.mlp.PermuteBlock_8                     -            -  \n",
            "18_decoder.mlp.GELU_9                             -            -  \n",
            "19_decoder.mlp.Linear_10                   131.072k     131.072k  \n",
            "20_decoder.mlp.PermuteBlock_11                    -            -  \n",
            "21_decoder.mlp.BatchNorm1d_12                 512.0        256.0  \n",
            "22_decoder.mlp.PermuteBlock_13                    -            -  \n",
            "23_decoder.mlp.GELU_14                            -            -  \n",
            "24_decoder.mlp.Linear_15                    32.768k      32.768k  \n",
            "25_decoder.mlp.PermuteBlock_16                    -            -  \n",
            "26_decoder.mlp.BatchNorm1d_17                 256.0        128.0  \n",
            "27_decoder.mlp.PermuteBlock_18                    -            -  \n",
            "28_decoder.mlp.GELU_19                            -            -  \n",
            "29_decoder.mlp.Linear_20                     5.289k       5.248k  \n",
            "30_decoder.LogSoftmax_softmax                     -            -  \n",
            "------------------------------------------------------------------------------------------------\n",
            "                           Totals\n",
            "Total params           18.582953M\n",
            "Trainable params       18.582953M\n",
            "Non-trainable params          0.0\n",
            "Mult-Adds             144.792576M\n",
            "================================================================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"summary(model, x\",\n  \"rows\": 31,\n  \"fields\": [\n    {\n      \"column\": \"Layer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 31,\n        \"samples\": [\n          \"27_decoder.mlp.PermuteBlock_18\",\n          \"15_decoder.mlp.PermuteBlock_6\",\n          \"23_decoder.mlp.GELU_14\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Kernel Shape\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Output Shape\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Params\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3336110.7686228245,\n        \"min\": 256.0,\n        \"max\": 10493952.0,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          256.0,\n          32768.0,\n          43520.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Mult-Adds\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 36116403.68074004,\n        \"min\": 128.0,\n        \"max\": 126271488.0,\n        \"num_unique_values\": 12,\n        \"samples\": [\n          128.0,\n          32768.0,\n          126271488.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-bc3b5720-0d40-441f-8a98-2b410b8c46b5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_augmentations.PermuteBlock_0</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 28, 2936]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_augmentations.TimeMasking_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 28, 2936]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_augmentations.FrequencyMasking_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 28, 2936]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_augmentations.PermuteBlock_3</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 2936, 28]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_encoder.Conv1d_embedding</th>\n",
              "      <td>[28, 512, 3]</td>\n",
              "      <td>[64, 512, 2936]</td>\n",
              "      <td>43520.0</td>\n",
              "      <td>126271488.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_encoder.pblstm1.LockedDropout_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 1468, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_encoder.pblstm1.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[21060, 1024]</td>\n",
              "      <td>6299648.0</td>\n",
              "      <td>6291456.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_encoder.pblstm2.LockedDropout_dropout</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 734, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_encoder.pblstm2.LSTM_blstm</th>\n",
              "      <td>-</td>\n",
              "      <td>[10517, 1024]</td>\n",
              "      <td>10493952.0</td>\n",
              "      <td>10485760.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_decoder.mlp.Linear_0</th>\n",
              "      <td>[1024, 1024]</td>\n",
              "      <td>[64, 734, 1024]</td>\n",
              "      <td>1048576.0</td>\n",
              "      <td>1048576.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_decoder.mlp.PermuteBlock_1</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 1024, 734]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_decoder.mlp.BatchNorm1d_2</th>\n",
              "      <td>[1024]</td>\n",
              "      <td>[64, 1024, 734]</td>\n",
              "      <td>2048.0</td>\n",
              "      <td>1024.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_decoder.mlp.PermuteBlock_3</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 734, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_decoder.mlp.GELU_4</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 734, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_decoder.mlp.Linear_5</th>\n",
              "      <td>[1024, 512]</td>\n",
              "      <td>[64, 734, 512]</td>\n",
              "      <td>524288.0</td>\n",
              "      <td>524288.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15_decoder.mlp.PermuteBlock_6</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 512, 734]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16_decoder.mlp.BatchNorm1d_7</th>\n",
              "      <td>[512]</td>\n",
              "      <td>[64, 512, 734]</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>512.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_decoder.mlp.PermuteBlock_8</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 734, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18_decoder.mlp.GELU_9</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 734, 512]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19_decoder.mlp.Linear_10</th>\n",
              "      <td>[512, 256]</td>\n",
              "      <td>[64, 734, 256]</td>\n",
              "      <td>131072.0</td>\n",
              "      <td>131072.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_decoder.mlp.PermuteBlock_11</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 256, 734]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21_decoder.mlp.BatchNorm1d_12</th>\n",
              "      <td>[256]</td>\n",
              "      <td>[64, 256, 734]</td>\n",
              "      <td>512.0</td>\n",
              "      <td>256.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22_decoder.mlp.PermuteBlock_13</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 734, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23_decoder.mlp.GELU_14</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 734, 256]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24_decoder.mlp.Linear_15</th>\n",
              "      <td>[256, 128]</td>\n",
              "      <td>[64, 734, 128]</td>\n",
              "      <td>32768.0</td>\n",
              "      <td>32768.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25_decoder.mlp.PermuteBlock_16</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 128, 734]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26_decoder.mlp.BatchNorm1d_17</th>\n",
              "      <td>[128]</td>\n",
              "      <td>[64, 128, 734]</td>\n",
              "      <td>256.0</td>\n",
              "      <td>128.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27_decoder.mlp.PermuteBlock_18</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 734, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28_decoder.mlp.GELU_19</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 734, 128]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29_decoder.mlp.Linear_20</th>\n",
              "      <td>[128, 41]</td>\n",
              "      <td>[64, 734, 41]</td>\n",
              "      <td>5289.0</td>\n",
              "      <td>5248.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30_decoder.LogSoftmax_softmax</th>\n",
              "      <td>-</td>\n",
              "      <td>[64, 734, 41]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bc3b5720-0d40-441f-8a98-2b410b8c46b5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bc3b5720-0d40-441f-8a98-2b410b8c46b5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bc3b5720-0d40-441f-8a98-2b410b8c46b5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8b9de614-c983-4bb5-b553-b60019a28c29\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8b9de614-c983-4bb5-b553-b60019a28c29')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8b9de614-c983-4bb5-b553-b60019a28c29 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                         Kernel Shape      Output Shape  \\\n",
              "Layer                                                                     \n",
              "0_augmentations.PermuteBlock_0                      -    [64, 28, 2936]   \n",
              "1_augmentations.TimeMasking_1                       -    [64, 28, 2936]   \n",
              "2_augmentations.FrequencyMasking_2                  -    [64, 28, 2936]   \n",
              "3_augmentations.PermuteBlock_3                      -    [64, 2936, 28]   \n",
              "4_encoder.Conv1d_embedding               [28, 512, 3]   [64, 512, 2936]   \n",
              "5_encoder.pblstm1.LockedDropout_dropout             -  [64, 1468, 1024]   \n",
              "6_encoder.pblstm1.LSTM_blstm                        -     [21060, 1024]   \n",
              "7_encoder.pblstm2.LockedDropout_dropout             -   [64, 734, 2048]   \n",
              "8_encoder.pblstm2.LSTM_blstm                        -     [10517, 1024]   \n",
              "9_decoder.mlp.Linear_0                   [1024, 1024]   [64, 734, 1024]   \n",
              "10_decoder.mlp.PermuteBlock_1                       -   [64, 1024, 734]   \n",
              "11_decoder.mlp.BatchNorm1d_2                   [1024]   [64, 1024, 734]   \n",
              "12_decoder.mlp.PermuteBlock_3                       -   [64, 734, 1024]   \n",
              "13_decoder.mlp.GELU_4                               -   [64, 734, 1024]   \n",
              "14_decoder.mlp.Linear_5                   [1024, 512]    [64, 734, 512]   \n",
              "15_decoder.mlp.PermuteBlock_6                       -    [64, 512, 734]   \n",
              "16_decoder.mlp.BatchNorm1d_7                    [512]    [64, 512, 734]   \n",
              "17_decoder.mlp.PermuteBlock_8                       -    [64, 734, 512]   \n",
              "18_decoder.mlp.GELU_9                               -    [64, 734, 512]   \n",
              "19_decoder.mlp.Linear_10                   [512, 256]    [64, 734, 256]   \n",
              "20_decoder.mlp.PermuteBlock_11                      -    [64, 256, 734]   \n",
              "21_decoder.mlp.BatchNorm1d_12                   [256]    [64, 256, 734]   \n",
              "22_decoder.mlp.PermuteBlock_13                      -    [64, 734, 256]   \n",
              "23_decoder.mlp.GELU_14                              -    [64, 734, 256]   \n",
              "24_decoder.mlp.Linear_15                   [256, 128]    [64, 734, 128]   \n",
              "25_decoder.mlp.PermuteBlock_16                      -    [64, 128, 734]   \n",
              "26_decoder.mlp.BatchNorm1d_17                   [128]    [64, 128, 734]   \n",
              "27_decoder.mlp.PermuteBlock_18                      -    [64, 734, 128]   \n",
              "28_decoder.mlp.GELU_19                              -    [64, 734, 128]   \n",
              "29_decoder.mlp.Linear_20                    [128, 41]     [64, 734, 41]   \n",
              "30_decoder.LogSoftmax_softmax                       -     [64, 734, 41]   \n",
              "\n",
              "                                             Params    Mult-Adds  \n",
              "Layer                                                             \n",
              "0_augmentations.PermuteBlock_0                  NaN          NaN  \n",
              "1_augmentations.TimeMasking_1                   NaN          NaN  \n",
              "2_augmentations.FrequencyMasking_2              NaN          NaN  \n",
              "3_augmentations.PermuteBlock_3                  NaN          NaN  \n",
              "4_encoder.Conv1d_embedding                  43520.0  126271488.0  \n",
              "5_encoder.pblstm1.LockedDropout_dropout         NaN          NaN  \n",
              "6_encoder.pblstm1.LSTM_blstm              6299648.0    6291456.0  \n",
              "7_encoder.pblstm2.LockedDropout_dropout         NaN          NaN  \n",
              "8_encoder.pblstm2.LSTM_blstm             10493952.0   10485760.0  \n",
              "9_decoder.mlp.Linear_0                    1048576.0    1048576.0  \n",
              "10_decoder.mlp.PermuteBlock_1                   NaN          NaN  \n",
              "11_decoder.mlp.BatchNorm1d_2                 2048.0       1024.0  \n",
              "12_decoder.mlp.PermuteBlock_3                   NaN          NaN  \n",
              "13_decoder.mlp.GELU_4                           NaN          NaN  \n",
              "14_decoder.mlp.Linear_5                    524288.0     524288.0  \n",
              "15_decoder.mlp.PermuteBlock_6                   NaN          NaN  \n",
              "16_decoder.mlp.BatchNorm1d_7                 1024.0        512.0  \n",
              "17_decoder.mlp.PermuteBlock_8                   NaN          NaN  \n",
              "18_decoder.mlp.GELU_9                           NaN          NaN  \n",
              "19_decoder.mlp.Linear_10                   131072.0     131072.0  \n",
              "20_decoder.mlp.PermuteBlock_11                  NaN          NaN  \n",
              "21_decoder.mlp.BatchNorm1d_12                 512.0        256.0  \n",
              "22_decoder.mlp.PermuteBlock_13                  NaN          NaN  \n",
              "23_decoder.mlp.GELU_14                          NaN          NaN  \n",
              "24_decoder.mlp.Linear_15                    32768.0      32768.0  \n",
              "25_decoder.mlp.PermuteBlock_16                  NaN          NaN  \n",
              "26_decoder.mlp.BatchNorm1d_17                 256.0        128.0  \n",
              "27_decoder.mlp.PermuteBlock_18                  NaN          NaN  \n",
              "28_decoder.mlp.GELU_19                          NaN          NaN  \n",
              "29_decoder.mlp.Linear_20                     5289.0       5248.0  \n",
              "30_decoder.LogSoftmax_softmax                   NaN          NaN  "
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PermuteBlock(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.transpose(1, 2)\n",
        "\n",
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self, dropout=0.25):\n",
        "        super(LockedDropout, self).__init__()\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        if not self.training or self.dropout == 0:\n",
        "            return x\n",
        "        mask = x.new_empty(1, x.size(1), x.size(2), requires_grad=False).bernoulli_(1 - self.dropout)\n",
        "        mask = mask / (1 - self.dropout)\n",
        "        return mask * x\n",
        "\n",
        "class pBLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(pBLSTM, self).__init__()\n",
        "        self.blstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, bidirectional=True, batch_first=True)\n",
        "        self.dropout = LockedDropout(0.25)\n",
        "\n",
        "    def forward(self, x_packed):\n",
        "        x, x_lens = nn.utils.rnn.pad_packed_sequence(x_packed, batch_first=True)\n",
        "        x, x_lens = self.trunc_reshape(x, x_lens)\n",
        "        x = self.dropout(x)\n",
        "        x_packed = nn.utils.rnn.pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
        "        lstm_out, _ = self.blstm(x_packed)\n",
        "        return lstm_out\n",
        "\n",
        "    def trunc_reshape(self, x, x_lens):\n",
        "        if x.size(1) % 2 != 0:\n",
        "            x_lens = x_lens - 1\n",
        "            max_len = x_lens.max().item()\n",
        "            x = x[:, :max_len, :]\n",
        "        batch_size, time_steps, feature_dim = x.size()\n",
        "        x = x.contiguous().view(batch_size, time_steps // 2, feature_dim * 2)\n",
        "        x_lens = x_lens // 2\n",
        "        return x, x_lens\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, emb_size=1024):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Conv1d(input_size, emb_size, kernel_size=3, padding=1)\n",
        "        self.pblstm1 = pBLSTM(input_size=emb_size * 2, hidden_size=emb_size)\n",
        "        self.pblstm2 = pBLSTM(input_size=emb_size * 4, hidden_size=emb_size)\n",
        "\n",
        "    def forward(self, x, x_lens):\n",
        "        x = x.transpose(1, 2)\n",
        "        x = self.embedding(x)\n",
        "        x = x.transpose(1, 2)\n",
        "        x_packed = nn.utils.rnn.pack_padded_sequence(x, x_lens, batch_first=True, enforce_sorted=False)\n",
        "        x_packed = self.pblstm1(x_packed)\n",
        "        x_packed = self.pblstm2(x_packed)\n",
        "        encoder_out, encoder_lens = nn.utils.rnn.pad_packed_sequence(x_packed, batch_first=True)\n",
        "        return encoder_out, encoder_lens\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, embed_size=2048, output_size=41): \n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embed_size, 1024, bias=False),\n",
        "            PermuteBlock(),\n",
        "            nn.BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            PermuteBlock(),\n",
        "            nn.GELU(),\n",
        "\n",
        "            nn.Linear(1024, 512, bias=False),\n",
        "            PermuteBlock(),\n",
        "            nn.BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            PermuteBlock(),\n",
        "            nn.GELU(),\n",
        "\n",
        "            nn.Linear(512, 256, bias=False),\n",
        "            PermuteBlock(),\n",
        "            nn.BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            PermuteBlock(),\n",
        "            nn.GELU(),\n",
        "\n",
        "            nn.Linear(256, 128, bias=False),\n",
        "            PermuteBlock(),\n",
        "            nn.BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
        "            PermuteBlock(),\n",
        "            nn.GELU(),\n",
        "\n",
        "            nn.Linear(128, output_size, bias=True)\n",
        "        )\n",
        "\n",
        "        self.softmax = nn.LogSoftmax(dim=2)\n",
        "\n",
        "    def forward(self, encoder_out):\n",
        "        out = self.mlp(encoder_out)\n",
        "        out = self.softmax(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "import torchaudio.transforms as tat\n",
        "\n",
        "class ASRModel(nn.Module):\n",
        "    def __init__(self, input_size=28, embed_size=2048, output_size=41):\n",
        "        super(ASRModel, self).__init__()\n",
        "\n",
        "        self.augmentations = nn.Sequential(\n",
        "            PermuteBlock(),\n",
        "            tat.TimeMasking(time_mask_param=65),\n",
        "            tat.FrequencyMasking(freq_mask_param=5),\n",
        "            PermuteBlock(),\n",
        "        )\n",
        "\n",
        "        self.encoder = Encoder(input_size=input_size, emb_size=embed_size//2)\n",
        "        self.decoder = Decoder(embed_size=embed_size, output_size=output_size)\n",
        "\n",
        "    def forward(self, x, lengths_x):\n",
        "        if self.training:\n",
        "            x = self.augmentations(x)\n",
        "        encoder_out, encoder_lens = self.encoder(x, lengths_x)\n",
        "        decoder_out = self.decoder(encoder_out)\n",
        "        return decoder_out, encoder_lens\n",
        "\n",
        "\n",
        "\n",
        "model = ASRModel(\n",
        "    input_size  = 28,\n",
        "    embed_size  = 1024,\n",
        "    output_size = len(PHONEMES)\n",
        ").to(device)\n",
        "print(model)\n",
        "summary(model, x.to(device), lx)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training Config\n",
        "Initialize Loss Criterion, Optimizer, CTC Beam Decoder, Scheduler, Scaler (Mixed-Precision), etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "iGoozH2nd6KB"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "\n",
        "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
        "# CTC Loss: https://pytorch.org/docs/stable/generated/torch.nn.CTCLoss.html\n",
        "# Refer to the handout for hints\n",
        "\n",
        "optimizer =  torch.optim.AdamW(model.parameters(), lr=config['lr'], weight_decay=1e-5) # What goes in here?\n",
        "\n",
        "# Declare the decoder. Use the CTC Beam Decoder to decode phonemes\n",
        "# CTC Beam Decoder Doc: https://github.com/parlance/ctcdecode\n",
        "decoder = CTCBeamDecoder(labels=PHONEMES, beam_width=config['beam_width'], blank_id=0, log_probs_input=True)\n",
        "\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=config['epochs'])\n",
        "\n",
        "# Mixed Precision, if you need it\n",
        "scaler = torch.cuda.amp.GradScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "TRmh4V5yxufK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from ctcdecode import CTCBeamDecoder  # Ensure you have ctcdecode installed\n",
        "\n",
        "# CTC Loss 설정\n",
        "criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
        "\n",
        "# AdamW 옵티마이저 설정 (weight decay = 0.01)\n",
        "optimizer = AdamW(model.parameters(), lr=config['lr'], weight_decay=0.01)\n",
        "\n",
        "# CTC Beam Decoder 설정\n",
        "decoder = CTCBeamDecoder(labels=PHONEMES, beam_width=config['beam_width'], blank_id=0, log_probs_input=True)\n",
        "\n",
        "# Cosine Annealing Scheduler 설정\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=config['epochs'])\n",
        "\n",
        "# Mixed Precision을 위한 GradScaler 설정\n",
        "scaler = torch.cuda.amp.GradScaler()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmc6_4eWL2Xp"
      },
      "source": [
        "# Decode Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KHjnCDddL36E"
      },
      "outputs": [],
      "source": [
        "def decode_prediction(output, output_lens, decoder, PHONEME_MAP= LABELS):\n",
        "\n",
        "    # Look at docs for CTC.decoder and find out what is returned here. Check the shape of output and expected shape in decode.\n",
        "    beam_results, beam_scores, timesteps, out_lens = decoder.decode(output, seq_lens=output_lens) #lengths - list of lengths\n",
        "\n",
        "    pred_strings                    = []\n",
        "\n",
        "    for i in range(output_lens.shape[0]):\n",
        "        # Create the prediction from the output of decoder.decode. Don't forget to map it using PHONEMES_MAP.\n",
        "        pred_indices = beam_results[i][0][:out_lens[i][0]].tolist()\n",
        "        pred_string = ''.join([PHONEME_MAP[idx] for idx in pred_indices])\n",
        "        pred_strings.append(pred_string)\n",
        "\n",
        "    return pred_strings\n",
        "\n",
        "def calculate_levenshtein(output, label, output_lens, label_lens, decoder, PHONEME_MAP= LABELS): # y - sequence of integers\n",
        "\n",
        "    dist            = 0\n",
        "    batch_size      = label.shape[0]\n",
        "\n",
        "    pred_strings    = decode_prediction(output, output_lens, decoder, PHONEME_MAP)\n",
        "\n",
        "    for i in range(batch_size):\n",
        "        # Get predicted string and label string for each element in the batch\n",
        "        pred_string = pred_strings[i]\n",
        "        label_indices = label[i][:label_lens[i]].tolist()\n",
        "        label_string = ''.join([PHONEME_MAP[idx] for idx in label_indices])\n",
        "        dist += Levenshtein.distance(pred_string, label_string)\n",
        "\n",
        "    dist /= batch_size\n",
        "\n",
        "    return dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qk9iZud1LXT"
      },
      "source": [
        "# Test Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnTLL-5gMBrY",
        "outputId": "08e71954-7b2f-4973-d4b8-de5fe602b273"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 734, 41])\n",
            "torch.Size([734, 64, 41]) torch.Size([64, 265])\n",
            "tensor(7.7435, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "208.015625\n"
          ]
        }
      ],
      "source": [
        "# test code to check shapes\n",
        "\n",
        "model.eval()\n",
        "for i, data in enumerate(val_loader, 0):\n",
        "    x, y, lx, ly = data\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    h, lh = model(x, lx)\n",
        "    print(h.shape)\n",
        "    h = torch.permute(h, (1, 0, 2))\n",
        "    print(h.shape, y.shape)\n",
        "    loss = criterion(h, y, lh, ly)\n",
        "    print(loss)\n",
        "\n",
        "    print(calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lx, ly, decoder, LABELS))\n",
        "\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd5aNaLVoR_g"
      },
      "source": [
        "# WandB\n",
        "\n",
        "You will need to fetch your api key from wandb.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiDduMaDIARE",
        "outputId": "c6bb00d7-2ff0-4117-d35d-56292a1714e9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login(key=\"f20506dbcca14d82ab5324952dad2c08e99085e2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "4s52yBOvICPZ",
        "outputId": "0e5a3d36-711d-4bac-df98-a12ce0e21977"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:dr9fpsxq) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">early-submission</strong> at: <a href='https://wandb.ai/gapbu-carnegie-mellon-university/hw3p2-ablations/runs/dr9fpsxq' target=\"_blank\">https://wandb.ai/gapbu-carnegie-mellon-university/hw3p2-ablations/runs/dr9fpsxq</a><br/> View project at: <a href='https://wandb.ai/gapbu-carnegie-mellon-university/hw3p2-ablations' target=\"_blank\">https://wandb.ai/gapbu-carnegie-mellon-university/hw3p2-ablations</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20241109_003711-dr9fpsxq/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:dr9fpsxq). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.5"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241109_004304-b1rqj3dx</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gapbu-carnegie-mellon-university/hw3p2-ablations/runs/b1rqj3dx' target=\"_blank\">early-submission</a></strong> to <a href='https://wandb.ai/gapbu-carnegie-mellon-university/hw3p2-ablations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gapbu-carnegie-mellon-university/hw3p2-ablations' target=\"_blank\">https://wandb.ai/gapbu-carnegie-mellon-university/hw3p2-ablations</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gapbu-carnegie-mellon-university/hw3p2-ablations/runs/b1rqj3dx' target=\"_blank\">https://wandb.ai/gapbu-carnegie-mellon-university/hw3p2-ablations/runs/b1rqj3dx</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "run = wandb.init(\n",
        "    name = \"early-submission\", ## Wandb creates random run names if you skip this field\n",
        "    reinit = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    # run_id = ### Insert specific run id here if you want to resume a previous run\n",
        "    # resume = \"must\" ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw3p2-ablations\", ### Project should be created in your wandb account\n",
        "    config = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fLLj5KIMMOe"
      },
      "source": [
        "# Train Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "ri87MAdhMUz5"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def train_model(model, train_loader, criterion, optimizer):\n",
        "\n",
        "    model.train()\n",
        "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, data in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast():\n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            lr=\"{:.06f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "        batch_bar.update() # Update tqdm bar\n",
        "\n",
        "        # Another couple things you need for FP16.\n",
        "        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n",
        "        scaler.step(optimizer) # This is a replacement for optimizer.step()\n",
        "        scaler.update() # This is something added just for FP16\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close() # You need this to close the tqdm bar\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "def validate_model(model, val_loader, decoder, phoneme_map= LABELS):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    total_loss = 0\n",
        "    vdist = 0\n",
        "\n",
        "    for i, data in enumerate(val_loader):\n",
        "\n",
        "        x, y, lx, ly = data\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            h, lh = model(x, lx)\n",
        "            h = torch.permute(h, (1, 0, 2))\n",
        "            loss = criterion(h, y, lh, ly)\n",
        "\n",
        "        total_loss += float(loss)\n",
        "        vdist += calculate_levenshtein(torch.permute(h, (1, 0, 2)), y, lh, ly, decoder, phoneme_map)\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))), dist=\"{:.04f}\".format(float(vdist / (i + 1))))\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "        del x, y, lx, ly, h, lh, loss\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    total_loss = total_loss/len(val_loader)\n",
        "    val_dist = vdist/len(val_loader)\n",
        "    return total_loss, val_dist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpYExu4vT4_g"
      },
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "husa5_EYMUz6"
      },
      "outputs": [],
      "source": [
        "def save_model(model, optimizer, scheduler, metric, epoch, path):\n",
        "    torch.save(\n",
        "        {'model_state_dict'         : model.state_dict(),\n",
        "         'optimizer_state_dict'     : optimizer.state_dict(),\n",
        "         'scheduler_state_dict'     : scheduler.state_dict(),\n",
        "         metric[0]                  : metric[1],\n",
        "         'epoch'                    : epoch},\n",
        "         path\n",
        "    )\n",
        "\n",
        "def load_model(path, model, metric='valid_acc', optimizer= None, scheduler= None):\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    if optimizer != None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    if scheduler != None:\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "\n",
        "    epoch   = checkpoint['epoch']\n",
        "    metric  = checkpoint[metric]\n",
        "\n",
        "    return [model, optimizer, scheduler, epoch, metric]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "tExvyl1BIdMC"
      },
      "outputs": [],
      "source": [
        "# This is for checkpointing, if you're doing it over multiple sessions\n",
        "\n",
        "last_epoch_completed = 0\n",
        "start = last_epoch_completed\n",
        "end = config[\"epochs\"]\n",
        "best_lev_dist = float(\"inf\") # if you're restarting from some checkpoint, use what you saw there.\n",
        "# epoch_model_path = # set the model path( Optional, you can just store best one. Make sure to make the changes below )\n",
        "best_model_path = '/content/checkpoints/best_model.pth'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR43E28rM9Ak",
        "outputId": "47e80049-a418-451a-90a2-257297a8c3b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 1/25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train:   4%|▍         | 18/446 [00:13<05:13,  1.36it/s, loss=4.0551, lr=0.002000]"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "for epoch in range(0, config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch: {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    curr_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    train_loss = train_model(model, train_loader, criterion, optimizer)\n",
        "    valid_loss, valid_dist = validate_model(model, val_loader, decoder, LABELS)\n",
        "    scheduler.step(valid_dist)\n",
        "\n",
        "    print(\"\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_loss, curr_lr))\n",
        "    print(\"\\tVal Dist {:.04f}%\\t Val Loss {:.04f}\".format(valid_dist, valid_loss))\n",
        "\n",
        "\n",
        "    wandb.log({\n",
        "        'train_loss': train_loss,\n",
        "        'valid_dist': valid_dist,\n",
        "        'valid_loss': valid_loss,\n",
        "        'lr'        : curr_lr\n",
        "    })\n",
        "\n",
        "    # save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, epoch_model_path)\n",
        "    # wandb.save(epoch_model_path)\n",
        "    # print(\"Saved epoch model\")\n",
        "\n",
        "    if valid_dist <= best_lev_dist:\n",
        "        best_lev_dist = valid_dist\n",
        "        save_model(model, optimizer, scheduler, ['valid_dist', valid_dist], epoch, best_model_path)\n",
        "        wandb.save(best_model_path)\n",
        "        print(\"Saved best model\")\n",
        "      # You may find it interesting to exlplore Wandb Artifcats to version your models\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2H4EEj-sD32"
      },
      "source": [
        "# Generate Predictions and Submit to Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-1kewIq6lpbV"
      },
      "outputs": [],
      "source": [
        "model = ASRModel(\n",
        "    input_size  = 28,\n",
        "    embed_size  = 192,\n",
        "    output_size = len(PHONEMES)\n",
        ").to(device)\n",
        "\n",
        "model, _, _, _, _ = load_model(best_model_path, model, 'valid_dist', optimizer, scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2moYJhTWsOG-",
        "outputId": "38c07bc6-37db-4979-c02a-76d6c338cf77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 41/41 [00:31<00:00,  1.30it/s]\n"
          ]
        }
      ],
      "source": [
        "# Make predictions\n",
        "\n",
        "# Follow the steps below:\n",
        "# 1. Create a new object for CTCBeamDecoder with larger (why?) number of beams\n",
        "# 2. Get prediction string by decoding the results of the beam decoder\n",
        "\n",
        "TEST_BEAM_WIDTH = 10\n",
        "\n",
        "test_decoder    = CTCBeamDecoder(labels=PHONEMES, beam_width=TEST_BEAM_WIDTH, blank_id=0, log_probs_input=True)\n",
        "results = []\n",
        "\n",
        "model.eval()\n",
        "print(\"Testing\")\n",
        "for data in tqdm(test_loader):\n",
        "\n",
        "    x, lx   = data\n",
        "    x       = x.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        h, lh = model(x, lx)\n",
        "\n",
        "    prediction_string= decode_prediction(h, lh, test_decoder, PHONEME_MAP=LABELS)\n",
        "    results.extend(prediction_string)\n",
        "\n",
        "    del x, lx, h, lh\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "d70dvu_lsMlv"
      },
      "outputs": [],
      "source": [
        "data_dir = f\"{root}/test-clean/random_submission.csv\"\n",
        "df = pd.read_csv(data_dir)\n",
        "df.label = results\n",
        "df.to_csv('/content/submission/submission_10.csv', index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1sZmEIs4yIz"
      },
      "outputs": [],
      "source": [
        "# !kaggle competitions submit -c hw3p2-785-f24 -f submission.csv -m \"I made it!\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "rd5aNaLVoR_g"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
